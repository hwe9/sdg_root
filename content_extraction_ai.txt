/sdg_root/src/content_extraction

-- Dockerfile

FROM python:3.11-slim

# Set working directory
WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    curl \
    libxml2-dev \
    libxslt-dev \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Create non-root user
RUN useradd -m -u 1000 contentuser && chown -R contentuser:contentuser /app
USER contentuser

# Expose port
EXPOSE 8004

# Health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8004/health || exit 1

# Run the application
CMD ["python", "-m", "uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8004"]

-- requirements.txt

fastapi==0.104.1
uvicorn[standard]==0.24.0
aiohttp==3.9.1
beautifulsoup4==4.12.2
feedparser==6.0.10
pydantic==2.5.0
httpx==0.25.2
python-multipart==0.0.6
lxml==4.9.3
html5lib==1.1
python-dateutil==2.8.2
asyncio==3.4.3

-- main.py

"""
Content Extraction Service - FastAPI Application
Handles extraction from multiple sources: Gemini, web pages, newsletters, RSS feeds
"""
import logging
from typing import List, Dict, Any, Optional
import asyncio
from datetime import datetime

from fastapi import FastAPI, HTTPException, BackgroundTasks, Depends, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field, HttpUrl
import httpx

from extractors.gemini_extractor import GeminiExtractor
from extractors.web_extractor import WebExtractor
from extractors.newsletter_extractor import NewsletterExtractor
from extractors.rss_extractor import RSSExtractor

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Pydantic models
class ExtractionRequest(BaseModel):
    url: HttpUrl = Field(..., description="Source URL to extract content from")
    source_type: Optional[str] = Field(None, description="Source type hint: web, rss, newsletter, gemini")
    language: Optional[str] = Field("en", description="Expected content language")
    region: Optional[str] = Field(None, description="Expected content region")
    
class BatchExtractionRequest(BaseModel):
    urls: List[HttpUrl] = Field(..., description="List of URLs to extract", max_items=20)
    source_type: Optional[str] = Field(None, description="Source type for all URLs")
    language: Optional[str] = Field("en", description="Expected content language")
    region: Optional[str] = Field(None, description="Expected content region")

class GeminiAnalysisRequest(BaseModel):
    content: str = Field(..., description="Content to analyze with Gemini", min_length=100)
    source_url: Optional[HttpUrl] = Field(None, description="Original source URL")
    language: Optional[str] = Field("en", description="Content language")

class ExtractionResponse(BaseModel):
    success: bool
    extracted_count: int
    content: List[Dict[str, Any]]
    metadata: Dict[str, Any]
    processing_time: float

# Global extractors
extractors = {}

async def initialize_extractors():
    """Initialize extraction services"""
    config = {
        "retry_attempts": 3,
        "retry_delay": 1.0,
        "timeout": 30,
        "concurrent_requests": 5,
        "max_entries_per_feed": 50,
        "user_agent": "SDG-Pipeline-ContentExtractor/1.0"
    }
    
    extractors['web'] = WebExtractor(config)
    extractors['newsletter'] = NewsletterExtractor(config)
    extractors['rss'] = RSSExtractor(config)
    extractors['gemini'] = GeminiExtractor(config)
    
    logger.info("Content extractors initialized")

# FastAPI app
app = FastAPI(
    title="SDG Content Extraction Service",
    description="Microservice for extracting and analyzing content from multiple sources",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.on_event("startup")
async def startup_event():
    """Initialize services on startup"""
    await initialize_extractors()

@app.on_event("shutdown")
async def shutdown_event():
    """Cleanup on shutdown"""
    for extractor in extractors.values():
        if hasattr(extractor, 'session') and extractor.session:
            await extractor.session.close()

# Health check
@app.get("/health")
async def health_check():
    """Service health check"""
    return {
        "status": "healthy",
        "service": "SDG Content Extraction Service",
        "version": "1.0.0",
        "extractors_loaded": len(extractors),
        "available_extractors": list(extractors.keys())
    }

# Single URL extraction
@app.post("/extract", response_model=ExtractionResponse)
async def extract_content(request: ExtractionRequest):
    """Extract content from a single URL"""
    start_time = datetime.now()
    
    try:
        # Determine extractor type
        extractor_type = await determine_extractor_type(str(request.url), request.source_type)
        extractor = extractors.get(extractor_type)
        
        if not extractor:
            raise HTTPException(status_code=400, detail=f"Unsupported source type: {extractor_type}")
        
        # Extract content
        async with extractor:
            extracted_content = await extractor.extract(
                str(request.url),
                language=request.language,
                region=request.region
            )
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        return ExtractionResponse(
            success=True,
            extracted_count=len(extracted_content),
            content=[item.to_dict() for item in extracted_content],
            metadata={
                "source_url": str(request.url),
                "extractor_used": extractor_type,
                "processing_time": processing_time
            },
            processing_time=processing_time
        )
        
    except Exception as e:
        logger.error(f"Error extracting from {request.url}: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# Batch extraction
@app.post("/extract/batch", response_model=ExtractionResponse)
async def extract_batch_content(request: BatchExtractionRequest):
    """Extract content from multiple URLs"""
    start_time = datetime.now()
    
    try:
        all_content = []
        extractor_usage = {}
        
        # Group URLs by extractor type
        url_groups = {}
        for url in request.urls:
            extractor_type = await determine_extractor_type(str(url), request.source_type)
            url_groups.setdefault(extractor_type, []).append(str(url))
        
        # Process each group with appropriate extractor
        for extractor_type, urls in url_groups.items():
            extractor = extractors.get(extractor_type)
            if not extractor:
                logger.warning(f"Skipping unsupported extractor type: {extractor_type}")
                continue
            
            async with extractor:
                batch_content = await extractor.process_batch(
                    urls,
                    language=request.language,
                    region=request.region
                )
                all_content.extend(batch_content)
                extractor_usage[extractor_type] = len(urls)
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        return ExtractionResponse(
            success=True,
            extracted_count=len(all_content),
            content=[item.to_dict() for item in all_content],
            metadata={
                "total_urls": len(request.urls),
                "extractor_usage": extractor_usage,
                "processing_time": processing_time
            },
            processing_time=processing_time
        )
        
    except Exception as e:
        logger.error(f"Error in batch extraction: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# RSS feed extraction
@app.post("/extract/rss")
async def extract_rss_feeds(
    feed_urls: List[HttpUrl],
    max_entries: int = Query(50, description="Maximum entries per feed"),
    language: Optional[str] = Query("en", description="Expected language"),
    region: Optional[str] = Query(None, description="Expected region")
):
    """Extract content from RSS feeds"""
    start_time = datetime.now()
    
    try:
        rss_extractor = extractors['rss']
        
        # Update config for this request
        rss_extractor.max_entries = max_entries
        
        async with rss_extractor:
            all_content = await rss_extractor.extract_multiple_feeds(
                [str(url) for url in feed_urls],
                language=language,
                region=region
            )
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        return ExtractionResponse(
            success=True,
            extracted_count=len(all_content),
            content=[item.to_dict() for item in all_content],
            metadata={
                "feed_count": len(feed_urls),
                "max_entries_per_feed": max_entries,
                "processing_time": processing_time
            },
            processing_time=processing_time
        )
        
    except Exception as e:
        logger.error(f"Error extracting RSS feeds: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# Gemini analysis
@app.post("/analyze/gemini")
async def analyze_with_gemini(request: GeminiAnalysisRequest):
    """Analyze content using Gemini 2.5"""
    start_time = datetime.now()
    
    try:
        gemini_extractor = extractors['gemini']
        
        async with gemini_extractor:
            analyzed_content = await gemini_extractor.extract(
                request.content,
                source_url=str(request.source_url) if request.source_url else "",
                language=request.language
            )
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        return ExtractionResponse(
            success=True,
            extracted_count=len(analyzed_content),
            content=[item.to_dict() for item in analyzed_content],
            metadata={
                "analysis_type": "gemini_2.5",
                "content_length": len(request.content),
                "processing_time": processing_time
            },
            processing_time=processing_time
        )
        
    except Exception as e:
        logger.error(f"Error in Gemini analysis: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# Utility endpoints
@app.get("/extractors")
async def get_available_extractors():
    """Get information about available extractors"""
    return {
        "extractors": {
            name: {
                "type": type(extractor).__name__,
                "description": extractor.__class__.__doc__.strip() if extractor.__class__.__doc__ else ""
            }
            for name, extractor in extractors.items()
        }
    }

@app.post("/validate-url")
async def validate_url(url: HttpUrl, source_type: Optional[str] = None):
    """Validate if URL can be processed by available extractors"""
    try:
        extractor_type = await determine_extractor_type(str(url), source_type)
        extractor = extractors.get(extractor_type)
        
        if not extractor:
            return {
                "valid": False,
                "reason": f"No extractor available for type: {extractor_type}"
            }
        
        is_valid = extractor.validate_source(str(url))
        
        return {
            "valid": is_valid,
            "extractor_type": extractor_type,
            "supported": True
        }
        
    except Exception as e:
        return {
            "valid": False,
            "reason": str(e)
        }

# Integration with existing SDG pipeline
@app.post("/extract-and-forward")
async def extract_and_forward_to_pipeline(
    request: ExtractionRequest,
    background_tasks: BackgroundTasks
):
    """Extract content and forward to data processing service"""
    try:
        # Extract content
        extraction_result = await extract_content(request)
        
        if extraction_result.success:
            # Forward to data processing service in background
            background_tasks.add_task(
                forward_to_processing_service,
                extraction_result.content
            )
        
        return {
            "extraction_success": extraction_result.success,
            "extracted_count": extraction_result.extracted_count,
            "forwarded_to_processing": True
        }
        
    except Exception as e:
        logger.error(f"Error in extract-and-forward: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# Helper functions
async def determine_extractor_type(url: str, hint: Optional[str] = None) -> str:
    """Determine which extractor to use for a URL"""
    url_lower = url.lower()
    
    # Use hint if provided and valid
    if hint and hint in extractors:
        return hint
    
    # Auto-detect based on URL patterns
    if any(pattern in url_lower for pattern in ['/rss', '/feed', '.rss', '.xml']):
        return 'rss'
    elif any(pattern in url_lower for pattern in ['newsletter', 'digest', 'bulletin']):
        return 'newsletter'
    elif url_lower.startswith(('http://', 'https://')):
        return 'web'
    else:
        return 'gemini'  # Default for content analysis

async def forward_to_processing_service(content_items: List[Dict[str, Any]]):
    """Forward extracted content to data processing service"""
    try:
        processing_url = "http://data_processing_service:8001/process-content"
        
        async with httpx.AsyncClient() as client:
            response = await client.post(
                processing_url,
                json={"content_items": content_items},
                timeout=60
            )
            
            if response.status_code == 200:
                logger.info(f"Successfully forwarded {len(content_items)} items to processing service")
            else:
                logger.error(f"Error forwarding to processing service: {response.status_code}")
                
    except Exception as e:
        logger.error(f"Error forwarding content to processing service: {e}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8004,
        reload=True,
        log_level="info"
    )

    
/sdg_root/src/content_extraction/extractors

-- base_extractor.py

"""
Base extractor class for all content extraction sources
"""
import logging
from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from datetime import datetime
import asyncio
import aiohttp
from urllib.parse import urljoin, urlparse
import hashlib

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ExtractedContent:
    """Standard format for extracted content"""
    title: str
    content: str
    summary: Optional[str] = None
    url: str = ""
    source_type: str = ""
    language: str = "en"
    region: str = ""
    extracted_at: datetime = None
    metadata: Dict[str, Any] = None
    quality_score: float = 0.0
    sdg_relevance: List[int] = None
    
    def __post_init__(self):
        if self.extracted_at is None:
            self.extracted_at = datetime.utcnow()
        if self.metadata is None:
            self.metadata = {}
        if self.sdg_relevance is None:
            self.sdg_relevance = []
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "title": self.title,
            "content": self.content,
            "summary": self.summary,
            "url": self.url,
            "source_type": self.source_type,
            "language": self.language,
            "region": self.region,
            "extracted_at": self.extracted_at.isoformat(),
            "metadata": self.metadata,
            "quality_score": self.quality_score,
            "sdg_relevance": self.sdg_relevance
        }
    
    def content_hash(self) -> str:
        """Generate content hash for deduplication"""
        content_string = f"{self.title}{self.content}{self.url}"
        return hashlib.md5(content_string.encode()).hexdigest()

class BaseExtractor(ABC):
    """
    Abstract base class for all content extractors
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.session = None
        self.retry_attempts = config.get("retry_attempts", 3)
        self.retry_delay = config.get("retry_delay", 1.0)
        self.timeout = config.get("timeout", 30)
        self.user_agent = config.get("user_agent", 
            "SDG-Pipeline-Bot/1.0 (+https://sdg-pipeline.org/bot)")
        
    async def __aenter__(self):
        """Async context manager entry"""
        connector = aiohttp.TCPConnector(limit=100, limit_per_host=10)
        timeout = aiohttp.ClientTimeout(total=self.timeout)
        
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers={"User-Agent": self.user_agent}
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        if self.session:
            await self.session.close()
    
    @abstractmethod
    async def extract(self, source_url: str, **kwargs) -> List[ExtractedContent]:
        """Extract content from source"""
        pass
    
    @abstractmethod
    def validate_source(self, source_url: str) -> bool:
        """Validate if source URL is supported"""
        pass
    
    async def fetch_with_retry(self, url: str, **kwargs) -> Optional[aiohttp.ClientResponse]:
        """Fetch URL with retry logic"""
        for attempt in range(self.retry_attempts):
            try:
                async with self.session.get(url, **kwargs) as response:
                    if response.status == 200:
                        return response
                    elif response.status in [429, 503, 504]:  # Rate limiting or server errors
                        wait_time = self.retry_delay * (2 ** attempt)
                        logger.warning(f"Rate limited on {url}, waiting {wait_time}s")
                        await asyncio.sleep(wait_time)
                    else:
                        logger.warning(f"HTTP {response.status} for {url}")
                        return None
            except asyncio.TimeoutError:
                logger.warning(f"Timeout on attempt {attempt + 1} for {url}")
                if attempt < self.retry_attempts - 1:
                    await asyncio.sleep(self.retry_delay * (2 ** attempt))
            except Exception as e:
                logger.error(f"Error fetching {url}: {e}")
                if attempt < self.retry_attempts - 1:
                    await asyncio.sleep(self.retry_delay)
        
        return None
    
    def estimate_quality_score(self, content: ExtractedContent) -> float:
        """Estimate content quality score"""
        score = 0.0
        
        # Title quality (0-0.2)
        if content.title and len(content.title.strip()) > 10:
            score += 0.2
        
        # Content length (0-0.3)
        content_length = len(content.content.strip())
        if content_length > 500:
            score += 0.3
        elif content_length > 200:
            score += 0.15
        
        # Has URL (0-0.1)
        if content.url and content.url.startswith(('http://', 'https://')):
            score += 0.1
        
        # Has metadata (0-0.2)
        if content.metadata and len(content.metadata) > 2:
            score += 0.2
        
        # Language detection (0-0.1)
        if content.language and content.language != "unknown":
            score += 0.1
        
        # SDG relevance (0-0.1)
        if content.sdg_relevance and len(content.sdg_relevance) > 0:
            score += 0.1
        
        return min(score, 1.0)
    
    async def process_batch(self, urls: List[str], **kwargs) -> List[ExtractedContent]:
        """Process multiple URLs concurrently"""
        semaphore = asyncio.Semaphore(self.config.get("concurrent_requests", 5))
        
        async def extract_single(url: str) -> List[ExtractedContent]:
            async with semaphore:
                try:
                    return await self.extract(url, **kwargs)
                except Exception as e:
                    logger.error(f"Error extracting {url}: {e}")
                    return []
        
        tasks = [extract_single(url) for url in urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Flatten results and filter exceptions
        extracted_content = []
        for result in results:
            if isinstance(result, list):
                extracted_content.extend(result)
        
        return extracted_content

        
--- chatgpt_extractor.py

"""
ChatGPT HTML Result Extractor (Non-API)
Parses ChatGPT web interface results and extracts content with follow-up links
"""
import logging
from typing import List, Dict, Any, Optional
import re
import asyncio
from bs4 import BeautifulSoup, Comment
from urllib.parse import urljoin, urlparse
from datetime import datetime

from .base_extractor import BaseExtractor, ExtractedContent

logger = logging.getLogger(__name__)

class ChatGPTExtractor(BaseExtractor):
    """
    ChatGPT web interface result extractor
    Parses ChatGPT conversation HTML and extracts insights with source links
    """
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.chatgpt_selectors = self._load_chatgpt_selectors()
        self.link_keywords = self._load_link_keywords()
    
    def _load_chatgpt_selectors(self) -> Dict[str, List[str]]:
        """Load ChatGPT-specific HTML selectors"""
        return {
            "message_containers": [
                "div[data-message-author-role='assistant']",
                ".message-content",
                ".markdown",
                "[data-testid='conversation-turn-message']",
                ".prose"
            ],
            "user_messages": [
                "div[data-message-author-role='user']",
                ".user-message",
                "[data-testid='user-message']"
            ],
            "assistant_messages": [
                "div[data-message-author-role='assistant']", 
                ".assistant-message",
                "[data-testid='assistant-message']"
            ],
            "code_blocks": [
                "pre code",
                ".code-block", 
                "pre"
            ],
            "citations": [
                ".citation",
                "sup a",
                "[data-citation]"
            ]
        }
    
    def _load_link_keywords(self) -> Dict[str, List[str]]:
        """Keywords to identify valuable links"""
        return {
            "download_links": [
                "download", "pdf", "report", "study", "data", "dataset", 
                "excel", "csv", "doc", "presentation", "whitepaper"
            ],
            "reference_links": [
                "source", "reference", "cite", "study", "research", 
                "article", "paper", "journal", "publication"
            ],
            "official_links": [
                "gov", "un.org", "who.int", "worldbank.org", "oecd.org",
                "europa.eu", "unicef.org", "undp.org", "unesco.org"
            ],
            "sdg_links": [
                "sdg", "sustainable", "development", "goal", "target",
                "indicator", "progress", "agenda"
            ]
        }
    
    def validate_source(self, source_url: str) -> bool:
        """Validate if URL is from ChatGPT interface"""
        chatgpt_domains = [
            'chat.openai.com',
            'chatgpt.com',
            'openai.com/chat'
        ]
        
        parsed = urlparse(source_url.lower())
        return any(domain in parsed.netloc for domain in chatgpt_domains)
    
    async def extract(self, source_url: str, **kwargs) -> List[ExtractedContent]:
        """Extract content from ChatGPT conversation page"""
        try:
            response = await self.fetch_with_retry(source_url)
            if not response:
                return []
            
            html_content = await response.text()
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Remove script and style elements
            self._clean_soup(soup)
            
            # Extract conversation turns
            conversation_data = self._extract_conversation(soup, source_url)
            
            # Extract follow-up links and resources
            extracted_links = self._extract_valuable_links(soup, source_url)
            
            # Create structured content
            extracted_items = []
            
            for turn_data in conversation_data:
                if turn_data['role'] == 'assistant' and len(turn_data['content']) > 100:
                    extracted_content = ExtractedContent(
                        title=self._generate_title(turn_data['content']),
                        content=turn_data['content'],
                        summary=self._create_summary(turn_data['content']),
                        url=source_url,
                        source_type="chatgpt_result",
                        language=kwargs.get('language', 'en'),
                        region=kwargs.get('region', ''),
                        metadata={
                            'conversation_turn': turn_data['turn_number'],
                            'extracted_links': extracted_links,
                            'user_query': turn_data.get('user_query', ''),
                            'extraction_method': 'chatgpt_html_parser',
                            'contains_code': turn_data.get('has_code', False),
                            'contains_citations': turn_data.get('has_citations', False),
                            'response_length': len(turn_data['content'])
                        }
                    )
                    
                    # Detect SDG relevance
                    extracted_content.sdg_relevance = self._detect_sdg_relevance(turn_data['content'])
                    
                    # Calculate quality score
                    extracted_content.quality_score = self._calculate_chatgpt_quality_score(
                        extracted_content, turn_data, extracted_links
                    )
                    
                    extracted_items.append(extracted_content)
            
            return extracted_items
            
        except Exception as e:
            logger.error(f"Error extracting ChatGPT content from {source_url}: {e}")
            return []
    
    def _clean_soup(self, soup: BeautifulSoup):
        """Remove unwanted elements from soup"""
        # Remove scripts, styles, and navigation
        for element in soup(['script', 'style', 'nav', 'header', 'footer']):
            element.decompose()
        
        # Remove comments
        for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):
            comment.extract()
    
    def _extract_conversation(self, soup: BeautifulSoup, source_url: str) -> List[Dict[str, Any]]:
        """Extract conversation turns from ChatGPT interface"""
        conversation_turns = []
        
        # Try different selector strategies
        messages = []
        for selector_group in self.chatgpt_selectors["message_containers"]:
            found_messages = soup.select(selector_group)
            if found_messages:
                messages = found_messages
                break
        
        if not messages:
            # Fallback: look for common conversation patterns
            messages = soup.find_all(['div', 'article'], class_=re.compile(r'(message|conversation|chat|response)'))
        
        current_user_query = ""
        turn_number = 1
        
        for i, message in enumerate(messages):
            # Determine if this is user or assistant message
            message_text = message.get_text(separator=' ', strip=True)
            if len(message_text) < 20:  # Skip very short messages
                continue
            
            # Check message role
            role = self._determine_message_role(message, message_text)
            
            if role == 'user':
                current_user_query = message_text
            elif role == 'assistant':
                # Check for code blocks
                has_code = bool(message.find_all(['pre', 'code']))
                
                # Check for citations or links
                has_citations = bool(message.find_all('a') or message.find_all(['sup', 'cite']))
                
                conversation_turns.append({
                    'turn_number': turn_number,
                    'role': 'assistant',
                    'content': message_text,
                    'user_query': current_user_query,
                    'has_code': has_code,
                    'has_citations': has_citations,
                    'html_element': message
                })
                turn_number += 1
        
        return conversation_turns
    
    def _determine_message_role(self, message_element, message_text: str) -> str:
        """Determine if message is from user or assistant"""
        # Check data attributes first
        role_attr = message_element.get('data-message-author-role')
        if role_attr:
            return role_attr
        
        # Check class names
        class_names = ' '.join(message_element.get('class', [])).lower()
        if any(term in class_names for term in ['user', 'human']):
            return 'user'
        elif any(term in class_names for term in ['assistant', 'ai', 'bot']):
            return 'assistant'
        
        # Heuristic based on content patterns
        if re.match(r'^(what|how|why|can you|please|could you)', message_text.lower()):
            return 'user'
        elif len(message_text) > 200:  # Assistant responses tend to be longer
            return 'assistant'
        
        return 'unknown'
    
    def _extract_valuable_links(self, soup: BeautifulSoup, base_url: str) -> List[Dict[str, Any]]:
        """Extract and categorize valuable links from the page"""
        links = []
        
        # Find all links
        for link in soup.find_all('a', href=True):
            href = link['href']
            link_text = link.get_text(strip=True)
            
            # Skip empty links or javascript
            if not href or href.startswith(('javascript:', '#', 'mailto:')):
                continue
            
            # Convert relative URLs to absolute
            if href.startswith('/'):
                href = urljoin(base_url, href)
            elif not href.startswith(('http://', 'https://')):
                continue
            
            # Categorize link
            link_category = self._categorize_link(href, link_text)
            
            if link_category:  # Only include categorized links
                links.append({
                    'url': href,
                    'text': link_text,
                    'category': link_category,
                    'domain': urlparse(href).netloc,
                    'is_download': self._is_download_link(href, link_text),
                    'sdg_relevance': self._assess_link_sdg_relevance(href, link_text)
                })
        
        # Sort by relevance (SDG relevance + category priority)
        links.sort(key=lambda x: (x['sdg_relevance'], x['is_download']), reverse=True)
        
        return links[:20]  # Limit to most relevant links
    
    def _categorize_link(self, url: str, link_text: str) -> Optional[str]:
        """Categorize link based on URL and text content"""
        url_lower = url.lower()
        text_lower = link_text.lower()
        combined_text = f"{url_lower} {text_lower}"
        
        # Check for official/government sources
        if any(domain in url_lower for domain in self.link_keywords["official_links"]):
            return "official_source"
        
        # Check for download links
        if any(keyword in combined_text for keyword in self.link_keywords["download_links"]):
            return "download_resource"
        
        # Check for reference/research links
        if any(keyword in combined_text for keyword in self.link_keywords["reference_links"]):
            return "research_reference"
        
        # Check for SDG-specific links
        if any(keyword in combined_text for keyword in self.link_keywords["sdg_links"]):
            return "sdg_resource"
        
        # Check file extensions
        if any(ext in url_lower for ext in ['.pdf', '.doc', '.xls', '.ppt', '.csv']):
            return "document_file"
        
        return None
    
    def _is_download_link(self, url: str, link_text: str) -> bool:
        """Check if link is likely a download"""
        download_indicators = [
            '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx',
            '.csv', '.zip', '.tar', '.gz', 'download', 'attachment'
        ]
        
        combined = f"{url.lower()} {link_text.lower()}"
        return any(indicator in combined for indicator in download_indicators)
    
    def _assess_link_sdg_relevance(self, url: str, link_text: str) -> float:
        """Assess how relevant a link is to SDG content"""
        score = 0.0
        combined_text = f"{url.lower()} {link_text.lower()}"
        
        # Official SDG sources get high score
        sdg_domains = ['un.org', 'sdgs.un.org', 'sustainabledevelopment.un.org']
        if any(domain in url.lower() for domain in sdg_domains):
            score += 0.5
        
        # SDG keywords
        sdg_terms = ['sdg', 'sustainable development', 'agenda 2030', 'goal', 'target']
        score += sum(0.1 for term in sdg_terms if term in combined_text)
        
        # Research/data indicators
        research_terms = ['research', 'data', 'report', 'study', 'analysis']
        score += sum(0.05 for term in research_terms if term in combined_text)
        
        return min(score, 1.0)
    
    def _generate_title(self, content: str) -> str:
        """Generate title from ChatGPT response content"""
        # Look for clear topic indicators
        sentences = content.split('.')
        first_sentence = sentences[0].strip() if sentences else content[:100]
        
        # Remove common ChatGPT prefixes
        prefixes_to_remove = [
            "Based on", "According to", "Here's", "Here are", "I'll help you",
            "Let me", "To answer", "In response"
        ]
        
        for prefix in prefixes_to_remove:
            if first_sentence.startswith(prefix):
                # Try to find the actual topic
                remaining = first_sentence[len(prefix):].strip()
                if len(remaining) > 20:
                    first_sentence = remaining
                break
        
        # Clean up and limit length
        title = re.sub(r'^[^a-zA-Z0-9]*', '', first_sentence)
        return title[:150] if len(title) > 150 else title
    
    def _create_summary(self, content: str) -> str:
        """Create summary from ChatGPT response"""
        # Take first 2-3 sentences or up to 300 characters
        sentences = re.split(r'[.!?]', content)
        summary_sentences = []
        char_count = 0
        
        for sentence in sentences[:3]:
            sentence = sentence.strip()
            if sentence and char_count + len(sentence) <= 300:
                summary_sentences.append(sentence)
                char_count += len(sentence)
            else:
                break
        
        summary = '. '.join(summary_sentences)
        return summary + '.' if summary and not summary.endswith('.') else summary
    
    def _detect_sdg_relevance(self, content: str) -> List[int]:
        """Detect SDG relevance in ChatGPT response"""
        sdg_patterns = {
            1: r'\b(poverty|poor|income|wealth|social.protection)\b',
            2: r'\b(hunger|food|nutrition|agriculture|farming)\b',
            3: r'\b(health|healthcare|medical|disease|mortality)\b',
            4: r'\b(education|learning|school|literacy|skills)\b',
            5: r'\b(gender|women|girls|equality|empowerment)\b',
            6: r'\b(water|sanitation|hygiene|clean.water)\b',
            7: r'\b(energy|renewable|electricity|clean.energy)\b',
            8: r'\b(employment|jobs|economic.growth|decent.work)\b',
            9: r'\b(infrastructure|innovation|industry|technology)\b',
            10: r'\b(inequality|inclusion|discrimination|equity)\b',
            11: r'\b(cities|urban|housing|transport|sustainable.cities)\b',
            12: r'\b(consumption|production|waste|recycling|sustainable)\b',
            13: r'\b(climate|carbon|emission|greenhouse|warming)\b',
            14: r'\b(ocean|marine|sea|fisheries|aquatic)\b',
            15: r'\b(forest|biodiversity|ecosystem|wildlife|conservation)\b',
            16: r'\b(peace|justice|institutions|governance|law)\b',
            17: r'\b(partnership|cooperation|global|development.finance)\b'
        }
        
        content_lower = re.sub(r'[^\w\s]', ' ', content.lower())
        relevant_sdgs = []
        
        for sdg_id, pattern in sdg_patterns.items():
            if re.search(pattern, content_lower):
                relevant_sdgs.append(sdg_id)
        
        return relevant_sdgs
    
    def _calculate_chatgpt_quality_score(self, content: ExtractedContent, 
                                       turn_data: Dict[str, Any],
                                       extracted_links: List[Dict[str, Any]]) -> float:
        """Calculate quality score for ChatGPT extracted content"""
        score = 0.0
        
        # Base content quality (0-0.4)
        content_length = len(content.content)
        if content_length > 500:
            score += 0.4
        elif content_length > 200:
            score += 0.2
        
        # Has structured elements (0-0.2)
        if turn_data.get('has_code'):
            score += 0.1
        if turn_data.get('has_citations'):
            score += 0.1
        
        # Link quality (0-0.2)
        if extracted_links:
            high_quality_links = [l for l in extracted_links if l.get('sdg_relevance', 0) > 0.3]
            if len(high_quality_links) >= 3:
                score += 0.2
            elif len(high_quality_links) >= 1:
                score += 0.1
        
        # SDG relevance (0-0.1)
        if len(content.sdg_relevance) >= 2:
            score += 0.1
        elif len(content.sdg_relevance) >= 1:
            score += 0.05
        
        # User query context (0-0.1)
        user_query = turn_data.get('user_query', '')
        if len(user_query) > 20:  # Had meaningful user question
            score += 0.1
        
        return min(score, 1.0)
        
-- gemini_extractor.py

"""
Gemini 2.5 content analysis extractor
Processes content through Gemini for SDG analysis
"""
import logging
from typing import List, Dict, Any, Optional
import asyncio
import json
import re
from .base_extractor import BaseExtractor, ExtractedContent

logger = logging.getLogger(__name__)

class GeminiExtractor(BaseExtractor):
    """
    Extractor for Gemini 2.5 analysis results
    Processes existing content through Gemini for enhanced SDG insights
    """
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.gemini_prompts = self._load_gemini_prompts()
        self.sdg_keywords = self._load_sdg_keywords()
    
    def _load_gemini_prompts(self) -> Dict[str, str]:
        """Load Gemini analysis prompts"""
        return {
            "sdg_analysis": """
            Analyze the following text for relevance to the UN Sustainable Development Goals (SDGs).
            
            Text: {content}
            
            Please provide:
            1. Primary SDG goals (1-17) that this content addresses
            2. Confidence score (0-1) for each identified SDG
            3. Key themes and topics
            4. Regional relevance if mentioned
            5. Summary of SDG-related content
            
            Format your response as JSON with the following structure:
            {{
                "sdg_goals": [{{goal_number, confidence_score}}],
                "themes": ["theme1", "theme2"],
                "region": "region_name",
                "summary": "content_summary",
                "quality_indicators": {{
                    "has_data": boolean,
                    "has_citations": boolean,
                    "policy_relevant": boolean
                }}
            }}
            """,
            
            "content_enhancement": """
            Enhance and summarize the following content for SDG research purposes:
            
            Original Content: {content}
            
            Please provide:
            1. A concise summary (200-300 words)
            2. Key findings or insights
            3. Methodology mentioned (if any)
            4. Data sources referenced
            5. Policy implications
            
            Focus on aspects relevant to sustainable development research.
            """,
            
            "quality_assessment": """
            Assess the quality and credibility of this content:
            
            Content: {content}
            Source: {source_url}
            
            Evaluate:
            1. Scientific rigor (0-10)
            2. Data reliability (0-10) 
            3. Bias indicators
            4. Citation quality
            5. Overall credibility score (0-10)
            
            Provide reasoning for each score.
            """
        }
    
    def _load_sdg_keywords(self) -> Dict[int, List[str]]:
        """Load SDG-specific keywords for validation"""
        # This would typically load from your keywords.py file
        return {
            1: ["poverty", "income inequality", "social protection"],
            2: ["hunger", "food security", "nutrition", "agriculture"],
            3: ["health", "mortality", "disease", "healthcare"],
            4: ["education", "learning", "literacy", "skills"],
            5: ["gender", "women", "girls", "equality"],
            6: ["water", "sanitation", "hygiene"],
            7: ["energy", "renewable", "electricity", "clean"],
            8: ["employment", "economic growth", "decent work"],
            9: ["infrastructure", "innovation", "industry"],
            10: ["inequality", "inclusion", "discrimination"],
            11: ["cities", "urban", "housing", "transport"],
            12: ["consumption", "production", "waste", "sustainability"],
            13: ["climate", "greenhouse gas", "adaptation"],
            14: ["ocean", "marine", "fisheries"],
            15: ["forest", "biodiversity", "ecosystem"],
            16: ["peace", "justice", "institutions", "governance"],
            17: ["partnership", "cooperation", "finance"]
        }
    
    def validate_source(self, source_url: str) -> bool:
        """Validate if source is suitable for Gemini analysis"""
        # Accept any content for Gemini analysis
        return True
    
    async def extract(self, content_input: str, **kwargs) -> List[ExtractedContent]:
        """
        Process content through Gemini analysis
        content_input can be raw text, URL, or structured content
        """
        try:
            # Determine input type
            if content_input.startswith(('http://', 'https://')):
                # URL input - fetch content first
                content_text = await self._fetch_url_content(content_input)
                source_url = content_input
            else:
                # Direct text input
                content_text = content_input
                source_url = kwargs.get('source_url', '')
            
            if not content_text or len(content_text.strip()) < 100:
                logger.warning("Content too short for Gemini analysis")
                return []
            
            # Process through Gemini (simulated - replace with actual Gemini API)
            analysis_result = await self._simulate_gemini_analysis(content_text)
            
            # Create enhanced content object
            enhanced_content = ExtractedContent(
                title=self._extract_title(content_text),
                content=content_text,
                summary=analysis_result.get('summary', ''),
                url=source_url,
                source_type="gemini_analysis",
                language=kwargs.get('language', 'en'),
                region=analysis_result.get('region', ''),
                metadata={
                    'gemini_analysis': analysis_result,
                    'processing_method': 'gemini_2.5',
                    'themes': analysis_result.get('themes', []),
                    'quality_indicators': analysis_result.get('quality_indicators', {})
                },
                sdg_relevance=[goal['goal_number'] for goal in analysis_result.get('sdg_goals', [])]
            )
            
            # Calculate quality score
            enhanced_content.quality_score = self._calculate_gemini_quality_score(analysis_result)
            
            return [enhanced_content]
            
        except Exception as e:
            logger.error(f"Error in Gemini extraction: {e}")
            return []
    
    async def _fetch_url_content(self, url: str) -> str:
        """Fetch content from URL for analysis"""
        response = await self.fetch_with_retry(url)
        if response:
            content = await response.text()
            # Basic HTML cleaning
            clean_content = re.sub(r'<[^>]+>', ' ', content)
            clean_content = re.sub(r'\s+', ' ', clean_content).strip()
            return clean_content
        return ""
    
    async def _simulate_gemini_analysis(self, content: str) -> Dict[str, Any]:
        """
        Simulate Gemini 2.5 analysis
        Replace this with actual Gemini API calls
        """
        # Simulate processing delay
        await asyncio.sleep(0.5)
        
        # Simple SDG detection based on keywords
        detected_sdgs = []
        for sdg_id, keywords in self.sdg_keywords.items():
            keyword_matches = sum(1 for keyword in keywords if keyword.lower() in content.lower())
            if keyword_matches > 0:
                confidence = min(keyword_matches / len(keywords), 1.0)
                if confidence > 0.1:  # Minimum confidence threshold
                    detected_sdgs.append({
                        "goal_number": sdg_id,
                        "confidence_score": confidence
                    })
        
        # Generate summary (first 300 chars as simulation)
        summary = content[:300] + "..." if len(content) > 300 else content
        
        # Extract themes (simple word frequency analysis)
        words = re.findall(r'\b\w+\b', content.lower())
        word_freq = {}
        for word in words:
            if len(word) > 4:  # Only meaningful words
                word_freq[word] = word_freq.get(word, 0) + 1
        
        themes = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:5]
        themes = [theme[0] for theme in themes]
        
        # Quality indicators
        quality_indicators = {
            "has_data": any(word in content.lower() for word in ['data', 'statistics', 'research', 'study']),
            "has_citations": any(word in content.lower() for word in ['doi', 'reference', 'citation', 'source']),
            "policy_relevant": any(word in content.lower() for word in ['policy', 'government', 'legislation', 'regulation'])
        }
        
        return {
            "sdg_goals": detected_sdgs,
            "themes": themes,
            "region": self._detect_region(content),
            "summary": summary,
            "quality_indicators": quality_indicators
        }
    
    def _detect_region(self, content: str) -> str:
        """Simple region detection"""
        regions = {
            "EU": ["europe", "european", "eu", "brussels"],
            "USA": ["united states", "america", "usa", "us"],
            "China": ["china", "chinese", "beijing"],
            "India": ["india", "indian", "delhi"],
            "ASEAN": ["asean", "southeast asia", "vietnam", "thailand"],
            "BRICS": ["brics", "brazil", "russia", "south africa"]
        }
        
        content_lower = content.lower()
        for region, keywords in regions.items():
            if any(keyword in content_lower for keyword in keywords):
                return region
        
        return ""
    
    def _extract_title(self, content: str) -> str:
        """Extract or generate title from content"""
        # Simple title extraction - first sentence or first 100 chars
        sentences = content.split('.')
        if sentences and len(sentences[0].strip()) > 10:
            return sentences[0].strip()[:100]
        return content[:100].strip()
    
    def _calculate_gemini_quality_score(self, analysis: Dict[str, Any]) -> float:
        """Calculate quality score based on Gemini analysis"""
        score = 0.0
        
        # SDG relevance (0-0.4)
        sdg_goals = analysis.get('sdg_goals', [])
        if sdg_goals:
            avg_confidence = sum(goal['confidence_score'] for goal in sdg_goals) / len(sdg_goals)
            score += avg_confidence * 0.4
        
        # Quality indicators (0-0.3)
        quality_indicators = analysis.get('quality_indicators', {})
        quality_count = sum(quality_indicators.values())
        score += (quality_count / 3) * 0.3
        
        # Themes richness (0-0.2)
        themes = analysis.get('themes', [])
        if len(themes) >= 3:
            score += 0.2
        elif len(themes) >= 1:
            score += 0.1
        
        # Regional relevance (0-0.1)
        if analysis.get('region'):
            score += 0.1
        
        return min(score, 1.0)
    
    async def analyze_existing_content(self, content_items: List[Dict[str, Any]]) -> List[ExtractedContent]:
        """Analyze existing content items through Gemini"""
        results = []
        
        for item in content_items:
            content_text = item.get('content', '')
            if content_text:
                enhanced = await self.extract(
                    content_text,
                    source_url=item.get('url', ''),
                    language=item.get('language', 'en')
                )
                results.extend(enhanced)
        
        return results
        
-- perplexity_extractor.py

"""
Perplexity.ai HTML Result Extractor (Non-API)
Parses Perplexity search results and extracts content with source citations
"""
import logging
from typing import List, Dict, Any, Optional
import re
import asyncio
from bs4 import BeautifulSoup, Comment
from urllib.parse import urljoin, urlparse
from datetime import datetime

from .base_extractor import BaseExtractor, ExtractedContent

logger = logging.getLogger(__name__)

class PerplexityExtractor(BaseExtractor):
    """
    Perplexity.ai search result extractor
    Parses Perplexity response pages and extracts answers with source citations
    """
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.perplexity_selectors = self._load_perplexity_selectors()
        self.citation_patterns = self._load_citation_patterns()
    
    def _load_perplexity_selectors(self) -> Dict[str, List[str]]:
        """Load Perplexity-specific HTML selectors"""
        return {
            "answer_containers": [
                ".answer-content",
                ".response-text", 
                "[data-testid='answer']",
                ".prose-content",
                ".main-answer"
            ],
            "query_containers": [
                ".query-text",
                ".search-query",
                "[data-testid='query']",
                ".user-question"
            ],
            "source_citations": [
                ".citation",
                ".source-link",
                "[data-citation-index]",
                ".reference",
                "sup a"
            ],
            "source_list": [
                ".sources-list",
                ".references-section",
                ".source-references",
                "[data-testid='sources']"
            ],
            "follow_up_questions": [
                ".follow-up-questions",
                ".related-queries",
                ".suggested-questions"
            ]
        }
    
    def _load_citation_patterns(self) -> Dict[str, str]:
        """Load patterns for identifying citations"""
        return {
            "numbered_citation": r'\[(\d+)\]',
            "superscript_citation": r'<sup[^>]*>(\d+)</sup>',
            "parenthetical_citation": r'\((\d+)\)',
            "source_indicator": r'(source|according to|based on|from):\s*(.+?)(?=\.|$)',
        }
    
    def validate_source(self, source_url: str) -> bool:
        """Validate if URL is from Perplexity"""
        perplexity_domains = [
            'perplexity.ai',
            'www.perplexity.ai'
        ]
        
        parsed = urlparse(source_url.lower())
        return any(domain in parsed.netloc for domain in perplexity_domains)
    
    async def extract(self, source_url: str, **kwargs) -> List[ExtractedContent]:
        """Extract content from Perplexity search result page"""
        try:
            response = await self.fetch_with_retry(source_url)
            if not response:
                return []
            
            html_content = await response.text()
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Clean soup
            self._clean_soup(soup)
            
            # Extract query context
            user_query = self._extract_user_query(soup)
            
            # Extract main answer
            answer_content = self._extract_main_answer(soup, source_url)
            
            # Extract source citations and references
            citations = self._extract_citations(soup, source_url)
            
            # Extract follow-up questions
            follow_up_questions = self._extract_follow_up_questions(soup)
            
            if not answer_content:
                return []
            
            # Create extracted content
            extracted_content = ExtractedContent(
                title=self._generate_title(user_query, answer_content['text']),
                content=answer_content['text'],
                summary=self._create_summary(answer_content['text']),
                url=source_url,
                source_type="perplexity_search",
                language=kwargs.get('language', 'en'),
                region=kwargs.get('region', ''),
                metadata={
                    'user_query': user_query,
                    'source_citations': citations,
                    'follow_up_questions': follow_up_questions,
                    'extraction_method': 'perplexity_html_parser',
                    'answer_structure': answer_content.get('structure_type', 'prose'),
                    'citation_count': len(citations),
                    'high_quality_sources': self._count_high_quality_sources(citations)
                }
            )
            
            # Detect SDG relevance
            extracted_content.sdg_relevance = self._detect_sdg_relevance(
                answer_content['text'] + ' ' + user_query
            )
            
            # Calculate quality score
            extracted_content.quality_score = self._calculate_perplexity_quality_score(
                extracted_content, answer_content, citations
            )
            
            return [extracted_content]
            
        except Exception as e:
            logger.error(f"Error extracting Perplexity content from {source_url}: {e}")
            return []
    
    def _clean_soup(self, soup: BeautifulSoup):
        """Remove unwanted elements"""
        for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
            element.decompose()
        
        for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):
            comment.extract()
    
    def _extract_user_query(self, soup: BeautifulSoup) -> str:
        """Extract the user's search query"""
        # Try different selectors for query
        for selector in self.perplexity_selectors["query_containers"]:
            query_elem = soup.select_one(selector)
            if query_elem:
                query_text = query_elem.get_text(strip=True)
                if len(query_text) > 5:
                    return query_text
        
        # Fallback: look for query in page title or meta tags
        title_elem = soup.find('title')
        if title_elem:
            title_text = title_elem.get_text().strip()
            # Remove "Perplexity" branding
            title_text = re.sub(r'\s*-\s*Perplexity.*$', '', title_text, flags=re.IGNORECASE)
            if len(title_text) > 10:
                return title_text
        
        return ""
    
    def _extract_main_answer(self, soup: BeautifulSoup, source_url: str) -> Optional[Dict[str, Any]]:
        """Extract the main answer content"""
        # Try different selectors for answer content
        answer_elem = None
        for selector in self.perplexity_selectors["answer_containers"]:
            found_elem = soup.select_one(selector)
            if found_elem:
                answer_elem = found_elem
                break
        
        if not answer_elem:
            # Fallback: look for largest text block
            text_blocks = soup.find_all(['div', 'p', 'article'], string=True)
            if text_blocks:
                # Find the longest text block
                answer_elem = max(text_blocks, key=lambda x: len(x.get_text(strip=True)))
        
        if not answer_elem:
            return None
        
        # Extract text content
        answer_text = answer_elem.get_text(separator=' ', strip=True)
        
        if len(answer_text) < 50:
            return None
        
        # Analyze structure
        structure_type = self._analyze_answer_structure(answer_elem)
        
        # Extract any embedded lists or structured data
        lists = answer_elem.find_all(['ul', 'ol'])
        tables = answer_elem.find_all('table')
        
        return {
            'text': answer_text,
            'structure_type': structure_type,
            'has_lists': len(lists) > 0,
            'has_tables': len(tables) > 0,
            'word_count': len(answer_text.split())
        }
    
    def _analyze_answer_structure(self, answer_elem) -> str:
        """Analyze the structure of the answer"""
        # Check for lists
        if answer_elem.find_all(['ul', 'ol']):
            return "structured_list"
        
        # Check for tables
        if answer_elem.find_all('table'):
            return "tabular_data"
        
        # Check for multiple paragraphs
        paragraphs = answer_elem.find_all('p')
        if len(paragraphs) > 2:
            return "multi_paragraph"
        
        return "prose"
    
    def _extract_citations(self, soup: BeautifulSoup, base_url: str) -> List[Dict[str, Any]]:
        """Extract source citations and references"""
        citations = []
        
        # Method 1: Find citation links within text
        citation_links = soup.select('a[href*="source"], sup a, [data-citation] a')
        for link in citation_links:
            href = link.get('href')
            if href:
                citation_data = self._process_citation_link(link, href, base_url)
                if citation_data:
                    citations.append(citation_data)
        
        # Method 2: Find sources section
        for selector in self.perplexity_selectors["source_list"]:
            sources_section = soup.select_one(selector)
            if sources_section:
                source_links = sources_section.find_all('a', href=True)
                for link in source_links:
                    href = link['href']
                    citation_data = self._process_citation_link(link, href, base_url)
                    if citation_data:
                        citations.append(citation_data)
        
        # Method 3: Look for numbered references
        numbered_refs = soup.find_all(string=re.compile(r'\[\d+\]'))
        for ref in numbered_refs:
            # Try to find associated link
            parent = ref.parent if ref.parent else None
            if parent:
                link = parent.find('a', href=True)
                if link:
                    citation_data = self._process_citation_link(link, link['href'], base_url)
                    if citation_data:
                        citations.append(citation_data)
        
        # Remove duplicates and sort by relevance
        seen_urls = set()
        unique_citations = []
        for citation in citations:
            if citation['url'] not in seen_urls:
                seen_urls.add(citation['url'])
                unique_citations.append(citation)
        
        # Sort by quality score
        unique_citations.sort(key=lambda x: x.get('quality_score', 0), reverse=True)
        
        return unique_citations[:15]  # Limit to most relevant citations
    
    def _process_citation_link(self, link_elem, href: str, base_url: str) -> Optional[Dict[str, Any]]:
        """Process individual citation link"""
        # Skip invalid links
        if not href or href.startswith(('javascript:', '#', 'mailto:')):
            return None
        
        # Convert relative URLs
        if href.startswith('/'):
            href = urljoin(base_url, href)
        elif not href.startswith(('http://', 'https://')):
            return None
        
        # Extract link text and title
        link_text = link_elem.get_text(strip=True)
        link_title = link_elem.get('title', '')
        
        # Get domain information
        parsed_url = urlparse(href)
        domain = parsed_url.netloc
        
        # Assess source quality
        quality_score = self._assess_source_quality(href, link_text, domain)
        
        # Categorize source type
        source_type = self._categorize_source(href, link_text, domain)
        
        # Check for download potential
        is_download = self._is_potential_download(href, link_text)
        
        return {
            'url': href,
            'text': link_text,
            'title': link_title,
            'domain': domain,
            'source_type': source_type,
            'quality_score': quality_score,
            'is_download': is_download,
            'sdg_relevance': self._assess_citation_sdg_relevance(href, link_text)
        }
    
    def _assess_source_quality(self, url: str, text: str, domain: str) -> float:
        """Assess the quality of a citation source"""
        score = 0.0
        
        # High-quality domains
        quality_domains = [
            'un.org', 'who.int', 'worldbank.org', 'oecd.org', 'unesco.org',
            'unicef.org', 'undp.org', 'wto.org', 'imf.org', 'europa.eu',
            '.gov', '.edu', '.org'
        ]
        
        for quality_domain in quality_domains:
            if quality_domain in domain:
                score += 0.4
                break
        
        # Academic/research indicators
        academic_indicators = ['journal', 'research', 'study', 'paper', 'academic']
        if any(indicator in text.lower() or indicator in url.lower() for indicator in academic_indicators):
            score += 0.2
        
        # Data/statistics indicators
        data_indicators = ['data', 'statistics', 'report', 'dataset']
        if any(indicator in text.lower() or indicator in url.lower() for indicator in data_indicators):
            score += 0.2
        
        # Recent publication indicators
        if re.search(r'20(2[0-5])', url + text):  # 2020-2025
            score += 0.1
        
        # PDF or document indicators (often more substantial)
        if any(ext in url.lower() for ext in ['.pdf', '.doc', '.report']):
            score += 0.1
        
        return min(score, 1.0)
    
    def _categorize_source(self, url: str, text: str, domain: str) -> str:
        """Categorize the type of source"""
        url_lower = url.lower()
        text_lower = text.lower()
        
        # Government/Official
        if any(indicator in domain for indicator in ['.gov', 'un.org', 'who.int', 'worldbank.org']):
            return "official_government"
        
        # Academic/Research
        if any(indicator in text_lower for indicator in ['journal', 'research', 'study', 'academic']):
            return "academic_research"
        
        # News/Media
        if any(indicator in domain for indicator in ['news', 'times', 'post', 'guardian', 'reuters']):
            return "news_media"
        
        # NGO/Organization
        if '.org' in domain:
            return "organization_ngo"
        
        # Data/Statistics
        if any(indicator in text_lower for indicator in ['data', 'statistics', 'dataset']):
            return "data_statistics"
        
        return "general_web"
    
    def _is_potential_download(self, url: str, text: str) -> bool:
        """Check if citation might be a downloadable resource"""
        download_indicators = [
            '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx',
            '.csv', 'download', 'report', 'dataset', 'attachment'
        ]
        
        combined = f"{url.lower()} {text.lower()}"
        return any(indicator in combined for indicator in download_indicators)
    
    def _assess_citation_sdg_relevance(self, url: str, text: str) -> float:
        """Assess SDG relevance of citation"""
        score = 0.0
        combined = f"{url.lower()} {text.lower()}"
        
        # Direct SDG mentions
        sdg_terms = ['sdg', 'sustainable development', 'agenda 2030', 'goal', 'target']
        score += sum(0.2 for term in sdg_terms if term in combined)
        
        # SDG-related organizations
        sdg_orgs = ['un.org', 'undp.org', 'sustainabledevelopment', 'sdgs.un.org']
        if any(org in url.lower() for org in sdg_orgs):
            score += 0.3
        
        return min(score, 1.0)
    
    def _extract_follow_up_questions(self, soup: BeautifulSoup) -> List[str]:
        """Extract follow-up or related questions"""
        questions = []
        
        for selector in self.perplexity_selectors["follow_up_questions"]:
            questions_section = soup.select_one(selector)
            if questions_section:
                # Extract individual questions
                question_elements = questions_section.find_all(['li', 'div', 'p'])
                for elem in question_elements:
                    question_text = elem.get_text(strip=True)
                    if len(question_text) > 10 and question_text.endswith('?'):
                        questions.append(question_text)
        
        return questions[:5]  # Limit to 5 follow-up questions
    
    def _generate_title(self, user_query: str, answer_content: str) -> str:
        """Generate title from query and answer"""
        if user_query and len(user_query) > 5:
            return user_query[:200]
        
        # Fallback to first sentence of answer
        sentences = answer_content.split('.')
        first_sentence = sentences[0].strip() if sentences else answer_content[:100]
        return first_sentence[:150]
    
    def _create_summary(self, content: str) -> str:
        """Create summary from Perplexity answer"""
        # Take first 2-3 sentences
        sentences = re.split(r'[.!?]', content)
        summary_sentences = []
        char_count = 0
        
        for sentence in sentences[:3]:
            sentence = sentence.strip()
            if sentence and char_count + len(sentence) <= 250:
                summary_sentences.append(sentence)
                char_count += len(sentence)
            else:
                break
        
        summary = '. '.join(summary_sentences)
        return summary + '.' if summary and not summary.endswith('.') else summary
    
    def _detect_sdg_relevance(self, content: str) -> List[int]:
        """Detect SDG relevance in content"""
        # Enhanced SDG detection patterns
        sdg_patterns = {
            1: r'\b(poverty|poor|income.inequality|wealth.gap|social.protection|basic.needs)\b',
            2: r'\b(hunger|food.security|malnutrition|agriculture|farming|crop.yield)\b',
            3: r'\b(health|healthcare|medical|disease|mortality|wellbeing|pandemic)\b',
            4: r'\b(education|school|learning|literacy|skills|training|university)\b',
            5: r'\b(gender|women|girls|equality|empowerment|discrimination)\b',
            6: r'\b(water|sanitation|hygiene|drinking.water|clean.water|wastewater)\b',
            7: r'\b(energy|renewable|solar|wind|electricity|clean.energy|fossil)\b',
            8: r'\b(employment|jobs|economic.growth|decent.work|unemployment|labor)\b',
            9: r'\b(infrastructure|innovation|industry|technology|research|development)\b',
            10: r'\b(inequality|inclusion|discrimination|equity|marginalized|income.gap)\b',
            11: r'\b(cities|urban|housing|transport|sustainable.cities|smart.city)\b',
            12: r'\b(consumption|production|waste|recycling|sustainable|circular.economy)\b',
            13: r'\b(climate|carbon|emission|greenhouse|global.warming|adaptation|mitigation)\b',
            14: r'\b(ocean|marine|sea|fisheries|aquatic|coral|plastic.pollution)\b',
            15: r'\b(forest|biodiversity|ecosystem|wildlife|conservation|deforestation)\b',
            16: r'\b(peace|justice|institutions|governance|rule.of.law|corruption)\b',
            17: r'\b(partnership|cooperation|global|development.finance|aid|collaboration)\b'
        }
        
        content_lower = re.sub(r'[^\w\s]', ' ', content.lower())
        relevant_sdgs = []
        
        for sdg_id, pattern in sdg_patterns.items():
            if re.search(pattern, content_lower):
                relevant_sdgs.append(sdg_id)
        
        return relevant_sdgs
    
    def _count_high_quality_sources(self, citations: List[Dict[str, Any]]) -> int:
        """Count high-quality sources in citations"""
        return len([c for c in citations if c.get('quality_score', 0) > 0.6])
    
    def _calculate_perplexity_quality_score(self, content: ExtractedContent,
                                          answer_data: Dict[str, Any],
                                          citations: List[Dict[str, Any]]) -> float:
        """Calculate quality score for Perplexity content"""
        score = 0.0
        
        # Answer quality (0-0.4)
        word_count = answer_data.get('word_count', 0)
        if word_count > 300:
            score += 0.4
        elif word_count > 150:
            score += 0.25
        elif word_count > 75:
            score += 0.1
        
        # Structure quality (0-0.2)
        if answer_data.get('has_lists'):
            score += 0.1
        if answer_data.get('structure_type') in ['structured_list', 'tabular_data']:
            score += 0.1
        
        # Citation quality (0-0.3)
        high_quality_citations = self._count_high_quality_sources(citations)
        if high_quality_citations >= 3:
            score += 0.3
        elif high_quality_citations >= 1:
            score += 0.15
        
        # SDG relevance (0-0.1)
        if len(content.sdg_relevance) >= 2:
            score += 0.1
        elif len(content.sdg_relevance) >= 1:
            score += 0.05
        
        return min(score, 1.0)
        
-- newsletter_extractor.py

"""
Newsletter content extractor
Handles email newsletters and similar formatted content
"""
import logging
from typing import List, Dict, Any, Optional
import re
from bs4 import BeautifulSoup
from .base_extractor import BaseExtractor, ExtractedContent
from .web_extractor import WebExtractor

logger = logging.getLogger(__name__)

class NewsletterExtractor(WebExtractor):
    """
    Newsletter content extractor
    Extends WebExtractor with newsletter-specific logic
    """
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.newsletter_patterns = self._load_newsletter_patterns()
    
    def _load_newsletter_patterns(self) -> Dict[str, List[str]]:
        """Load newsletter-specific patterns"""
        return {
            "newsletter_indicators": [
                "newsletter",
                "weekly update",
                "monthly digest",
                "news digest",
                "bulletin",
                "briefing"
            ],
            "content_sections": [
                ".newsletter-content",
                ".email-content",
                ".digest-content", 
                ".bulletin-content",
                "table[role='presentation']",
                "td.content"
            ],
            "article_sections": [
                ".article-item",
                ".news-item",
                ".digest-item",
                ".story",
                "tr.article",
                "div[style*='border']"
            ]
        }
    
    def validate_source(self, source_url: str) -> bool:
        """Validate if source is newsletter-like"""
        newsletter_indicators = self.newsletter_patterns["newsletter_indicators"]
        url_lower = source_url.lower()
        
        # Check URL for newsletter indicators
        if any(indicator in url_lower for indicator in newsletter_indicators):
            return True
        
        # Also validate as general web content
        return super().validate_source(source_url)
    
    async def extract(self, source_url: str, **kwargs) -> List[ExtractedContent]:
        """Extract newsletter content with multi-article support"""
        try:
            # Use parent class to fetch and clean content
            response = await self.fetch_with_retry(source_url)
            if not response:
                return []
            
            html_content = await response.text()
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Check if this is actually a newsletter
            if not self._is_newsletter_content(soup, source_url):
                # Fall back to regular web extraction
                return await super().extract(source_url, **kwargs)
            
            # Clean soup
            self._clean_soup(soup)
            
            # Extract multiple articles from newsletter
            articles = self._extract_newsletter_articles(soup, source_url)
            
            if not articles:
                # Fall back to single content extraction
                single_content = self._extract_structured_content(soup, source_url)
                if single_content:
                    single_content.source_type = "newsletter"
                    return [single_content]
            
            return articles
            
        except Exception as e:
            logger.error(f"Error extracting newsletter from {source_url}: {e}")
            return []
    
    def _is_newsletter_content(self, soup: BeautifulSoup, source_url: str) -> bool:
        """Determine if content is newsletter-like"""
        # Check URL
        url_lower = source_url.lower()
        newsletter_indicators = self.newsletter_patterns["newsletter_indicators"]
        if any(indicator in url_lower for indicator in newsletter_indicators):
            return True
        
        # Check page content
        page_text = soup.get_text().lower()
        if any(indicator in page_text[:500] for indicator in newsletter_indicators):
            return True
        
        # Check for newsletter-like structure
        # Multiple article-like sections
        article_sections = soup.select('.article, .story, .news-item, .digest-item')
        if len(article_sections) >= 3:
            return True
        
        # Email-like table structure
        if soup.find('table', {'role': 'presentation'}):
            return True
        
        # Newsletter-specific meta tags
        newsletter_meta = soup.find('meta', attrs={
            'name': lambda x: x and 'newsletter' in x.lower() if x else False
        })
        if newsletter_meta:
            return True
        
        return False
    
    def _extract_newsletter_articles(self, soup: BeautifulSoup, source_url: str) -> List[ExtractedContent]:
        """Extract individual articles from newsletter"""
        articles = []
        
        # Try different strategies to find article sections
        article_sections = self._find_article_sections(soup)
        
        newsletter_metadata = self._extract_newsletter_metadata(soup, source_url)
        
        for i, section in enumerate(article_sections):
            article = self._extract_newsletter_article(section, source_url, i, newsletter_metadata)
            if article:
                articles.append(article)
        
        return articles
    
    def _find_article_sections(self, soup: BeautifulSoup) -> List:
        """Find individual article sections in newsletter"""
        sections = []
        
        # Strategy 1: Use newsletter-specific selectors
        for selector in self.newsletter_patterns["article_sections"]:
            found_sections = soup.select(selector)
            if found_sections and len(found_sections) >= 2:
                return found_sections
        
        # Strategy 2: Find repeating patterns
        # Look for multiple divs/sections with similar structure
        potential_sections = []
        
        # Try divs with similar classes
        all_divs = soup.find_all('div', class_=True)
        class_groups = {}
        
        for div in all_divs:
            classes = ' '.join(div.get('class', []))
            if classes:
                class_groups.setdefault(classes, []).append(div)
        
        # Find groups with multiple similar elements
        for class_name, divs in class_groups.items():
            if len(divs) >= 3:  # At least 3 similar sections
                # Check if they contain substantial content
                content_divs = []
                for div in divs:
                    text = div.get_text(strip=True)
                    if len(text) > 100:  # Minimum content length
                        content_divs.append(div)
                
                if len(content_divs) >= 2:
                    potential_sections = content_divs
                    break
        
        # Strategy 3: Table rows (email newsletters often use tables)
        if not potential_sections:
            table_rows = soup.find_all('tr')
            content_rows = []
            
            for row in table_rows:
                text = row.get_text(strip=True)
                if len(text) > 100 and not self._is_header_footer_row(row):
                    content_rows.append(row)
            
            if len(content_rows) >= 2:
                potential_sections = content_rows
        
        # Strategy 4: Paragraphs with headers
        if not potential_sections:
            # Find h2/h3 elements that might be article headers
            headers = soup.find_all(['h2', 'h3', 'h4'])
            for header in headers:
                # Get content after header until next header
                article_content = []
                current = header.next_sibling
                
                while current:
                    if hasattr(current, 'name') and current.name in ['h1', 'h2', 'h3', 'h4']:
                        break
                    if hasattr(current, 'get_text'):
                        text = current.get_text(strip=True)
                        if text:
                            article_content.append(current)
                    current = current.next_sibling
                
                if article_content:
                    # Create a wrapper for this article section
                    wrapper = soup.new_tag('div')
                    wrapper.append(header)
                    for content in article_content:
                        wrapper.append(content)
                    potential_sections.append(wrapper)
        
        return potential_sections[:20]  # Limit number of sections
    
    def _is_header_footer_row(self, row) -> bool:
        """Check if table row is header/footer content"""
        text = row.get_text().lower()
        header_footer_indicators = [
            'unsubscribe', 'privacy policy', 'terms of service',
            'follow us', 'social media', 'contact us', 'newsletter',
            'view in browser', 'email preferences', 'copyright'
        ]
        
        return any(indicator in text for indicator in header_footer_indicators)
    
    def _extract_newsletter_article(self, section, source_url: str, index: int, 
                                  newsletter_metadata: Dict[str, Any]) -> Optional[ExtractedContent]:
        """Extract individual article from newsletter section"""
        try:
            # Extract title
            title = self._extract_section_title(section)
            if not title:
                title = f"Newsletter Article {index + 1}"
            
            # Extract content
            content = section.get_text(separator=' ', strip=True)
            if len(content) < 100:  # Minimum content length
                return None
            
            # Clean content
            content = re.sub(r'\s+', ' ', content).strip()
            
            # Extract links
            links = self._extract_section_links(section, source_url)
            
            # Generate article URL (use first link or base URL with anchor)
            article_url = source_url
            if links:
                article_url = links[0]['url']
            else:
                article_url = f"{source_url}#article-{index + 1}"
            
            # Create summary (first paragraph or truncated content)
            summary = self._extract_section_summary(section, content)
            
            # Combine newsletter metadata with article-specific metadata
            article_metadata = {
                **newsletter_metadata,
                "article_index": index + 1,
                "links": links,
                "section_type": "newsletter_article"
            }
            
            # Detect SDG relevance
            sdg_relevance = self._detect_sdg_keywords_newsletter(content + " " + title)
            
            article = ExtractedContent(
                title=title,
                content=content,
                summary=summary,
                url=article_url,
                source_type="newsletter",
                language=newsletter_metadata.get('language', 'en'),
                region=newsletter_metadata.get('region', ''),
                metadata=article_metadata,
                sdg_relevance=sdg_relevance
            )
            
            # Calculate quality score
            article.quality_score = self._calculate_newsletter_quality_score(article, section)
            
            return article
            
        except Exception as e:
            logger.error(f"Error extracting newsletter article {index}: {e}")
            return None
    
    def _extract_section_title(self, section) -> str:
        """Extract title from newsletter section"""
        # Look for header tags
        for header_tag in ['h1', 'h2', 'h3', 'h4', 'h5']:
            header = section.find(header_tag)
            if header:
                title = header.get_text(strip=True)
                if len(title) > 5:
                    return title[:200]
        
        # Look for bold/strong text at beginning
        bold_elements = section.find_all(['b', 'strong'])
        for bold in bold_elements:
            text = bold.get_text(strip=True)
            if len(text) > 10 and len(text) < 100:
                return text
        
        # Look for first sentence
        text = section.get_text(strip=True)
        sentences = re.split(r'[.!?]', text)
        if sentences and len(sentences[0]) > 10:
            return sentences[0][:200]
        
        return ""
    
    def _extract_section_links(self, section, base_url: str) -> List[Dict[str, str]]:
        """Extract links from newsletter section"""
        links = []
        for link in section.find_all('a', href=True):
            href = link['href']
            text = link.get_text(strip=True)
            
            # Convert relative URLs to absolute
            if href.startswith('/'):
                from urllib.parse import urljoin, urlparse
                base_domain = f"{urlparse(base_url).scheme}://{urlparse(base_url).netloc}"
                href = urljoin(base_domain, href)
            elif not href.startswith(('http://', 'https://')):
                continue
            
            if text and href:
                links.append({
                    'url': href,
                    'text': text,
                    'type': 'article_link'
                })
        
        return links[:5]  # Limit links per article
    
    def _extract_section_summary(self, section, content: str) -> str:
        """Extract or generate summary for newsletter section"""
        # Look for explicit summary/excerpt
        summary_selectors = ['.summary', '.excerpt', '.intro', '.lead']
        for selector in summary_selectors:
            summary_elem = section.select_one(selector)
            if summary_elem:
                summary = summary_elem.get_text(strip=True)
                if len(summary) > 50:
                    return summary[:400]
        
        # Use first paragraph if available
        paragraphs = section.find_all('p')
        if paragraphs:
            first_p = paragraphs[0].get_text(strip=True)
            if len(first_p) > 50:
                return first_p[:400]
        
        # Fallback to truncated content
        return content[:300] + "..." if len(content) > 300 else content
    
    def _extract_newsletter_metadata(self, soup: BeautifulSoup, source_url: str) -> Dict[str, Any]:
        """Extract newsletter-level metadata"""
        metadata = {
            "source_type": "newsletter",
            "extraction_method": "newsletter_extractor"
        }
        
        # Newsletter title
        title_elem = soup.find('title')
        if title_elem:
            metadata['newsletter_title'] = title_elem.get_text(strip=True)
        
        # Newsletter date
        date_selectors = [
            'meta[name="date"]',
            'meta[property="article:published_time"]',
            '.newsletter-date',
            '.issue-date'
        ]
        
        for selector in date_selectors:
            date_elem = soup.select_one(selector)
            if date_elem:
                date_value = date_elem.get('content') or date_elem.get_text(strip=True)
                if date_value:
                    metadata['newsletter_date'] = date_value
                    break
        
        # Newsletter organization/sender
        org_selectors = [
            'meta[name="author"]',
            'meta[property="article:author"]',
            '.newsletter-from',
            '.sender'
        ]
        
        for selector in org_selectors:
            org_elem = soup.select_one(selector)
            if org_elem:
                org_value = org_elem.get('content') or org_elem.get_text(strip=True)
                if org_value:
                    metadata['newsletter_organization'] = org_value
                    break
        
        # Language detection
        html_elem = soup.find('html')
        if html_elem and html_elem.get('lang'):
            metadata['language'] = html_elem['lang'][:2]
        else:
            metadata['language'] = 'en'
        
        # Region detection (basic)
        content_text = soup.get_text()
        metadata['region'] = self._detect_region_from_content(content_text, soup)
        
        return metadata
    
    def _detect_sdg_keywords_newsletter(self, text: str) -> List[int]:
        """Enhanced SDG keyword detection for newsletters"""
        # Newsletter content often has more context, so use enhanced patterns
        enhanced_sdg_patterns = {
            1: r'\b(poverty|poor|low.income|wealth.gap|social.protection|basic.needs)\b',
            2: r'\b(hunger|food.security|malnutrition|agriculture|farming|crop)\b',
            3: r'\b(health|healthcare|medical|disease|mortality|wellbeing|pandemic)\b',
            4: r'\b(education|school|learning|literacy|skills|training|university)\b',
            5: r'\b(gender|women|girls|equality|empowerment|discrimination)\b',
            6: r'\b(water|sanitation|hygiene|drinking.water|clean.water)\b',
            7: r'\b(energy|renewable|solar|wind|electricity|clean.energy|fossil)\b',
            8: r'\b(employment|jobs|economic.growth|decent.work|unemployment)\b',
            9: r'\b(infrastructure|innovation|industry|technology|research|development)\b',
            10: r'\b(inequality|inclusion|discrimination|equity|marginalized)\b',
            11: r'\b(cities|urban|housing|transport|sustainable.cities|smart.city)\b',
            12: r'\b(consumption|production|waste|recycling|sustainable|circular.economy)\b',
            13: r'\b(climate|carbon|emission|greenhouse|global.warming|adaptation)\b',
            14: r'\b(ocean|marine|sea|fisheries|aquatic|coral|plastic.pollution)\b',
            15: r'\b(forest|biodiversity|ecosystem|wildlife|conservation|deforestation)\b',
            16: r'\b(peace|justice|institutions|governance|rule.of.law|corruption)\b',
            17: r'\b(partnership|cooperation|global|development.finance|aid)\b'
        }
        
        text_lower = text.lower()
        text_lower = re.sub(r'[^\w\s]', ' ', text_lower)  # Remove punctuation
        relevant_sdgs = []
        
        for sdg_id, pattern in enhanced_sdg_patterns.items():
            if re.search(pattern, text_lower):
                relevant_sdgs.append(sdg_id)
        
        return relevant_sdgs
    
    def _calculate_newsletter_quality_score(self, article: ExtractedContent, section) -> float:
        """Calculate quality score for newsletter article"""
        score = 0.0
        
        # Base quality score from parent
        base_score = self.estimate_quality_score(article)
        score += base_score * 0.6
        
        # Newsletter-specific quality factors
        
        # Has links (0-0.1)
        links = article.metadata.get('links', [])
        if links:
            score += 0.1
        
        # Content structure (0-0.1)
        # Check if section has proper HTML structure
        if section.find(['p', 'div', 'span']):
            score += 0.1
        
        # Newsletter organization info (0-0.1)
        if article.metadata.get('newsletter_organization'):
            score += 0.1
        
        # SDG relevance bonus (0-0.1)
        if len(article.sdg_relevance) >= 2:
            score += 0.1
        elif len(article.sdg_relevance) >= 1:
            score += 0.05
        
        return min(score, 1.0)
        
-- rss_extractor.py

"""
RSS Feed content extractor
"""
import logging
from typing import List, Dict, Any, Optional
import asyncio
import feedparser
from datetime import datetime
import re
from .base_extractor import BaseExtractor, ExtractedContent

logger = logging.getLogger(__name__)

class RSSExtractor(BaseExtractor):
    """
    RSS/Atom feed content extractor
    """
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.max_entries = config.get("max_entries_per_feed", 50)
    
    def validate_source(self, source_url: str) -> bool:
        """Validate RSS feed URL"""
        rss_indicators = ['/rss', '/feed', '.rss', '.xml', 'rss.xml', 'feed.xml']
        return any(indicator in source_url.lower() for indicator in rss_indicators)
    
    async def extract(self, source_url: str, **kwargs) -> List[ExtractedContent]:
        """Extract content from RSS feed"""
        try:
            # Fetch RSS feed
            response = await self.fetch_with_retry(source_url)
            if not response:
                return []
            
            feed_content = await response.text()
            
            # Parse feed
            feed = feedparser.parse(feed_content)
            
            if feed.bozo and hasattr(feed, 'bozo_exception'):
                logger.warning(f"RSS parsing issues for {source_url}: {feed.bozo_exception}")
            
            extracted_items = []
            
            # Process feed entries
            for entry in feed.entries[:self.max_entries]:
                content_item = self._process_feed_entry(entry, feed, source_url)
                if content_item:
                    extracted_items.append(content_item)
            
            logger.info(f"Extracted {len(extracted_items)} items from RSS feed: {source_url}")
            return extracted_items
            
        except Exception as e:
            logger.error(f"Error processing RSS feed {source_url}: {e}")
            return []
    
    def _process_feed_entry(self, entry: Any, feed: Any, source_url: str) -> Optional[ExtractedContent]:
        """Process individual RSS feed entry"""
        try:
            # Extract title
            title = getattr(entry, 'title', '').strip()
            if not title:
                return None
            
            # Extract content
            content = self._extract_entry_content(entry)
            if not content or len(content.strip()) < 50:
                return None
            
            # Extract summary
            summary = getattr(entry, 'summary', '')
            if not summary:
                summary = content[:300] + "..." if len(content) > 300 else content
            
            # Extract URL
            entry_url = getattr(entry, 'link', source_url)
            
            # Extract publication date
            pub_date = self._extract_publication_date(entry)
            
            # Extract metadata
            metadata = self._extract_entry_metadata(entry, feed)
            
            # Detect language and region
            language = self._detect_entry_language(entry, feed)
            region = self._detect_entry_region(content, metadata)
            
            content_item = ExtractedContent(
                title=title[:200],  # Limit title length
                content=content,
                summary=summary[:500],  # Limit summary length
                url=entry_url,
                source_type="rss_feed",
                language=language,
                region=region,
                extracted_at=datetime.utcnow(),
                metadata=metadata
            )
            
            # Calculate quality score
            content_item.quality_score = self._calculate_rss_quality_score(content_item, entry)
            
            # Simple SDG relevance detection
            content_item.sdg_relevance = self._detect_sdg_keywords(content)
            
            return content_item
            
        except Exception as e:
            logger.error(f"Error processing RSS entry: {e}")
            return None
    
    def _extract_entry_content(self, entry: Any) -> str:
        """Extract content from RSS entry"""
        # Try different content fields
        content_fields = ['content', 'description', 'summary']
        
        for field in content_fields:
            content_data = getattr(entry, field, None)
            if content_data:
                if isinstance(content_data, list) and content_data:
                    # Handle content list (like Atom feeds)
                    content_text = content_data[0].get('value', '')
                elif isinstance(content_data, dict):
                    content_text = content_data.get('value', '')
                else:
                    content_text = str(content_data)
                
                if content_text:
                    # Clean HTML tags
                    clean_content = re.sub(r'<[^>]+>', ' ', content_text)
                    clean_content = re.sub(r'\s+', ' ', clean_content).strip()
                    
                    if len(clean_content) > 100:
                        return clean_content
        
        return ""
    
    def _extract_publication_date(self, entry: Any) -> Optional[str]:
        """Extract publication date from entry"""
        date_fields = ['published_parsed', 'updated_parsed']
        
        for field in date_fields:
            date_data = getattr(entry, field, None)
            if date_data:
                try:
                    # Convert time struct to ISO format
                    dt = datetime(*date_data[:6])
                    return dt.isoformat()
                except:
                    continue
        
        # Try string fields as fallback
        string_date_fields = ['published', 'updated']
        for field in string_date_fields:
            date_str = getattr(entry, field, '')
            if date_str:
                return date_str
        
        return None
    
    def _extract_entry_metadata(self, entry: Any, feed: Any) -> Dict[str, Any]:
        """Extract metadata from RSS entry and feed"""
        metadata = {
            "feed_title": getattr(feed.feed, 'title', ''),
            "feed_description": getattr(feed.feed, 'description', ''),
            "feed_url": getattr(feed.feed, 'link', ''),
            "entry_id": getattr(entry, 'id', ''),
            "extraction_method": "rss_feed"
        }
        
        # Author information
        author = getattr(entry, 'author', '')
        if author:
            metadata['author'] = author
        
        # Tags/categories
        tags = getattr(entry, 'tags', [])
        if tags:
            tag_list = []
            for tag in tags:
                if isinstance(tag, dict):
                    tag_list.append(tag.get('term', ''))
                else:
                    tag_list.append(str(tag))
            metadata['tags'] = [t for t in tag_list if t]
        
        # Publication date
        pub_date = self._extract_publication_date(entry)
        if pub_date:
            metadata['publication_date'] = pub_date
        
        # Additional fields
        for field in ['rights', 'publisher']:
            value = getattr(entry, field, '')
            if value:
                metadata[field] = value
        
        return metadata
    
    def _detect_entry_language(self, entry: Any, feed: Any) -> str:
        """Detect language from entry or feed"""
        # Check entry language
        entry_lang = getattr(entry, 'language', '')
        if entry_lang:
            return entry_lang[:2]
        
        # Check feed language
        feed_lang = getattr(feed.feed, 'language', '')
        if feed_lang:
            return feed_lang[:2]
        
        # Simple detection based on content
        title = getattr(entry, 'title', '')
        content = self._extract_entry_content(entry)
        combined_text = f"{title} {content}".lower()
        
        # Basic language indicators
        if any(word in combined_text for word in ['the', 'and', 'is', 'in']):
            return 'en'
        elif any(word in combined_text for word in ['der', 'die', 'das', 'und']):
            return 'de'
        elif any(word in combined_text for word in ['le', 'de', 'et', 'dans']):
            return 'fr'
        
        return 'en'  # Default
    
    def _detect_entry_region(self, content: str, metadata: Dict[str, Any]) -> str:
        """Detect region from content and metadata"""
        # Check feed metadata first
        feed_url = metadata.get('feed_url', '')
        if feed_url:
            # Domain-based region detection
            domain_regions = {
                '.eu': 'EU',
                '.gov': 'USA',
                '.cn': 'China',
                '.in': 'India'
            }
            for domain, region in domain_regions.items():
                if domain in feed_url:
                    return region
        
        # Content-based detection
        region_indicators = {
            'EU': ['european', 'europe', 'brussels', 'eu'],
            'USA': ['america', 'united states', 'washington'],
            'China': ['china', 'chinese', 'beijing'],
            'India': ['india', 'indian', 'delhi']
        }
        
        content_lower = content.lower()
        for region, keywords in region_indicators.items():
            if any(keyword in content_lower for keyword in keywords):
                return region
        
        return ""
    
    def _calculate_rss_quality_score(self, content: ExtractedContent, entry: Any) -> float:
        """Calculate quality score for RSS content"""
        score = 0.0
        
        # Title quality (0-0.2)
        if content.title and len(content.title.strip()) > 20:
            score += 0.2
        elif content.title and len(content.title.strip()) > 10:
            score += 0.1
        
        # Content length (0-0.3)
        content_length = len(content.content)
        if content_length > 1000:
            score += 0.3
        elif content_length > 500:
            score += 0.2
        elif content_length > 200:
            score += 0.1
        
        # Has author (0-0.1)
        if content.metadata.get('author'):
            score += 0.1
        
        # Has publication date (0-0.1)
        if content.metadata.get('publication_date'):
            score += 0.1
        
        # Has tags/categories (0-0.1)
        if content.metadata.get('tags'):
            score += 0.1
        
        # Has valid URL (0-0.1)
        if content.url and content.url.startswith(('http://', 'https://')):
            score += 0.1
        
        # SDG relevance (0-0.1)
        if content.sdg_relevance:
            score += 0.1
        
        return min(score, 1.0)
    
    def _detect_sdg_keywords(self, content: str) -> List[int]:
        """Simple SDG keyword detection"""
        sdg_patterns = {
            1: r'\b(poverty|poor|income|wealth|social protection)\b',
            2: r'\b(hunger|food|nutrition|agriculture|farming)\b',
            3: r'\b(health|medical|disease|mortality|healthcare)\b',
            4: r'\b(education|learning|school|literacy|skills)\b',
            5: r'\b(gender|women|girls|equality|empowerment)\b',
            6: r'\b(water|sanitation|hygiene|drinking water)\b',
            7: r'\b(energy|renewable|electricity|clean energy)\b',
            8: r'\b(employment|jobs|economic growth|decent work)\b',
            9: r'\b(infrastructure|innovation|industry|technology)\b',
            10: r'\b(inequality|inclusion|discrimination|equity)\b',
            11: r'\b(cities|urban|housing|transport|sustainable cities)\b',
            12: r'\b(consumption|production|waste|recycling|sustainable)\b',
            13: r'\b(climate|carbon|emission|greenhouse|adaptation)\b',
            14: r'\b(ocean|marine|sea|fisheries|aquatic)\b',
            15: r'\b(forest|biodiversity|ecosystem|wildlife|conservation)\b',
            16: r'\b(peace|justice|institutions|governance|rule of law)\b',
            17: r'\b(partnership|cooperation|global|development finance)\b'
        }
        
        content_lower = content.lower()
        relevant_sdgs = []
        
        for sdg_id, pattern in sdg_patterns.items():
            if re.search(pattern, content_lower):
                relevant_sdgs.append(sdg_id)
        
        return relevant_sdgs
    
    async def extract_multiple_feeds(self, feed_urls: List[str], **kwargs) -> List[ExtractedContent]:
        """Extract content from multiple RSS feeds concurrently"""
        all_content = []
        
        # Process feeds in batches to avoid overwhelming servers
        batch_size = self.config.get("concurrent_feeds", 3)
        
        for i in range(0, len(feed_urls), batch_size):
            batch_urls = feed_urls[i:i + batch_size]
            batch_results = await self.process_batch(batch_urls, **kwargs)
            all_content.extend(batch_results)
            
            # Brief pause between batches
            if i + batch_size < len(feed_urls):
                await asyncio.sleep(1)
        
        logger.info(f"Extracted {len(all_content)} total items from {len(feed_urls)} RSS feeds")
        return all_content
        
-- web_extractor.py

"""
General web scraping extractor for various websites
"""
import logging
from typing import List, Dict, Any, Optional
import asyncio
from bs4 import BeautifulSoup, Comment
import re
from urllib.parse import urljoin, urlparse
from .base_extractor import BaseExtractor, ExtractedContent

logger = logging.getLogger(__name__)

class WebExtractor(BaseExtractor):
    """
    General purpose web content extractor
    Handles various website types with smart content detection
    """
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.content_selectors = self._load_content_selectors()
        self.blocked_elements = ['script', 'style', 'nav', 'header', 'footer', 'aside']
    
    def _load_content_selectors(self) -> Dict[str, List[str]]:
        """Load CSS selectors for different website types"""
        return {
            "article": [
                "article", 
                ".article", 
                ".post", 
                ".content", 
                ".main-content",
                ".entry-content",
                "#content",
                ".page-content"
            ],
            "title": [
                "h1",
                ".title", 
                ".article-title", 
                ".post-title",
                "title"
            ],
            "summary": [
                ".summary", 
                ".excerpt", 
                ".abstract", 
                ".lead",
                ".intro"
            ],
            "metadata": [
                ".metadata", 
                ".post-meta", 
                ".article-meta",
                "time[datetime]",
                ".author",
                ".date"
            ]
        }
    
    def validate_source(self, source_url: str) -> bool:
        """Validate if URL is suitable for web scraping"""
        try:
            parsed = urlparse(source_url)
            if not parsed.scheme or not parsed.netloc:
                return False
            
            # Block certain file types
            blocked_extensions = ['.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx']
            if any(source_url.lower().endswith(ext) for ext in blocked_extensions):
                return False
            
            return True
        except:
            return False
    
    async def extract(self, source_url: str, **kwargs) -> List[ExtractedContent]:
        """Extract content from web page"""
        try:
            if not self.validate_source(source_url):
                logger.warning(f"Invalid source URL: {source_url}")
                return []
            
            # Fetch page content
            response = await self.fetch_with_retry(source_url)
            if not response:
                return []
            
            html_content = await response.text()
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Remove unwanted elements
            self._clean_soup(soup)
            
            # Extract structured content
            extracted_content = self._extract_structured_content(soup, source_url)
            
            if extracted_content:
                # Enhance with additional processing
                extracted_content = await self._enhance_content(extracted_content, soup)
                return [extracted_content]
            
            return []
            
        except Exception as e:
            logger.error(f"Error extracting from {source_url}: {e}")
            return []
    
    def _clean_soup(self, soup: BeautifulSoup):
        """Remove unwanted elements from soup"""
        # Remove blocked elements
        for element_name in self.blocked_elements:
            for element in soup.find_all(element_name):
                element.decompose()
        
        # Remove comments
        for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):
            comment.extract()
        
        # Remove elements with no text content
        for element in soup.find_all():
            if not element.get_text(strip=True):
                element.decompose()
    
    def _extract_structured_content(self, soup: BeautifulSoup, source_url: str) -> Optional[ExtractedContent]:
        """Extract structured content from soup"""
        # Extract title
        title = self._extract_title(soup)
        if not title:
            return None
        
        # Extract main content
        content = self._extract_main_content(soup)
        if not content or len(content.strip()) < 100:
            return None
        
        # Extract summary
        summary = self._extract_summary(soup)
        
        # Extract metadata
        metadata = self._extract_metadata(soup, source_url)
        
        # Detect language and region
        language = self._detect_language(soup, content)
        region = self._detect_region_from_content(content, soup)
        
        extracted_content = ExtractedContent(
            title=title,
            content=content,
            summary=summary,
            url=source_url,
            source_type="web_scraping",
            language=language,
            region=region,
            metadata=metadata
        )
        
        # Calculate quality score
        extracted_content.quality_score = self.estimate_quality_score(extracted_content)
        
        return extracted_content
    
    def _extract_title(self, soup: BeautifulSoup) -> str:
        """Extract page title"""
        for selector in self.content_selectors["title"]:
            elements = soup.select(selector)
            if elements:
                title = elements[0].get_text(strip=True)
                if title and len(title) > 10:
                    return title[:200]  # Limit title length
        
        # Fallback to page title
        title_tag = soup.find('title')
        if title_tag:
            return title_tag.get_text(strip=True)[:200]
        
        return ""
    
    def _extract_main_content(self, soup: BeautifulSoup) -> str:
        """Extract main content using multiple strategies"""
        content_parts = []
        
        # Strategy 1: Use content selectors
        for selector in self.content_selectors["article"]:
            elements = soup.select(selector)
            if elements:
                for element in elements:
                    text = element.get_text(separator=' ', strip=True)
                    if len(text) > 200:  # Minimum content length
                        content_parts.append(text)
                        break
                if content_parts:
                    break
        
        # Strategy 2: Find paragraphs with substantial content
        if not content_parts:
            paragraphs = soup.find_all('p')
            paragraph_texts = []
            for p in paragraphs:
                text = p.get_text(strip=True)
                if len(text) > 50:  # Minimum paragraph length
                    paragraph_texts.append(text)
            
            if len(paragraph_texts) >= 3:  # At least 3 substantial paragraphs
                content_parts = paragraph_texts
        
        # Strategy 3: Extract all text as fallback
        if not content_parts:
            body = soup.find('body')
            if body:
                text = body.get_text(separator=' ', strip=True)
                if len(text) > 500:
                    content_parts = [text]
        
        # Combine and clean content
        if content_parts:
            combined_content = '\n\n'.join(content_parts)
            # Clean up whitespace
            combined_content = re.sub(r'\s+', ' ', combined_content).strip()
            return combined_content
        
        return ""
    
    def _extract_summary(self, soup: BeautifulSoup) -> str:
        """Extract summary/excerpt"""
        for selector in self.content_selectors["summary"]:
            elements = soup.select(selector)
            if elements:
                summary = elements[0].get_text(strip=True)
                if summary and len(summary) > 50:
                    return summary[:500]  # Limit summary length
        
        # Fallback: first paragraph or meta description
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc and meta_desc.get('content'):
            return meta_desc['content'][:500]
        
        # First substantial paragraph
        paragraphs = soup.find_all('p')
        for p in paragraphs:
            text = p.get_text(strip=True)
            if len(text) > 100:
                return text[:500]
        
        return ""
    
    def _extract_metadata(self, soup: BeautifulSoup, source_url: str) -> Dict[str, Any]:
        """Extract metadata from page"""
        metadata = {
            "source_domain": urlparse(source_url).netloc,
            "extraction_method": "web_scraping"
        }
        
        # Meta tags
        meta_tags = soup.find_all('meta')
        for tag in meta_tags:
            name = tag.get('name') or tag.get('property')
            content = tag.get('content')
            if name and content:
                metadata[f"meta_{name}"] = content
        
        # Author
        author_selectors = ['.author', '[rel="author"]', '.byline']
        for selector in author_selectors:
            author_elem = soup.select_one(selector)
            if author_elem:
                metadata['author'] = author_elem.get_text(strip=True)
                break
        
        # Publication date
        date_selectors = ['time[datetime]', '.date', '.published']
        for selector in date_selectors:
            date_elem = soup.select_one(selector)
            if date_elem:
                date_text = date_elem.get('datetime') or date_elem.get_text(strip=True)
                if date_text:
                    metadata['publication_date'] = date_text
                    break
        
        # Word count
        content = soup.get_text()
        word_count = len(re.findall(r'\b\w+\b', content))
        metadata['word_count'] = word_count
        
        return metadata
    
    def _detect_language(self, soup: BeautifulSoup, content: str) -> str:
        """Detect content language"""
        # Check HTML lang attribute
        html_tag = soup.find('html')
        if html_tag and html_tag.get('lang'):
            lang = html_tag['lang'][:2]  # Get first 2 characters
            return lang
        
        # Simple language detection based on common words
        language_indicators = {
            'en': ['the', 'and', 'is', 'in', 'to', 'of', 'for'],
            'de': ['der', 'die', 'das', 'und', 'ist', 'in', 'zu'],
            'fr': ['le', 'de', 'et', 'à', 'un', 'il', 'être'],
            'es': ['el', 'de', 'que', 'y', 'a', 'en', 'un'],
            'zh': ['的', '是', '在', '了', '不', '与', '也'],
        }
        
        content_lower = content.lower()
        scores = {}
        
        for lang, indicators in language_indicators.items():
            score = sum(1 for word in indicators if word in content_lower)
            scores[lang] = score
        
        if scores:
            detected_lang = max(scores, key=scores.get)
            if scores[detected_lang] > 2:  # Minimum threshold
                return detected_lang
        
        return 'en'  # Default to English
    
    def _detect_region_from_content(self, content: str, soup: BeautifulSoup) -> str:
        """Detect region from content and metadata"""
        # Check meta tags first
        geo_meta = soup.find('meta', attrs={'name': 'geo.region'})
        if geo_meta:
            return geo_meta.get('content', '')
        
        # Region keywords detection
        region_keywords = {
            "EU": ["european union", "europe", "eu", "brussels", "eurostat"],
            "USA": ["united states", "america", "usa", "washington dc"],
            "China": ["china", "chinese", "beijing", "prc"],
            "India": ["india", "indian", "new delhi", "bharat"],
            "ASEAN": ["asean", "southeast asia", "southeast asian"],
            "BRICS": ["brics", "emerging economies"]
        }
        
        content_lower = content.lower()
        region_scores = {}
        
        for region, keywords in region_keywords.items():
            score = sum(1 for keyword in keywords if keyword in content_lower)
            if score > 0:
                region_scores[region] = score
        
        if region_scores:
            return max(region_scores, key=region_scores.get)
        
        return ""
    
    async def _enhance_content(self, content: ExtractedContent, soup: BeautifulSoup) -> ExtractedContent:
        """Enhance extracted content with additional processing"""
        # Extract links for further processing
        links = []
        for link in soup.find_all('a', href=True):
            href = link['href']
            if href.startswith(('http://', 'https://')):
                links.append({
                    'url': href,
                    'text': link.get_text(strip=True)
                })
        
        content.metadata['internal_links'] = links[:20]  # Limit links
        
        # Extract images
        images = []
        for img in soup.find_all('img', src=True):
            images.append({
                'src': img['src'],
                'alt': img.get('alt', ''),
                'title': img.get('title', '')
            })
        
        content.metadata['images'] = images[:10]  # Limit images
        
        # Simple SDG keyword detection for initial relevance
        content.sdg_relevance = self._detect_sdg_relevance(content.content)
        
        return content
    
    def _detect_sdg_relevance(self, content: str) -> List[int]:
        """Simple SDG relevance detection"""
        # Basic keyword-based SDG detection
        sdg_keywords = {
            1: ["poverty", "poor", "income"],
            2: ["hunger", "food", "agriculture"],
            3: ["health", "medical", "disease"],
            13: ["climate", "carbon", "emission", "greenhouse"],
            # Add more as needed
        }
        
        content_lower = content.lower()
        relevant_sdgs = []
        
        for sdg_id, keywords in sdg_keywords.items():
            if any(keyword in content_lower for keyword in keywords):
                relevant_sdgs.append(sdg_id)
        
        return relevant_sdgs
        
/sdg_root/src/content_extraction/processors

-- content_classifier.py

empty

-- html_processor.py

empty

-- quality_validator.py

empty


