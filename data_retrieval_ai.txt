/sdg_root/src/data_retrieval

-- Dockerfile

# sdg_root/src/data_retrieval/Dockerfile
FROM python:3.11-slim

# Installiere Python und ffmpeg
RUN apt-get update && apt-get install -y ffmpeg

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["python", "retrieval_worker.py/RetrievalWorker.run()"]


-- requirements.txt

weaviate-client>=4.0.0
requests
yt-dlp
sentence-transformers
faster-whisper
PyPDF2
python-docx
sqlalchemy
psycopg2-binary
pytest

-- main.py

from retrieval_worker import RetrievalWorker
import time

if __name__ == "__main__":
    # Konfiguration: Container-Pfade verwenden
    sources_file = "/app/quelle.txt"
    data_dir = "/app/raw_data"
    processed_file = "/app/processed_data/processed_data.json"

    worker = RetrievalWorker(
        sources_file=sources_file,
        data_dir=data_dir,
        processed_file=processed_file
    )

    print("Starting RetrievalWorker...")

    while True:
        try:
            worker.run()
            print("Warte 60 Minuten bis zum n√§chsten Zyklus...")
            time.sleep(3600)  # 60 Minuten
        except Exception as e:
            print(f"Fehler: {e}")
            time.sleep(60)   # 1 Minute bei Fehler

-- retrieval_worker.py

# src/data_retrieval/retrieval_worker.py

import os
import json
import requests
import csv
import fcntl
from datetime import datetime
from yt_dlp import YoutubeDL
from yt_dlp.utils import DownloadError, ExtractorError
from urllib.parse import quote
import logging
logging.basicConfig(level=logging.INFO)

class RetrievalWorker:
    def __init__(self, sources_file, data_dir, processed_file):
        self.sources_file = sources_file
        self.data_dir = data_dir
        self.processed_file = processed_file
        self.downloaded_urls_file = os.path.join(data_dir, "downloaded_urls.csv")

        print("---retrievalWorker instanz gestartet")
    
    def load_downloaded_urls(self):
        """Lade bereits heruntergeladene URLs aus CSV."""
        downloaded_urls = set()
        if os.path.exists(self.downloaded_urls_file):
            try:
                with open(self.downloaded_urls_file, 'r', encoding='utf-8') as f:
                    reader = csv.DictReader(f)
                    for row in reader:
                        downloaded_urls.add(row['url'])
                logging.info(f"‚úÖ {len(downloaded_urls)} bereits heruntergeladene URLs geladen")
            except Exception as e:
                logging.error(f"‚ùå Fehler beim Laden der URL-Historie: {e}")
        return downloaded_urls

    def save_downloaded_url(self, url, filename, status="success"):
        """Speichere erfolgreich heruntergeladene URL in CSV."""
        try:
            file_exists = os.path.exists(self.downloaded_urls_file)
            with open(self.downloaded_urls_file, 'a', encoding='utf-8', newline='') as f:
                fieldnames = ['url', 'filename', 'timestamp', 'status']
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                
                if not file_exists:
                    writer.writeheader()
                
                writer.writerow({
                    'url': url,
                    'filename': filename,
                    'timestamp': datetime.now().isoformat(),
                    'status': status
                })
            logging.info(f"‚úÖ URL gespeichert in Historie: {url}")
        except Exception as e:
            logging.error(f"‚ùå Fehler beim Speichern der URL: {e}")

    def run(self):
        all_urls = self.load_sources()
        downloaded_urls = self.load_downloaded_urls()
        processed_data = []
        new_urls = all_urls - downloaded_urls

        if not new_urls:
            logging.info("üîÑ Alle URLs bereits heruntergeladen. Keine neuen URLs zu verarbeiten.")
            return
        
        logging.info(f"üì• {len(new_urls)} neue URLs zu verarbeiten von {len(all_urls)} gesamt")

        for url in new_urls:
            try:
                if self.is_youtube(url):
                    item = self.download_youtube_content(url)
                else:
                    item = self.download_generic_content(url)
                if item:
                    processed_data.append(item)
                    self.save_downloaded_url(url, item['title'])
                    meta_filename = os.path.splitext(item['title'])[0].replace(" ", "_").replace("%20", "_") + ".json"
                    meta_path = os.path.join(self.data_dir, meta_filename)
                    with open(meta_path, "w", encoding="utf-8") as meta_f:
                        json.dump(item, meta_f, indent=2, ensure_ascii=False)
                    logging.info(f"‚úÖ Metadaten gespeichert: {meta_path}")
                else:
                    self.save_downloaded_url(url, "failed", "failed")
            except Exception as e:
                self.handle_errors(url, e)
                self.save_downloaded_url(url, "error", "error")
        self.save_to_file(processed_data)
        self.signal_processing(processed_data)

    def load_sources(self):
        all_urls = set()
        if os.path.exists(self.sources_file):
            with open(self.sources_file, "r", encoding="utf-8") as f:
                for line in f:
                    url = line.strip()
                    if url:
                        all_urls.add(url)
        logging.info(f"üìã {len(all_urls)} URLs aus Quellenliste geladen")
        return all_urls

    def is_youtube(self, url):
        return "youtube.com" in url or "youtu.be" in url

    def download_youtube_content(self, url):
        # Analog zu deinem main.py mit yt-dlp
        ydl_opts = {"skip_download": True, "quiet": True}
        try:
            with YoutubeDL(ydl_opts) as ydl:
                info = ydl.extract_info(url, download=False)
            return {
                "url": url,
                "title": info.get("title"),
                "description": info.get("description"),
                "metadata": info,
                "source_url": url
            }
        except (DownloadError, ExtractorError) as e:
            print(f"Error: {e}")
            return None

    def download_generic_content(self, url):
        logging.info(f"Starte Download: {url}")
        try:
            os.makedirs(self.data_dir, exist_ok=True)
            
            response = requests.get(url, timeout=15)
            response.raise_for_status()
            filename = os.path.basename(url)
            if not filename or '.' not in filename:
                filename = f"download_{abs(hash(url))}.pdf"
            file_path = os.path.join(self.data_dir, filename)
            with open(file_path, "wb") as f:
                fcntl.flock(f.fileno(), fcntl.LOCK_EX)
                f.write(response.content)
            file_size = os.path.getsize(file_path)
            logging.info(f"‚úÖ Download erfolgreich: {filename} ({file_size} Bytes)")
            return {
                "url": url,
                "title": filename,
                "file_path": file_path,
                "source_url": url
            }
        except Exception as e:
            logging.error(f"Download-Fehler bei {url}: {e}")
            print(f"Error: {e}")
            return None

    def save_to_file(self, data):
        with open(self.processed_file, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def signal_processing(self, processed_data):
        # Hier reicht in der Baseline das Ablegen der JSON (File-Drop)
        print(f"Signal f√ºr Processing-Service: {self.processed_file}")

    def handle_errors(self, url, error):
        print(f"Fehler bei {url}: {error}")

