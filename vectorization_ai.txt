/sdg_root/src/vectorization

-- Dockerfile

FROM python:3.11-slim

# Set working directory
WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Create non-root user
RUN useradd -m -u 1000 vectoruser && chown -R vectoruser:vectoruser /app
USER vectoruser

# Expose port
EXPOSE 8003

# Health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8003/health || exit 1

# Run the application
CMD ["python", "-m", "uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8003"]

-- requirements.txt

fastapi==0.104.1
uvicorn[standard]==0.24.0
sentence-transformers==2.2.2
transformers==4.36.0
torch>=1.9.0
numpy==1.24.3
scikit-learn==1.3.0
weaviate-client==3.25.3
openai==0.28.1
networkx==3.2
asyncio-mqtt==0.13.0
python-multipart==0.0.6
pydantic==2.5.0
python-jose[cryptography]==3.3.0

-- embedding_models.py

"""
Advanced embedding models for multilingual SDG content
Extracted and enhanced from your text_vektorizer.py and processing_logic.py
"""
import logging
from typing import List, Dict, Any, Optional, Union
import numpy as np
from sentence_transformers import SentenceTransformer
import openai
from transformers import AutoTokenizer, AutoModel
import torch
import torch.nn.functional as F
from abc import ABC, abstractmethod

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class BaseEmbeddingModel(ABC):
    """Abstract base class for embedding models"""
    
    @abstractmethod
    def encode(self, texts: Union[str, List[str]], **kwargs) -> np.ndarray:
        """Generate embeddings for input texts"""
        pass
    
    @abstractmethod
    def get_dimension(self) -> int:
        """Return embedding dimension"""
        pass

class SentenceTransformerModel(BaseEmbeddingModel):
    """Multilingual Sentence Transformer for SDG content"""
    
    def __init__(self, model_name: str = "paraphrase-multilingual-MiniLM-L12-v2"):
        self.model_name = model_name
        self.model = SentenceTransformer(model_name)
        self.dimension = self.model.get_sentence_embedding_dimension()
        logger.info(f"Loaded SentenceTransformer model: {model_name}")
    
    def encode(self, texts: Union[str, List[str]], 
               normalize_embeddings: bool = True,
               batch_size: int = 32,
               **kwargs) -> np.ndarray:
        """Generate embeddings with batch processing"""
        if isinstance(texts, str):
            texts = [texts]
        
        embeddings = self.model.encode(
            texts,
            normalize_embeddings=normalize_embeddings,
            batch_size=batch_size,
            show_progress_bar=len(texts) > 100,
            **kwargs
        )
        return embeddings
    
    def get_dimension(self) -> int:
        return self.dimension

class OpenAIEmbeddingModel(BaseEmbeddingModel):
    """OpenAI embeddings for high-quality SDG analysis"""
    
    def __init__(self, model_name: str = "text-embedding-ada-002", api_key: str = None):
        self.model_name = model_name
        self.dimension = 1536 if model_name == "text-embedding-ada-002" else 1536
        if api_key:
            openai.api_key = api_key
        logger.info(f"Initialized OpenAI embedding model: {model_name}")
    
    def encode(self, texts: Union[str, List[str]], **kwargs) -> np.ndarray:
        """Generate OpenAI embeddings"""
        if isinstance(texts, str):
            texts = [texts]
        
        try:
            response = openai.Embedding.create(
                input=texts,
                model=self.model_name
            )
            embeddings = np.array([item['embedding'] for item in response['data']])
            return embeddings
        except Exception as e:
            logger.error(f"OpenAI embedding error: {e}")
            raise
    
    def get_dimension(self) -> int:
        return self.dimension

class SDGSpecificModel(BaseEmbeddingModel):
    """Custom model fine-tuned for SDG content"""
    
    def __init__(self, model_path: str = "bert-base-multilingual-cased"):
        self.model_path = model_path
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModel.from_pretrained(model_path)
        self.dimension = self.model.config.hidden_size
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)
        logger.info(f"Loaded custom SDG model: {model_path}")
    
    def mean_pooling(self, model_output, attention_mask):
        """Mean pooling for sentence embeddings"""
        token_embeddings = model_output[0]
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)
    
    def encode(self, texts: Union[str, List[str]], 
               batch_size: int = 16,
               max_length: int = 512,
               **kwargs) -> np.ndarray:
        """Generate custom SDG embeddings"""
        if isinstance(texts, str):
            texts = [texts]
        
        all_embeddings = []
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            
            # Tokenize
            encoded_input = self.tokenizer(
                batch_texts,
                padding=True,
                truncation=True,
                max_length=max_length,
                return_tensors='pt'
            ).to(self.device)
            
            # Generate embeddings
            with torch.no_grad():
                model_output = self.model(**encoded_input)
                embeddings = self.mean_pooling(model_output, encoded_input['attention_mask'])
                embeddings = F.normalize(embeddings, p=2, dim=1)
                
            all_embeddings.extend(embeddings.cpu().numpy())
        
        return np.array(all_embeddings)
    
    def get_dimension(self) -> int:
        return self.dimension

class EmbeddingManager:
    """
    Manager for multiple embedding models with SDG-specific optimizations
    Enhanced version of your text_vektorizer.py functionality
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.models: Dict[str, BaseEmbeddingModel] = {}
        self.default_model = "sentence_transformer"
        
        # Initialize default models
        self._initialize_models()
    
    def _initialize_models(self):
        """Initialize available embedding models"""
        try:
            # Sentence Transformer (multilingual)
            self.models["sentence_transformer"] = SentenceTransformerModel(
                self.config.get("sentence_transformer_model", "paraphrase-multilingual-MiniLM-L12-v2")
            )
            
            # OpenAI (if API key provided)
            if self.config.get("openai_api_key"):
                self.models["openai"] = OpenAIEmbeddingModel(
                    api_key=self.config.get("openai_api_key")
                )
            
            # Custom SDG model
            self.models["sdg_custom"] = SDGSpecificModel(
                self.config.get("custom_model_path", "bert-base-multilingual-cased")
            )
            
            logger.info(f"Initialized {len(self.models)} embedding models")
            
        except Exception as e:
            logger.error(f"Error initializing models: {e}")
            raise
    
    def encode(self, 
               texts: Union[str, List[str]], 
               model_name: str = None,
               **kwargs) -> np.ndarray:
        """Generate embeddings using specified model"""
        model_name = model_name or self.default_model
        
        if model_name not in self.models:
            raise ValueError(f"Model {model_name} not available. Available: {list(self.models.keys())}")
        
        return self.models[model_name].encode(texts, **kwargs)
    
    def encode_sdg_content(self, 
                          content: str,
                          sdg_goals: List[int] = None,
                          language: str = "en",
                          **kwargs) -> Dict[str, Any]:
        """
        Enhanced SDG-specific encoding with metadata
        Integrates your SDG classification from keywords.py
        """
        # Generate base embeddings
        embeddings = self.encode(content, **kwargs)
        
        # Add SDG-specific metadata
        metadata = {
            "embedding": embeddings[0] if len(embeddings) == 1 else embeddings,
            "content_length": len(content),
            "language": language,
            "timestamp": np.datetime64('now'),
            "model_used": kwargs.get("model_name", self.default_model),
            "dimension": len(embeddings) if len(embeddings) > 0 else 0
        }
        
        # Add SDG classification if provided
        if sdg_goals:
            metadata["sdg_goals"] = sdg_goals
            metadata["primary_sdg"] = sdg_goals if sdg_goals else None
        
        return metadata
    
    def get_model_info(self, model_name: str = None) -> Dict[str, Any]:
        """Get information about embedding model"""
        model_name = model_name or self.default_model
        model = self.models.get(model_name)
        
        if not model:
            return {}
        
        return {
            "model_name": model_name,
            "dimension": model.get_dimension(),
            "type": type(model).__name__,
            "available": True
        }
    
    def batch_encode_with_progress(self, 
                                 texts: List[str], 
                                 batch_size: int = 100,
                                 model_name: str = None) -> List[np.ndarray]:
        """Batch encoding with progress tracking"""
        model_name = model_name or self.default_model
        results = []
        
        total_batches = (len(texts) + batch_size - 1) // batch_size
        logger.info(f"Processing {len(texts)} texts in {total_batches} batches")
        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            batch_embeddings = self.encode(batch, model_name=model_name)
            results.extend(batch_embeddings)
            
            batch_num = (i // batch_size) + 1
            logger.info(f"Completed batch {batch_num}/{total_batches}")
        
        return results

# Language-specific embedding configurations
LANGUAGE_MODEL_MAPPING = {
    "en": "paraphrase-multilingual-MiniLM-L12-v2",
    "de": "paraphrase-multilingual-MiniLM-L12-v2", 
    "fr": "paraphrase-multilingual-MiniLM-L12-v2",
    "es": "paraphrase-multilingual-MiniLM-L12-v2",
    "zh": "paraphrase-multilingual-MiniLM-L12-v2",
    "hi": "paraphrase-multilingual-MiniLM-L12-v2"
}

def get_optimal_model_for_language(language: str) -> str:
    """Get optimal embedding model for specific language"""
    return LANGUAGE_MODEL_MAPPING.get(language, "paraphrase-multilingual-MiniLM-L12-v2")
    
    
-- main.py

# src/vectorization/main.py
# src/vectorization/main.py
"""
SDG Vectorization Service - FastAPI Application
Microservice for embedding generation, vector storage, and semantic search
"""
import logging
from typing import List, Dict, Any, Optional
import asyncio
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException, BackgroundTasks, Depends, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
import numpy as np
import uvicorn

from .embedding_models import EmbeddingManager
from .vector_db_client import VectorDBClient, get_vector_client  
from .similarity_search import SimilaritySearch, SDGRecommendationEngine

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Pydantic models for API
class EmbeddingRequest(BaseModel):
    texts: List[str] = Field(..., description="List of texts to embed")
    model_name: Optional[str] = Field("sentence_transformer", description="Embedding model to use")
    normalize: bool = Field(True, description="Normalize embeddings")

class DocumentRequest(BaseModel):
    title: str = Field(..., description="Document title")
    content: str = Field(..., description="Document content")  
    summary: Optional[str] = Field(None, description="Document summary")
    sdg_goals: List[int] = Field(..., description="Related SDG goals")
    region: str = Field(..., description="Geographic region")
    language: str = Field("en", description="Content language")
    source_url: Optional[str] = Field(None, description="Source URL")
    metadata: Optional[Dict[str, Any]] = Field(None, description="Additional metadata")

class SearchRequest(BaseModel):
    query: str = Field(..., description="Search query")
    search_type: str = Field("general", description="Search type: general, high_quality, recent, comprehensive")
    language: Optional[str] = Field("en", description="Content language")
    region: Optional[str] = Field(None, description="Geographic region filter")
    sdg_goals: Optional[List[int]] = Field(None, description="SDG goals filter")
    limit: int = Field(10, description="Maximum results", ge=1, le=100)

class RecommendationRequest(BaseModel):
    user_interests: List[int] = Field(..., description="User's SDG interests (1-17)")
    region: Optional[str] = Field(None, description="User's region")
    language: str = Field("en", description="Preferred language")
    limit: int = Field(10, description="Number of recommendations", ge=1, le=50)

# Global service instances
embedding_manager: Optional[EmbeddingManager] = None
vector_client: Optional[VectorDBClient] = None
similarity_search: Optional[SimilaritySearch] = None
recommendation_engine: Optional[SDGRecommendationEngine] = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan management"""
    # Startup
    global embedding_manager, vector_client, similarity_search, recommendation_engine
    
    try:
        # Load configuration (in production, use proper config management)
        config = {
            "weaviate": {
                "url": "http://localhost:8080", 
                "embedded": False,
                "max_connections": 10
            },
            "embeddings": {
                "sentence_transformer_model": "paraphrase-multilingual-MiniLM-L12-v2",
                "openai_api_key": None  # Set from environment
            }
        }
        
        # Initialize services
        logger.info("Initializing Vectorization Service...")
        
        embedding_manager = EmbeddingManager(config.get("embeddings", {}))
        vector_client = VectorDBClient(config.get("weaviate", {}))
        similarity_search = SimilaritySearch(vector_client, embedding_manager)
        recommendation_engine = SDGRecommendationEngine(similarity_search)
        
        logger.info("Vectorization Service initialized successfully")
        yield
        
    except Exception as e:
        logger.error(f"Error during startup: {e}")
        raise
    finally:
        # Shutdown
        if vector_client:
            vector_client.close()
        logger.info("Vectorization Service shutdown complete")

# Initialize FastAPI app
app = FastAPI(
    title="SDG Vectorization Service",
    description="Microservice for SDG content embedding generation, vector storage, and semantic search",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc",
    lifespan=lifespan
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Configure appropriately for production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Dependency injection
async def get_embedding_manager() -> EmbeddingManager:
    if embedding_manager is None:
        raise HTTPException(status_code=503, detail="Embedding manager not initialized")
    return embedding_manager

async def get_vector_client_dep() -> VectorDBClient:
    if vector_client is None:
        raise HTTPException(status_code=503, detail="Vector client not initialized") 
    return vector_client

async def get_similarity_search() -> SimilaritySearch:
    if similarity_search is None:
        raise HTTPException(status_code=503, detail="Similarity search not initialized")
    return similarity_search

async def get_recommendation_engine() -> SDGRecommendationEngine:
    if recommendation_engine is None:
        raise HTTPException(status_code=503, detail="Recommendation engine not initialized")
    return recommendation_engine

# API Endpoints

@app.get("/health", tags=["Health"])
async def health_check():
    """Service health check endpoint"""
    try:
        # Check all service components
        embedding_health = embedding_manager is not None
        vector_health = vector_client.health_check() if vector_client else {"status": "unavailable"}
        search_health = similarity_search.health_check() if similarity_search else {"status": "unavailable"}
        
        overall_status = "healthy" if (
            embedding_health and 
            vector_health.get("status") == "healthy" and
            search_health.get("status") == "healthy"
        ) else "unhealthy"
        
        return {
            "status": overall_status,
            "service": "SDG Vectorization Service",
            "version": "1.0.0",
            "components": {
                "embedding_manager": "healthy" if embedding_health else "unhealthy",
                "vector_client": vector_health,
                "similarity_search": search_health
            }
        }
    except Exception as e:
        logger.error(f"Health check error: {e}")
        return JSONResponse(
            status_code=503,
            content={"status": "error", "error": str(e)}
        )

@app.post("/embeddings", tags=["Embeddings"])
async def generate_embeddings(
    request: EmbeddingRequest,
    embedding_mgr: EmbeddingManager = Depends(get_embedding_manager)
):
    """Generate embeddings for input texts"""
    try:
        embeddings = embedding_mgr.encode(
            texts=request.texts,
            model_name=request.model_name
        )
        
        # Convert numpy arrays to lists for JSON serialization
        embeddings_list = embeddings.tolist()
        
        return {
            "embeddings": embeddings_list,
            "dimension": len(embeddings_list[0]) if embeddings_list else 0,
            "model_used": request.model_name,
            "text_count": len(request.texts)
        }
        
    except Exception as e:
        logger.error(f"Error generating embeddings: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/documents", tags=["Documents"])
async def store_document(
    request: DocumentRequest,
    background_tasks: BackgroundTasks,
    embedding_mgr: EmbeddingManager = Depends(get_embedding_manager),
    vector_db: VectorDBClient = Depends(get_vector_client_dep)
):
    """Store document with embeddings in vector database"""
    try:
        # Generate embeddings for title and content
        content_for_embedding = f"{request.title}\n\n{request.summary or request.content[:1000]}"
        embeddings = embedding_mgr.encode_sdg_content(
            content=content_for_embedding,
            sdg_goals=request.sdg_goals,
            language=request.language
        )
        
        # Prepare document for storage
        document = {
            "title": request.title,
            "content": request.content,
            "summary": request.summary,
            "sdg_goals": request.sdg_goals,
            "region": request.region,
            "language": request.language,
            "source_url": request.source_url,
            "confidence_score": 0.8,  # Default confidence
            "publication_date": "2024-01-01T00:00:00Z",  # Should come from metadata
            "vector": embeddings["embedding"]
        }
        
        # Add any additional metadata
        if request.metadata:
            document.update(request.metadata)
        
        # Store document (async in background)
        background_tasks.add_task(
            vector_db.insert_embeddings,
            documents=[document],
            class_name="SDGArticle"
        )
        
        return {
            "status": "accepted",
            "message": "Document queued for storage",
            "document_id": f"pending_{hash(request.title)}",  # Generate proper ID in production
            "embedding_dimension": embeddings["dimension"]
        }
        
    except Exception as e:
        logger.error(f"Error storing document: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/search", tags=["Search"])
async def semantic_search(
    request: SearchRequest,
    search_service: SimilaritySearch = Depends(get_similarity_search)
):
    """Perform semantic search across SDG content"""
    try:
        results = await search_service.semantic_search(
            query=request.query,
            search_type=request.search_type,
            language=request.language,
            region=request.region,
            sdg_goals=request.sdg_goals,
            limit=request.limit
        )
        
        return results
        
    except Exception as e:
        logger.error(f"Error in semantic search: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/search/sdg/{sdg_goal}", tags=["Search"])
async def search_by_sdg_goal(
    sdg_goal: int,
    limit: int = 20,
    region: Optional[str] = None,
    vector_db: VectorDBClient = Depends(get_vector_client_dep)
):
    """Search for content related to specific SDG goal"""
    try:
        if not (1 <= sdg_goal <= 17):
            raise HTTPException(status_code=400, detail="SDG goal must be between 1 and 17")
        
        results = vector_db.search_by_sdg_goals(
            sdg_goals=[sdg_goal],
            limit=limit
        )
        
        # Filter by region if specified
        if region:
            results = [r for r in results if r.get("region") == region]
        
        return {
            "sdg_goal": sdg_goal,
            "region": region,
            "total_results": len(results),
            "results": results
        }
        
    except Exception as e:
        logger.error(f"Error searching by SDG goal: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/search/region/{region}", tags=["Search"])
async def search_by_region(
    region: str,
    limit: int = 30,
    vector_db: VectorDBClient = Depends(get_vector_client_dep)
):
    """Search for region-specific SDG content"""
    try:
        results = vector_db.search_by_region(
            region=region,
            limit=limit
        )
        
        return {
            "region": region,
            "total_results": len(results),
            "results": results
        }
        
    except Exception as e:
        logger.error(f"Error searching by region: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/recommendations", tags=["Recommendations"])
async def get_recommendations(
    request: RecommendationRequest,
    rec_engine: SDGRecommendationEngine = Depends(get_recommendation_engine)
):
    """Get personalized SDG content recommendations"""
    try:
        # Validate SDG goals
        invalid_goals = [g for g in request.user_interests if not (1 <= g <= 17)]
        if invalid_goals:
            raise HTTPException(
                status_code=400, 
                detail=f"Invalid SDG goals: {invalid_goals}. Must be between 1 and 17."
            )
        
        recommendations = await rec_engine.recommend_content(
            user_interests=request.user_interests,
            region=request.region,
            language=request.language,
            limit=request.limit
        )
        
        return recommendations
        
    except Exception as e:
        logger.error(f"Error generating recommendations: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/statistics", tags=["Analytics"])
async def get_statistics(
    vector_db: VectorDBClient = Depends(get_vector_client_dep)
):
    """Get vector database statistics"""
    try:
        stats = vector_db.get_statistics()
        return {
            "database_statistics": stats,
            "total_documents": sum(stats.values()),
            "available_classes": list(stats.keys())
        }
    except Exception as e:
        logger.error(f"Error getting statistics: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/models", tags=["Configuration"])
async def get_available_models(
    embedding_mgr: EmbeddingManager = Depends(get_embedding_manager)
):
    """Get available embedding models"""
    try:
        models_info = {}
        for model_name in embedding_mgr.models.keys():
            models_info[model_name] = embedding_mgr.get_model_info(model_name)
        
        return {
            "available_models": models_info,
            "default_model": embedding_mgr.default_model
        }
    except Exception as e:
        logger.error(f"Error getting model information: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# Error handlers
@app.exception_handler(ValueError)
async def value_error_handler(request, exc):
    return JSONResponse(
        status_code=400,
        content={"detail": str(exc), "type": "ValueError"}
    )

@app.exception_handler(ConnectionError)
async def connection_error_handler(request, exc):
    return JSONResponse(
        status_code=503,
        content={"detail": "Service temporarily unavailable", "type": "ConnectionError"}
    )

if __name__ == "__main__":
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8003,
        reload=True,
        log_level="info"
    )
    
-- similarity_search.py

"""
Advanced Similarity Search for SDG Content
Semantic search, SDG interlinkage analysis, and content recommendations
"""
import logging
from typing import List, Dict, Any, Optional, Tuple, Union
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
import networkx as nx
from collections import defaultdict
import asyncio

from .vector_db_client import VectorDBClient
from .embedding_models import EmbeddingManager

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SimilaritySearch:
    """
    Advanced similarity search with SDG-specific intelligence
    Integrates with your sdg_interlinks.py for cross-SDG analysis
    """
    
    def __init__(self, 
                 vector_client: VectorDBClient,
                 embedding_manager: EmbeddingManager,
                 config: Dict[str, Any] = None):
        self.vector_client = vector_client
        self.embedding_manager = embedding_manager
        self.config = config or {}
        
        # SDG interlinkage matrix (from your sdg_interlinks.py)
        self.sdg_interlinkages = self._load_sdg_interlinkages()
        
        # Search configuration
        self.default_similarity_threshold = config.get("similarity_threshold", 0.7)
        self.max_results = config.get("max_results", 100)
        
    def _load_sdg_interlinkages(self) -> Dict[int, List[int]]:
        """
        Load SDG interlinkage data
        This should integrate with your existing sdg_interlinks.py
        """
        # Simplified interlinkage mapping - replace with your full data
        interlinkages = {
            1: [2, 3, 4, 6, 8, 10],  # No Poverty links
            2: [1, 3, 4, 5, 6, 8, 12], # Zero Hunger links
            3: [1, 2, 4, 5, 6, 8, 10, 11], # Good Health links
            4: [1, 2, 3, 5, 8, 10, 16], # Quality Education links
            5: [1, 2, 3, 4, 8, 10, 16], # Gender Equality links
            6: [1, 2, 3, 7, 11, 12, 13, 14, 15], # Clean Water links
            7: [1, 8, 9, 11, 12, 13], # Affordable Energy links
            8: [1, 2, 3, 4, 5, 7, 9, 10, 12, 16], # Decent Work links
            9: [7, 8, 11, 12, 17], # Industry Innovation links
            10: [1, 3, 4, 5, 8, 11, 16], # Reduced Inequalities links
            11: [1, 3, 6, 7, 9, 10, 12, 13, 15], # Sustainable Cities links
            12: [2, 6, 7, 8, 9, 11, 13, 14, 15], # Responsible Consumption links
            13: [6, 7, 11, 12, 14, 15], # Climate Action links
            14: [6, 12, 13, 15], # Life Below Water links
            15: [6, 11, 12, 13, 14], # Life on Land links
            16: [1, 4, 5, 8, 10, 17], # Peace Justice links
            17: [9, 16] # Partnerships links
        }
        return interlinkages
    
    async def semantic_search(self, 
                            query: str,
                            search_type: str = "general",
                            language: str = "en",
                            region: str = None,
                            sdg_goals: List[int] = None,
                            limit: int = 10) -> Dict[str, Any]:
        """
        Advanced semantic search with multiple search strategies
        """
        try:
            # Generate query embedding
            query_embedding = self.embedding_manager.encode(query)
            if len(query_embedding.shape) > 1:
                query_embedding = query_embedding[0]
            
            # Base search parameters
            search_params = {
                "query_vector": query_embedding,
                "limit": limit * 2,  # Get more results for filtering
                "additional_fields": [
                    "title", "summary", "content", "sdg_goals", 
                    "region", "language", "confidence_score", "publication_date"
                ]
            }
            
            # Apply filters based on search type
            where_filter = self._build_search_filter(
                search_type=search_type,
                language=language,
                region=region,
                sdg_goals=sdg_goals
            )
            
            if where_filter:
                search_params["where_filter"] = where_filter
            
            # Execute vector search
            raw_results = self.vector_client.search_similar(**search_params)
            
            # Post-process and rank results
            processed_results = await self._process_search_results(
                raw_results, query, search_type, limit
            )
            
            # Add SDG interlinkage suggestions
            interlinkage_suggestions = self._get_interlinkage_suggestions(processed_results)
            
            return {
                "query": query,
                "search_type": search_type,
                "total_results": len(processed_results),
                "results": processed_results[:limit],
                "interlinkage_suggestions": interlinkage_suggestions,
                "search_metadata": {
                    "language": language,
                    "region": region,
                    "sdg_goals": sdg_goals,
                    "similarity_threshold": self.default_similarity_threshold
                }
            }
            
        except Exception as e:
            logger.error(f"Error in semantic search: {e}")
            raise
    
    def _build_search_filter(self,
                           search_type: str,
                           language: str = None,
                           region: str = None,
                           sdg_goals: List[int] = None) -> Optional[Dict[str, Any]]:
        """Build Weaviate where filter based on search parameters"""
        filters = []
        
        # Language filter
        if language and language != "all":
            filters.append({
                "operator": "Equal",
                "path": ["language"],
                "valueText": language
            })
        
        # Region filter
        if region and region != "all":
            filters.append({
                "operator": "Equal", 
                "path": ["region"],
                "valueText": region
            })
        
        # SDG goals filter
        if sdg_goals:
            filters.append({
                "operator": "ContainsAny",
                "path": ["sdg_goals"],
                "valueIntArray": sdg_goals
            })
        
        # Search type specific filters
        if search_type == "high_quality":
            filters.append({
                "operator": "GreaterThan",
                "path": ["confidence_score"],
                "valueNumber": 0.8
            })
        elif search_type == "recent":
            # Filter for recent publications (last 2 years)
            filters.append({
                "operator": "GreaterThan",
                "path": ["publication_date"],
                "valueDate": "2022-01-01T00:00:00Z"
            })
        
        # Combine filters
        if not filters:
            return None
        elif len(filters) == 1:
            return filters[0]
        else:
            return {
                "operator": "And",
                "operands": filters
            }
    
    async def _process_search_results(self,
                                    raw_results: List[Dict[str, Any]],
                                    query: str,
                                    search_type: str,
                                    limit: int) -> List[Dict[str, Any]]:
        """Post-process and enhance search results"""
        processed_results = []
        
        for result in raw_results:
            # Extract additional metadata
            additional = result.get("_additional", {})
            certainty = additional.get("certainty", 0.0)
            distance = additional.get("distance", 1.0)
            
            # Skip results below similarity threshold
            if certainty < self.default_similarity_threshold:
                continue
            
            # Enhance result with computed fields
            enhanced_result = {
                **result,
                "similarity_score": certainty,
                "distance": distance,
                "relevance_score": self._compute_relevance_score(result, query, search_type),
                "sdg_coverage": self._analyze_sdg_coverage(result.get("sdg_goals", [])),
                "content_quality": self._assess_content_quality(result)
            }
            
            processed_results.append(enhanced_result)
        
        # Sort by relevance score
        processed_results.sort(key=lambda x: x["relevance_score"], reverse=True)
        
        return processed_results[:limit]
    
    def _compute_relevance_score(self, 
                               result: Dict[str, Any],
                               query: str,
                               search_type: str) -> float:
        """Compute comprehensive relevance score"""
        base_score = result.get("_additional", {}).get("certainty", 0.0)
        
        # Factor in confidence score
        confidence_score = result.get("confidence_score", 0.5)
        confidence_weight = 0.2
        
        # Factor in SDG goal relevance
        sdg_goals = result.get("sdg_goals", [])
        sdg_relevance = len(sdg_goals) / 17.0 if sdg_goals else 0.0  # Normalize by max SDGs
        sdg_weight = 0.2
        
        # Factor in content length (prefer substantial content)
        content_length = len(result.get("content", ""))
        length_score = min(content_length / 10000.0, 1.0)  # Normalize to 10k chars
        length_weight = 0.1
        
        # Search type specific adjustments
        type_bonus = 0.0
        if search_type == "high_quality" and confidence_score > 0.8:
            type_bonus = 0.1
        elif search_type == "comprehensive" and len(sdg_goals) > 2:
            type_bonus = 0.1
        
        relevance_score = (
            base_score * 0.5 +
            confidence_score * confidence_weight +
            sdg_relevance * sdg_weight +
            length_score * length_weight +
            type_bonus
        )
        
        return min(relevance_score, 1.0)
    
    def _analyze_sdg_coverage(self, sdg_goals: List[int]) -> Dict[str, Any]:
        """Analyze SDG goal coverage and interlinkages"""
        if not sdg_goals:
            return {"coverage": 0.0, "interlinkages": [], "primary_goal": None}
        
        primary_goal = sdg_goals[0] if sdg_goals else None
        coverage = len(sdg_goals) / 17.0  # Percentage of SDGs covered
        
        # Find interlinkages
        interlinkages = set()
        for goal in sdg_goals:
            if goal in self.sdg_interlinkages:
                interlinkages.update(self.sdg_interlinkages[goal])
        
        # Remove already covered goals
        interlinkages = list(interlinkages - set(sdg_goals))
        
        return {
            "coverage": coverage,
            "goals_covered": len(sdg_goals),
            "primary_goal": primary_goal,
            "interlinkages": interlinkages[:5],  # Top 5 related goals
            "interconnectivity": len(interlinkages) / 17.0
        }
    
    def _assess_content_quality(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """Assess content quality metrics"""
        content = result.get("content", "")
        title = result.get("title", "")
        summary = result.get("summary", "")
        
        # Basic quality metrics
        content_length = len(content)
        has_title = len(title.strip()) > 0
        has_summary = len(summary.strip()) > 0
        has_url = bool(result.get("source_url"))
        
        # Text quality heuristics
        sentence_count = content.count('.') + content.count('!') + content.count('?')
        avg_sentence_length = content_length / max(sentence_count, 1)
        
        quality_score = 0.0
        if has_title: quality_score += 0.2
        if has_summary: quality_score += 0.2  
        if has_url: quality_score += 0.1
        if content_length > 500: quality_score += 0.2
        if 50 <= avg_sentence_length <= 200: quality_score += 0.3  # Optimal sentence length
        
        return {
            "quality_score": quality_score,
            "content_length": content_length,
            "has_metadata": has_title and has_summary,
            "avg_sentence_length": avg_sentence_length,
            "estimated_reading_time": max(1, content_length // 250)  # Words per minute
        }
    
    def _get_interlinkage_suggestions(self, 
                                    results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Generate SDG interlinkage suggestions based on search results"""
        sdg_frequency = defaultdict(int)
        
        # Count SDG goal frequencies in results
        for result in results:
            for goal in result.get("sdg_goals", []):
                sdg_frequency[goal] += 1
        
        # Generate suggestions for top SDGs
        suggestions = []
        for goal, frequency in sorted(sdg_frequency.items(), key=lambda x: x[1], reverse=True)[:3]:
            interlinkages = self.sdg_interlinkages.get(goal, [])
            if interlinkages:
                suggestions.append({
                    "primary_sdg": goal,
                    "frequency": frequency,
                    "related_sdgs": interlinkages[:5],
                    "suggestion": f"Explore connections between SDG {goal} and related goals: {', '.join(map(str, interlinkages[:3]))}"
                })
        
        return suggestions
    
    async def find_similar_documents(self,
                                   document_id: str,
                                   similarity_threshold: float = 0.7,
                                   limit: int = 10) -> List[Dict[str, Any]]:
        """Find documents similar to a given document"""
        try:
            # Get source document embedding
            # This would require storing document embeddings with IDs
            # For now, implement a placeholder
            
            # In production, you'd:
            # 1. Retrieve document by ID
            # 2. Get its embedding
            # 3. Perform similarity search
            # 4. Return similar documents
            
            logger.warning("find_similar_documents not fully implemented - requires document ID storage")
            return []
            
        except Exception as e:
            logger.error(f"Error finding similar documents: {e}")
            raise
    
    async def cluster_search_results(self,
                                   results: List[Dict[str, Any]],
                                   num_clusters: int = 5) -> Dict[str, Any]:
        """Cluster search results for better organization"""
        try:
            if len(results) < num_clusters:
                return {"clusters": [{"documents": results, "theme": "All Results"}]}
            
            # Extract embeddings (would need to be stored with results)
            # For now, use SDG goals as clustering feature
            
            features = []
            for result in results:
                # Create feature vector based on SDG goals
                feature = [0] * 17
                for goal in result.get("sdg_goals", []):
                    if 1 <= goal <= 17:
                        feature[goal-1] = 1
                features.append(feature)
            
            # Perform clustering
            kmeans = KMeans(n_clusters=min(num_clusters, len(results)), random_state=42)
            cluster_labels = kmeans.fit_predict(features)
            
            # Organize results by cluster
            clusters = defaultdict(list)
            for i, label in enumerate(cluster_labels):
                clusters[label].append(results[i])
            
            # Generate cluster themes
            cluster_results = []
            for cluster_id, docs in clusters.items():
                # Determine theme based on most common SDGs
                sdg_counts = defaultdict(int)
                for doc in docs:
                    for goal in doc.get("sdg_goals", []):
                        sdg_counts[goal] += 1
                
                top_sdgs = sorted(sdg_counts.items(), key=lambda x: x[1], reverse=True)[:2]
                theme = f"SDG {top_sdgs}" if top_sdgs else "Mixed Content"
                if len(top_sdgs) > 1:
                    theme += f" & {top_sdgs}"
                
                cluster_results.append({
                    "cluster_id": cluster_id,
                    "theme": theme,
                    "document_count": len(docs),
                    "documents": docs,
                    "dominant_sdgs": [sdg for sdg, _ in top_sdgs]
                })
            
            return {
                "total_clusters": len(cluster_results),
                "clusters": cluster_results,
                "clustering_method": "SDG-based K-means"
            }
            
        except Exception as e:
            logger.error(f"Error clustering search results: {e}")
            return {"clusters": [{"documents": results, "theme": "Unclustered"}]}
    
    def health_check(self) -> Dict[str, Any]:
        """Health check for similarity search service"""
        try:
            # Test vector client connection
            vector_health = self.vector_client.health_check()
            
            # Test embedding manager
            test_embedding = self.embedding_manager.encode("test query")
            embedding_healthy = len(test_embedding) > 0
            
            return {
                "status": "healthy" if vector_health.get("status") == "healthy" and embedding_healthy else "unhealthy",
                "vector_db": vector_health,
                "embedding_manager": {
                    "status": "healthy" if embedding_healthy else "unhealthy",
                    "available_models": list(self.embedding_manager.models.keys())
                },
                "sdg_interlinkages_loaded": len(self.sdg_interlinkages) == 17
            }
            
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }

class SDGRecommendationEngine:
    """
    SDG-specific content recommendation engine
    """
    
    def __init__(self, similarity_search: SimilaritySearch):
        self.similarity_search = similarity_search
        self.sdg_weights = self._initialize_sdg_weights()
    
    def _initialize_sdg_weights(self) -> Dict[int, float]:
        """Initialize SDG importance weights based on global priorities"""
        # Weights based on UN priority areas and interconnectedness
        return {
            1: 1.0,   # No Poverty - foundational
            2: 0.95,  # Zero Hunger - critical
            3: 0.9,   # Good Health - essential
            4: 0.85,  # Quality Education - long-term impact
            5: 0.8,   # Gender Equality - cross-cutting
            6: 0.9,   # Clean Water - fundamental
            7: 0.85,  # Affordable Energy - enabling
            8: 0.8,   # Decent Work - economic
            9: 0.75,  # Industry Innovation - development
            10: 0.8,  # Reduced Inequalities - social
            11: 0.7,  # Sustainable Cities - urban
            12: 0.75, # Responsible Consumption - environmental
            13: 1.0,  # Climate Action - urgent priority
            14: 0.8,  # Life Below Water - environmental
            15: 0.8,  # Life on Land - environmental
            16: 0.85, # Peace Justice - governance
            17: 0.9   # Partnerships - enabling
        }
    
    async def recommend_content(self,
                              user_interests: List[int],
                              region: str = None,
                              language: str = "en",
                              limit: int = 10) -> Dict[str, Any]:
        """Generate personalized SDG content recommendations"""
        try:
            recommendations = []
            
            # Get content for user's primary interests
            for sdg_goal in user_interests:
                results = self.similarity_search.vector_client.search_by_sdg_goals(
                    sdg_goals=[sdg_goal],
                    limit=limit // len(user_interests) + 2
                )
                
                # Apply recommendation scoring
                for result in results:
                    score = self._calculate_recommendation_score(
                        result, user_interests, region, language
                    )
                    result["recommendation_score"] = score
                    recommendations.append(result)
            
            # Add interlinkage-based recommendations
            interlinkage_recs = await self._get_interlinkage_recommendations(
                user_interests, region, language, limit // 2
            )
            recommendations.extend(interlinkage_recs)
            
            # Sort and deduplicate
            recommendations.sort(key=lambda x: x["recommendation_score"], reverse=True)
            unique_recommendations = self._deduplicate_recommendations(recommendations)
            
            return {
                "user_interests": user_interests,
                "total_recommendations": len(unique_recommendations),
                "recommendations": unique_recommendations[:limit],
                "recommendation_metadata": {
                    "region": region,
                    "language": language,
                    "interlinkage_based": len(interlinkage_recs),
                    "direct_interest": len(recommendations) - len(interlinkage_recs)
                }
            }
            
        except Exception as e:
            logger.error(f"Error generating recommendations: {e}")
            raise
    
    def _calculate_recommendation_score(self,
                                      result: Dict[str, Any],
                                      user_interests: List[int],
                                      region: str = None,
                                      language: str = None) -> float:
        """Calculate recommendation score for content"""
        base_score = result.get("confidence_score", 0.5)
        
        # Interest alignment score
        result_sdgs = result.get("sdg_goals", [])
        interest_overlap = len(set(result_sdgs) & set(user_interests))
        interest_score = interest_overlap / max(len(user_interests), 1)
        
        # SDG importance weighting
        sdg_weight = sum(self.sdg_weights.get(goal, 0.5) for goal in result_sdgs) / max(len(result_sdgs), 1)
        
        # Region/language bonus
        region_bonus = 0.1 if result.get("region") == region else 0.0
        language_bonus = 0.1 if result.get("language") == language else 0.0
        
        # Content quality bonus
        quality_bonus = 0.1 if len(result.get("content", "")) > 1000 else 0.0
        
        recommendation_score = (
            base_score * 0.4 +
            interest_score * 0.3 +
            sdg_weight * 0.2 +
            region_bonus + language_bonus + quality_bonus
        )
        
        return min(recommendation_score, 1.0)
    
    async def _get_interlinkage_recommendations(self,
                                             user_interests: List[int],
                                             region: str,
                                             language: str,
                                             limit: int) -> List[Dict[str, Any]]:
        """Get recommendations based on SDG interlinkages"""
        related_sdgs = set()
        
        # Find related SDGs through interlinkages
        for goal in user_interests:
            related_sdgs.update(self.similarity_search.sdg_interlinkages.get(goal, []))
        
        # Remove already interested SDGs
        related_sdgs = list(related_sdgs - set(user_interests))
        
        # Get content for related SDGs
        if related_sdgs:
            results = self.similarity_search.vector_client.search_by_sdg_goals(
                sdg_goals=related_sdgs[:5],  # Top 5 related
                limit=limit
            )
            
            # Mark as interlinkage-based recommendations
            for result in results:
                result["recommendation_type"] = "interlinkage"
                result["recommendation_score"] = self._calculate_recommendation_score(
                    result, user_interests, region, language
                ) * 0.8  # Slight penalty for indirect interest
            
            return results
        
        return []
    
    def _deduplicate_recommendations(self, recommendations: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Remove duplicate recommendations based on content similarity"""
        seen_titles = set()
        unique_recs = []
        
        for rec in recommendations:
            title = rec.get("title", "").strip().lower()
            if title and title not in seen_titles:
                seen_titles.add(title)
                unique_recs.append(rec)
        
        return unique_recs
        
-- vector_db_client.py

"""
Vector Database Client for SDG Pipeline
Enhanced Weaviate integration with connection pooling and SDG schema
"""
import logging
import time
from typing import List, Dict, Any, Optional, Union
import weaviate
import numpy as np
from weaviate.embedded import EmbeddedOptions
import json
from datetime import datetime
import asyncio
from contextlib import asynccontextmanager
import threading

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class VectorDBClient:
    """
    Enhanced Weaviate client for SDG vector operations
    Includes connection pooling, retry logic, and SDG-specific schema
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.client = None
        self._connection_pool = []
        self._pool_lock = threading.Lock()
        self.max_connections = config.get("max_connections", 10)
        self.retry_attempts = config.get("retry_attempts", 3)
        self.retry_delay = config.get("retry_delay", 1.0)
        
        # SDG-specific configuration
        self.sdg_classes = [
            "SDGArticle", "SDGProgress", "SDGTarget", 
            "SDGIndicator", "RegionalData", "AITopic"
        ]
        
        self._initialize_client()
        self._setup_sdg_schema()
    
    def _initialize_client(self):
        """Initialize Weaviate client with configuration"""
        try:
            if self.config.get("embedded", False):
                # Embedded Weaviate for development
                self.client = weaviate.Client(
                    embedded_options=EmbeddedOptions(
                        hostname=self.config.get("hostname", "localhost"),
                        port=self.config.get("port", 8080),
                        grpc_port=self.config.get("grpc_port", 50051)
                    )
                )
            else:
                # Remote Weaviate instance
                auth_config = None
                if self.config.get("api_key"):
                    auth_config = weaviate.AuthApiKey(api_key=self.config["api_key"])
                
                headers = {}
                if self.config.get("openai_api_key"):
                    headers["X-OpenAI-Api-Key"] = self.config["openai_api_key"]
                
                self.client = weaviate.Client(
                    url=self.config.get("url", "http://localhost:8080"),
                    auth_client_secret=auth_config,
                    additional_headers=headers
                )
            
            # Test connection
            if self.client.is_ready():
                logger.info("Weaviate client initialized successfully")
            else:
                raise ConnectionError("Weaviate client not ready")
                
        except Exception as e:
            logger.error(f"Error initializing Weaviate client: {e}")
            raise
    
    def _setup_sdg_schema(self):
        """Setup SDG-specific Weaviate schema"""
        try:
            # Define SDG Article class schema
            sdg_article_schema = {
                "class": "SDGArticle",
                "description": "SDG-related articles and documents",
                "vectorizer": "none",  # We'll provide our own vectors
                "properties": [
                    {
                        "name": "title",
                        "dataType": ["text"],
                        "description": "Article title"
                    },
                    {
                        "name": "content",
                        "dataType": ["text"],
                        "description": "Full article content"
                    },
                    {
                        "name": "summary",
                        "dataType": ["text"],
                        "description": "Article summary"
                    },
                    {
                        "name": "sdg_goals",
                        "dataType": ["int[]"],
                        "description": "Related SDG goals (1-17)"
                    },
                    {
                        "name": "sdg_targets",
                        "dataType": ["text[]"],
                        "description": "Related SDG targets"
                    },
                    {
                        "name": "region",
                        "dataType": ["text"],
                        "description": "Geographic region"
                    },
                    {
                        "name": "language",
                        "dataType": ["text"],
                        "description": "Content language"
                    },
                    {
                        "name": "publication_date",
                        "dataType": ["date"],
                        "description": "Publication date"
                    },
                    {
                        "name": "source_url",
                        "dataType": ["text"],
                        "description": "Source URL"
                    },
                    {
                        "name": "confidence_score",
                        "dataType": ["number"],
                        "description": "SDG classification confidence"
                    }
                ]
            }
            
            # Create schema if it doesn't exist
            if not self.client.schema.exists("SDGArticle"):
                self.client.schema.create_class(sdg_article_schema)
                logger.info("Created SDGArticle schema")
            
            # Additional schemas for other SDG entities
            self._create_additional_schemas()
            
        except Exception as e:
            logger.error(f"Error setting up SDG schema: {e}")
            raise
    
    def _create_additional_schemas(self):
        """Create additional SDG-related schemas"""
        schemas = [
            {
                "class": "SDGProgress",
                "description": "SDG progress tracking data",
                "vectorizer": "none",
                "properties": [
                    {"name": "country", "dataType": ["text"]},
                    {"name": "sdg_goal", "dataType": ["int"]},
                    {"name": "indicator_value", "dataType": ["number"]},
                    {"name": "year", "dataType": ["int"]},
                    {"name": "data_source", "dataType": ["text"]}
                ]
            },
            {
                "class": "RegionalData", 
                "description": "Region-specific SDG data",
                "vectorizer": "none",
                "properties": [
                    {"name": "region", "dataType": ["text"]},
                    {"name": "country", "dataType": ["text"]},
                    {"name": "sdg_scores", "dataType": ["number[]"]},
                    {"name": "metadata", "dataType": ["text"]}
                ]
            }
        ]
        
        for schema in schemas:
            if not self.client.schema.exists(schema["class"]):
                self.client.schema.create_class(schema)
                logger.info(f"Created {schema['class']} schema")
    
    async def insert_embeddings(self, 
                              documents: List[Dict[str, Any]], 
                              class_name: str = "SDGArticle",
                              batch_size: int = 100) -> List[str]:
        """
        Insert documents with embeddings into Weaviate
        """
        uuids = []
        total_batches = (len(documents) + batch_size - 1) // batch_size
        
        logger.info(f"Inserting {len(documents)} documents in {total_batches} batches")
        
        for i in range(0, len(documents), batch_size):
            batch = documents[i:i + batch_size]
            batch_uuids = await self._insert_batch(batch, class_name)
            uuids.extend(batch_uuids)
            
            batch_num = (i // batch_size) + 1
            logger.info(f"Inserted batch {batch_num}/{total_batches}")
        
        return uuids
    
    async def _insert_batch(self, documents: List[Dict[str, Any]], class_name: str) -> List[str]:
        """Insert a batch of documents with retry logic"""
        for attempt in range(self.retry_attempts):
            try:
                with self.client.batch as batch:
                    batch.batch_size = len(documents)
                    uuids = []
                    
                    for doc in documents:
                        # Extract vector and properties
                        vector = doc.pop("vector", None)
                        uuid = self.client.batch.add_data_object(
                            data_object=doc,
                            class_name=class_name,
                            vector=vector
                        )
                        uuids.append(uuid)
                    
                    return uuids
                    
            except Exception as e:
                logger.warning(f"Batch insert attempt {attempt + 1} failed: {e}")
                if attempt < self.retry_attempts - 1:
                    await asyncio.sleep(self.retry_delay * (2 ** attempt))
                else:
                    raise
    
    def search_similar(self, 
                      query_vector: np.ndarray,
                      class_name: str = "SDGArticle", 
                      limit: int = 10,
                      where_filter: Dict[str, Any] = None,
                      additional_fields: List[str] = None) -> List[Dict[str, Any]]:
        """
        Semantic similarity search with SDG-specific filtering
        """
        try:
            # Build the query
            query_builder = (
                self.client.query
                .get(class_name, additional_fields or ["title", "summary", "sdg_goals", "region"])
                .with_near_vector({
                    "vector": query_vector.tolist(),
                    "certainty": 0.7
                })
                .with_limit(limit)
                .with_additional(["certainty", "distance"])
            )
            
            # Add where filter if provided
            if where_filter:
                query_builder = query_builder.with_where(where_filter)
            
            result = query_builder.do()
            
            # Extract results
            class_results = result.get("data", {}).get("Get", {}).get(class_name, [])
            return class_results
            
        except Exception as e:
            logger.error(f"Error in similarity search: {e}")
            raise
    
    def search_by_sdg_goals(self, 
                           sdg_goals: List[int],
                           query_vector: np.ndarray = None,
                           limit: int = 50) -> List[Dict[str, Any]]:
        """Search for content related to specific SDG goals"""
        where_filter = {
            "operator": "ContainsAny",
            "path": ["sdg_goals"], 
            "valueIntArray": sdg_goals
        }
        
        if query_vector is not None:
            return self.search_similar(
                query_vector=query_vector,
                where_filter=where_filter,
                limit=limit
            )
        else:
            # Pure filter search without vector similarity
            try:
                result = (
                    self.client.query
                    .get("SDGArticle", ["title", "summary", "sdg_goals", "region", "confidence_score"])
                    .with_where(where_filter)
                    .with_limit(limit)
                    .do()
                )
                return result.get("data", {}).get("Get", {}).get("SDGArticle", [])
            except Exception as e:
                logger.error(f"Error in SDG goal search: {e}")
                raise
    
    def search_by_region(self, 
                        region: str,
                        query_vector: np.ndarray = None,
                        limit: int = 30) -> List[Dict[str, Any]]:
        """Search for region-specific SDG content"""
        where_filter = {
            "operator": "Equal",
            "path": ["region"],
            "valueText": region
        }
        
        if query_vector is not None:
            return self.search_similar(
                query_vector=query_vector,
                where_filter=where_filter,
                limit=limit
            )
        else:
            try:
                result = (
                    self.client.query
                    .get("SDGArticle", ["title", "summary", "region", "sdg_goals"])
                    .with_where(where_filter)
                    .with_limit(limit)
                    .do()
                )
                return result.get("data", {}).get("Get", {}).get("SDGArticle", [])
            except Exception as e:
                logger.error(f"Error in region search: {e}")
                raise
    
    def get_statistics(self) -> Dict[str, Any]:
        """Get database statistics"""
        try:
            stats = {}
            for class_name in self.sdg_classes:
                if self.client.schema.exists(class_name):
                    result = self.client.query.aggregate(class_name).with_meta_count().do()
                    count = result.get("data", {}).get("Aggregate", {}).get(class_name, [{}])[0].get("meta", {}).get("count", 0)
                    stats[class_name] = count
            return stats
        except Exception as e:
            logger.error(f"Error getting statistics: {e}")
            return {}
    
    def health_check(self) -> Dict[str, Any]:
        """Perform health check on vector database"""
        try:
            is_ready = self.client.is_ready()
            is_live = self.client.is_live()
            
            return {
                "status": "healthy" if is_ready and is_live else "unhealthy",
                "ready": is_ready,
                "live": is_live,
                "timestamp": datetime.utcnow().isoformat(),
                "statistics": self.get_statistics()
            }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "timestamp": datetime.utcnow().isoformat()
            }
    
    def close(self):
        """Close the vector database connection"""
        if self.client:
            # Weaviate client doesn't have explicit close method
            # but we can clear the connection pool
            with self._pool_lock:
                self._connection_pool.clear()
            logger.info("Vector database client closed")

# Connection manager for async operations
@asynccontextmanager
async def get_vector_client(config: Dict[str, Any]):
    """Context manager for vector database operations"""
    client = VectorDBClient(config)
    try:
        yield client
    finally:
        client.close()
