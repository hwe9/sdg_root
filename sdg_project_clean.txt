### /source /mnt/gigabyte1tb/SDG/sdg_root/docker-compose.yml
networks:
  sdg_internal:
    driver: bridge
    internal: true 
  sdg_external:
    driver: bridge

services:

  nginx_proxy:
    image: nginx:alpine
    container_name: nginx_proxy
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/ssl/certs:ro
    networks:
      - sdg_external
      - sdg_internal
    depends_on:
      - auth_service
      - api_service

  auth_service:
  build: 
    context: ${SRC_ROOT}/auth
    dockerfile: Dockerfile
  container_name: auth_service
  expose:
      - "8005"
  volumes:
      - ${SRC_ROOT}/auth:/app
  environment:
      - SECRET_KEY_ENCRYPTED=${SECRET_KEY_ENCRYPTED}
      - DATABASE_URL_ENCRYPTED=${DATABASE_URL_ENCRYPTED}
      - ALLOWED_ORIGINS=${ALLOWED_ORIGINS}
  networks:
    - sdg_internal
  depends_on:
    database_service:
      condition: service_healthy
  command: python -m uvicorn main:app --host 0.0.0.0 --port 8005 --reload
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:8005/health"]
    interval: 30s
    timeout: 10s
    retries: 3
  deploy:
    resources:
      limits:
        cpus: '0.5'
        memory: '512M'

  api_service:
    build: ${SRC_ROOT}/api
    container_name: api_service
    expose:
      - "8000"
    volumes:
      - ${SRC_ROOT}/api:/app
    environment:
      - DATABASE_URL_ENCRYPTED=${DATABASE_URL_ENCRYPTED}
    networks:
      - sdg_internal
    depends_on:
      database_service:
        condition: service_healthy
    command: python -m uvicorn main:app --host 0.0.0.0 --port 8000 --reload
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '1.0'

  database_service:
    image: postgres:16-alpine
    container_name: database_service
    environment:
      - POSTGRES_USER=${POSTGRES_USER:-postgres}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-postgres}
      - POSTGRES_DB=${POSTGRES_DB:-sdg_pipeline}
      - POSTGRES_INITDB_ARGS: "--encoding=UTF-8 --locale=en_US.UTF-8"
    volumes:
      - ${DATABASE_SERVICE_VOLUMES}
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '1.0'

  weaviate_service:
    # profiles: ["vector"]
    image: semitechnologies/weaviate:1.24.1
    container_name: weaviate_service
    ports:
      - "8080:8080"
    environment:
      - QUERY_DEFAULTS_LIMIT=25
      - AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED=true
      - DEFAULT_VECTORIZER_MODULE=text2vec-transformers
      - ENABLE_MODULES=text2vec-transformers
      - TRANSFORMERS_INFERENCE_API=http://weaviate_transformer_service:8080
      - PERSISTENCE_DATA_PATH=/var/lib/weaviate
      - CLUSTER_HOSTNAME=node1
      - MODULES_CLIENT_TIMEOUT=600s
    volumes:
      - ${WEAVIATE_SERVICE_VOLUMES}
    healthcheck:
      test: ["CMD-SHELL", "wget --spider --quiet http://localhost:8080/v1/meta || exit 1"] 
      interval: 30s
      timeout: 20s
      retries: 20
      start_period: 600s
    depends_on:
      weaviate_transformer_service:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: '2.0'    
          memory: '4G'   
        reservations:
          memory: '2G'
    
  weaviate_transformer_service:
    # profiles: ["vector"]
    image: ${WEAVIATE_IMAGE}
    container_name: weaviate_transformer_service
    ports:
      - "8081:8080"
    environment:
      - ENABLE_GPU=0
      - TORCH_NUM_THREADS=4               
      - OMP_NUM_THREADS=4
    healthcheck:
      test: ["CMD-SHELL", "python -c 'import urllib.request; urllib.request.urlopen(\"http://localhost:8080/.well-known/ready\").read()' || exit 1"]
      interval: 45s
      timeout: 30s
      retries: 25
      start_period: 600s
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: '8G'
        reservations:
          memory: '4G'

  vectorization_service:
    build: ${SRC_ROOT}/vectorization
    container_name: vectorization_service
    ports:
      - "8003:8003"
    volumes:
      - ${SRC_ROOT}/vectorization:/app
    environment:
      - WEAVIATE_URL=${WEAVIATE_URL}
      - DATABASE_URL=${DATABASE_URL}
    depends_on:
      weaviate_service:
        condition: service_healthy
    command: python -m uvicorn main:app --host 0.0.0.0 --port 8000 --reload

  
  data_retrieval_service:
    build: ${SRC_ROOT}/data_retrieval
    container_name: data_retrieval_service
    ports:
      - "8002:8002"
    volumes:
      - ${SRC_ROOT}/data_retrieval:/app
      - ${DATA_ROOT}/raw_data:/app/raw_data
      - ${DATA_ROOT}/quelle.txt:/app/quelle.txt
    environment:
      - DATABASE_URL=${DATABASE_URL}
    depends_on:
      database_service:
        condition: service_healthy
    command: python main.py
    deploy:
      resources:
        limits:
          cpus: '1.0'

  data_processing_service:
    # profiles: ["processing"]
    build: ${SRC_ROOT}/data_processing
    container_name: data_processing_service
    ports:
      - "8001:8001"
    volumes:
      - ${SRC_ROOT}/data_processing:/app
      - ${DATA_ROOT}/raw_data:/app/raw_data
      - ${DATA_ROOT}/processed_data:/app/processed_data
      - ${DATA_ROOT}/images:/app/images
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - WEAVIATE_URL=${WEAVIATE_URL}
    depends_on:
      data_retrieval_service:
        condition: service_started
      database_service:
        condition: service_healthy
      weaviate_service:
        condition: service_healthy
      weaviate_transformer_service:
        condition: service_healthy
    command: python main.py
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: '4G'
        reservations:
          cpus: '2'
          memory: '3G'
  
  content_extraction_service:
  build: ${SRC_ROOT}/content_extraction
  container_name: content_extraction_service
  ports:
    - "8004:8004"
  volumes:
    - ${SRC_ROOT}/content_extraction:/app
  environment:
    - DATABASE_URL=${DATABASE_URL}
    - DATA_PROCESSING_URL=http://data_processing_service:8001
    - API_SERVICE_URL=http://api_service:8000
  depends_on:
    database_service:
      condition: service_healthy
  command: python -m uvicorn main:app --host 0.0.0.0 --port 8004 --reload
  deploy:
    resources:
      limits:
        cpus: '1.0'
        memory: '2G'
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:8004/health"]
    interval: 30s
    timeout: 10s
    retries: 3

  pgadmin_service:
    image: dpage/pgadmin4:latest
    container_name: pgadmin_service
    ports:
      - "5050:80"
    environment:
      - PGADMIN_DEFAULT_EMAIL=${PGADMIN_DEFAULT_EMAIL}
      - PGADMIN_DEFAULT_PASSWORD=${POSTGRES_PASSWORD}
      - PGADMIN_CONFIG_SERVER_MODE=False
      - PGADMIN_CONFIG_MASTER_PASSWORD_REQUIRED=False
    volumes:
      - ${PGADMIN_VOLUMES}
    depends_on:
      database_service:
        condition: service_healthy
    user: "5050:5050"
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
  
  weaviate_console:
    image: semitechnologies/weaviate-console
    container_name: weaviate_console
    ports:
      - "3001:3000"
    environment:
      - WEAVIATE_HOST=weaviate_service:8080
    depends_on:
      weaviate_service:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: '256M'
  
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
    
  



### /source /mnt/gigabyte1tb/SDG/sdg_root/requirements-security.txt
# /sdg_root/requirements-security.txt
cryptography>=41.0.0
keyring>=24.0.0
bcrypt>=4.0.0
PyJWT[crypto]>=2.8.0
pydantic[email]>=2.0.0
python-multipart>=0.0.6
slowapi>=0.1.9  # Rate limiting


### /source /mnt/gigabyte1tb/SDG/sdg_root/requirements.txt
fastapi
uvicorn[standard]
python-dotenv


### /source /mnt/gigabyte1tb/SDG/sdg_root/scripts/concat_code.py
import os
from pathlib import Path

import os
from pathlib import Path

def write_files_with_directory_exclusion(folder_config, output_base_path):
    """
    Schreibt Dateien in Output-Files mit flexibler Verzeichnis-Ausschließung.
    
    Args:
        folder_config (dict): Konfiguration pro Ordner
            {ordnerpfad: {
                'files': ['specific1.py', 'specific2.py'] oder 'all',
                'extensions': ['.py', '.txt'] oder 'all',
                'output_name': 'custom_name.txt',
                'recursive': True/False,
                'exclude_dirs': ['__pycache__', '.git', 'node_modules'],  # NEU!
                'exclude_patterns': ['*.pyc', '*.log'],  # NEU!
                'exclude_extensions': ['.exe', '.bin']
            }}
        output_base_path (str): Basis-Pfad für alle Output-Files
    """
    
    for folder, config in folder_config.items():
        # Output-Datei bestimmen
        output_filename = config.get('output_name', f"{os.path.basename(folder)}_filtered.txt")
        output_path = os.path.join(output_base_path, output_filename)
        
        # Output-Verzeichnis erstellen
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as outfile:
            files_to_process = []
            exclude_dirs = config.get('exclude_dirs', [])
            exclude_patterns = config.get('exclude_patterns', [])
            
            # Rekursive Suche mit Verzeichnis-Ausschluss
            def get_files_with_exclusion(root_path):
                result_files = []
                
                for root, dirs, files in os.walk(root_path):
                    # Verzeichnisse ausschließen (modifiziert dirs in-place)
                    dirs[:] = [d for d in dirs if not should_exclude_directory(d, exclude_dirs, root)]
                    
                    for file in files:
                        file_path = os.path.join(root, file)
                        if not should_exclude_file(file_path, exclude_patterns):
                            result_files.append(file_path)
                
                return result_files
            
            # Bestimme welche Dateien verarbeitet werden sollen
            if config.get('recursive', False):
                if config['files'] == 'all':
                    # Alle Dateien rekursiv mit Ausschlüssen
                    files_to_process = get_files_with_exclusion(folder)
                else:
                    # Spezifische Dateien rekursiv suchen
                    all_files = get_files_with_exclusion(folder)
                    specific_files = set(config['files'])
                    files_to_process = [f for f in all_files if os.path.basename(f) in specific_files]
            else:
                # Nur Hauptordner (ohne Rekursion)
                if config['files'] == 'all':
                    files_to_process = [os.path.join(folder, f) for f in os.listdir(folder) 
                                     if os.path.isfile(os.path.join(folder, f)) 
                                     and not should_exclude_file(os.path.join(folder, f), exclude_patterns)]
                else:
                    for filename in config['files']:
                        file_path = os.path.join(folder, filename)
                        if os.path.exists(file_path) and not should_exclude_file(file_path, exclude_patterns):
                            files_to_process.append(file_path)
            
            # Extension-Filter anwenden
            if 'extensions' in config and config['extensions'] != 'all':
                allowed_extensions = [ext.lower() for ext in config['extensions']]
                files_to_process = [f for f in files_to_process 
                                  if Path(f).suffix.lower() in allowed_extensions]
            
            # Ausschließen bestimmter Dateiendungen
            if 'exclude_extensions' in config:
                excluded_extensions = [ext.lower() for ext in config['exclude_extensions']]
                files_to_process = [f for f in files_to_process 
                                  if Path(f).suffix.lower() not in excluded_extensions]
            
            # Dateien sortieren
            files_to_process.sort()
            
            # Jede Datei verarbeiten
            for file_path in files_to_process:
                if os.path.exists(file_path) and os.path.isfile(file_path):
                    outfile.write(f"### /source {file_path}\n")
                    
                    if is_text_file(file_path):
                        try:
                            with open(file_path, 'r', encoding='utf-8') as infile:
                                content = infile.read()
                                outfile.write(content)
                        except UnicodeDecodeError:
                            try:
                                with open(file_path, 'r', encoding='latin-1') as infile:
                                    content = infile.read()
                                    outfile.write(content)
                            except Exception as e:
                                outfile.write(f"# Error reading file {file_path}: {e}\n")
                    else:
                        file_size = os.path.getsize(file_path)
                        outfile.write(f"# Binary file (Size: {file_size} bytes) - Content not displayed\n")
                    
                    outfile.write('\n\n')
            
            print(f"✓ {len(files_to_process)} Dateien geschrieben nach: {output_path}")
            if exclude_dirs:
                print(f"  → Ausgeschlossene Verzeichnisse: {exclude_dirs}")

def should_exclude_directory(dirname, exclude_dirs, current_path):
    """Prüft ob ein Verzeichnis ausgeschlossen werden soll"""
    # Einfacher Name-Match
    if dirname in exclude_dirs:
        return True
    
    # Pattern-Match (z.B. für versteckte Ordner)
    import fnmatch
    for pattern in exclude_dirs:
        if fnmatch.fnmatch(dirname, pattern):
            return True
    
    # Pfad-basierte Ausschlüsse
    full_path = os.path.join(current_path, dirname)
    for exclude_pattern in exclude_dirs:
        if exclude_pattern.startswith('/') and exclude_pattern in full_path:
            return True
    
    return False

def should_exclude_file(filepath, exclude_patterns):
    """Prüft ob eine Datei aufgrund von Patterns ausgeschlossen werden soll"""
    import fnmatch
    filename = os.path.basename(filepath)
    
    for pattern in exclude_patterns:
        if fnmatch.fnmatch(filename, pattern) or fnmatch.fnmatch(filepath, pattern):
            return True
    return False

def is_text_file(file_path):
    """Prüft ob eine Datei als Text lesbar ist"""
    text_extensions = {
        '.txt', '.py', '.js', '.html', '.css', '.md', '.json', '.xml', '.yaml', '.yml',
        '.conf', '.cfg', '.ini', '.log', '.csv', '.sql', '.sh', '.bat', '.ps1',
        '.dockerfile', '.gitignore', '.env', '.properties', '.toml', '.requirements'
    }
    
    if Path(file_path).suffix.lower() in text_extensions:
        return True
    
    try:
        with open(file_path, 'rb') as f:
            chunk = f.read(1024)
            if b'\0' in chunk:
                return False
            try:
                chunk.decode('utf-8')
                return True
            except UnicodeDecodeError:
                return False
    except:
        return False

# BEISPIEL-VERWENDUNG mit Verzeichnis-Ausschluss:
if __name__ == "__main__":
    
    folder_config = {
        # SDG-Projekt komplett, aber ohne unnötige Ordner
        '/mnt/gigabyte1tb/SDG': {
            'files': 'all',
            'extensions': ['.py', '.yml', '.yaml', '.txt', '.md', '.env', '.dockerfile'],
            'output_name': 'sdg_project_clean.txt',
            'recursive': True,
            'exclude_dirs': [
                'Book',
                'oss20b',
                'threads',
                'data', 
                '__pycache__',     # Python Cache
                '.git',            # Git Repository 
                '.pytest_cache',   # Pytest Cache
                'node_modules',    # Node.js Dependencies
                '.venv',           # Virtual Environment
                'venv',            # Virtual Environment
                '.idea',           # PyCharm Files
                '.vscode',         # VS Code Files
                'logs',            # Log-Verzeichnisse
                'temp',            # Temporäre Dateien
                '.*'               # Alle versteckten Ordner
            ],
            'exclude_patterns': [
                '*.pyc',           # Python Compiled Files
                '*.pyo',           # Python Optimized Files  
                '*.log',           # Log Files
                '*.tmp',           # Temporary Files
                '.DS_Store',       # macOS Files
                'Thumbs.db'        # Windows Thumbnails
            ],
            'exclude_extensions': ['.exe', '.dll', '.so', '.dylib', '.bin']
        },
        
        # Nur Source-Code, sehr restriktiv
        # '/home/user/sdg_project/src': {
        #     'files': 'all',
        #     'extensions': ['.py'],
        #     'output_name': 'source_code_only.txt',
        #     'recursive': True,
        #     'exclude_dirs': ['__pycache__', 'tests', '.pytest_cache'],
        #     'exclude_patterns': ['test_*.py', '*_test.py']
        # }
    }




output_base_path = '/mnt/gigabyte1tb/SDG/sdg_root'
write_files_with_directory_exclusion(folder_config, output_base_path)

print("Fertig! Output-File erzeugt.")


### /source /mnt/gigabyte1tb/SDG/sdg_root/scripts/rotate_jwt_keys.py
# /sdg_root/scripts/rotate_jwt_keys.py
"""
Script to rotate JWT keys for enhanced security
Run this periodically in production
"""
import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.auth.jwt_manager import JWTManager
from src.core.secrets_manager import secrets_manager
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def rotate_jwt_keys():
    """Rotate JWT signing keys"""
    try:
        logger.info("Starting JWT key rotation...")
        
        # Generate new keys
        jwt_manager = JWTManager()
        new_private_key, new_public_key = jwt_manager._generate_new_keys()
        
        logger.info("JWT keys rotated successfully")
        logger.info("Remember to update your environment variables with the new encrypted keys")
        logger.info("Old tokens will become invalid after the rotation")
        
    except Exception as e:
        logger.error(f"Error rotating JWT keys: {e}")
        raise

if __name__ == "__main__":
    rotate_jwt_keys()


### /source /mnt/gigabyte1tb/SDG/sdg_root/sdg_project_clean.txt
### /source /mnt/gigabyte1tb/SDG/sdg_root/docker-compose.yml
networks:
  sdg_internal:
    driver: bridge
    internal: true 
  sdg_external:
    driver: bridge

services:

  nginx_proxy:
    image: nginx:alpine
    container_name: nginx_proxy
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/ssl/certs:ro
    networks:
      - sdg_external
      - sdg_internal
    depends_on:
      - auth_service
      - api_service

  auth_service:
  build: 
    context: ${SRC_ROOT}/auth
    dockerfile: Dockerfile
  container_name: auth_service
  expose:
      - "8005"
  volumes:
      - ${SRC_ROOT}/auth:/app
  environment:
      - SECRET_KEY_ENCRYPTED=${SECRET_KEY_ENCRYPTED}
      - DATABASE_URL_ENCRYPTED=${DATABASE_URL_ENCRYPTED}
      - ALLOWED_ORIGINS=${ALLOWED_ORIGINS}
  networks:
    - sdg_internal
  depends_on:
    database_service:
      condition: service_healthy
  command: python -m uvicorn main:app --host 0.0.0.0 --port 8005 --reload
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:8005/health"]
    interval: 30s
    timeout: 10s
    retries: 3
  deploy:
    resources:
      limits:
        cpus: '0.5'
        memory: '512M'

  api_service:
    build: ${SRC_ROOT}/api
    container_name: api_service
    expose:
      - "8000"
    volumes:
      - ${SRC_ROOT}/api:/app
    environment:
      - DATABASE_URL_ENCRYPTED=${DATABASE_URL_ENCRYPTED}
    networks:
      - sdg_internal
    depends_on:
      database_service:
        condition: service_healthy
    command: python -m uvicorn main:app --host 0.0.0.0 --port 8000 --reload
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '1.0'

  database_service:
    image: postgres:16-alpine
    container_name: database_service
    environment:
      - POSTGRES_USER=${POSTGRES_USER:-postgres}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-postgres}
      - POSTGRES_DB=${POSTGRES_DB:-sdg_pipeline}
      - POSTGRES_INITDB_ARGS: "--encoding=UTF-8 --locale=en_US.UTF-8"
    volumes:
      - ${DATABASE_SERVICE_VOLUMES}
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '1.0'

  weaviate_service:
    # profiles: ["vector"]
    image: semitechnologies/weaviate:1.24.1
    container_name: weaviate_service
    ports:
      - "8080:8080"
    environment:
      - QUERY_DEFAULTS_LIMIT=25
      - AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED=true
      - DEFAULT_VECTORIZER_MODULE=text2vec-transformers
      - ENABLE_MODULES=text2vec-transformers
      - TRANSFORMERS_INFERENCE_API=http://weaviate_transformer_service:8080
      - PERSISTENCE_DATA_PATH=/var/lib/weaviate
      - CLUSTER_HOSTNAME=node1
      - MODULES_CLIENT_TIMEOUT=600s
    volumes:
      - ${WEAVIATE_SERVICE_VOLUMES}
    healthcheck:
      test: ["CMD-SHELL", "wget --spider --quiet http://localhost:8080/v1/meta || exit 1"] 
      interval: 30s
      timeout: 20s
      retries: 20
      start_period: 600s
    depends_on:
      weaviate_transformer_service:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: '2.0'    
          memory: '4G'   
        reservations:
          memory: '2G'
    
  weaviate_transformer_service:
    # profiles: ["vector"]
    image: ${WEAVIATE_IMAGE}
    container_name: weaviate_transformer_service
    ports:
      - "8081:8080"
    environment:
      - ENABLE_GPU=0
      - TORCH_NUM_THREADS=4               
      - OMP_NUM_THREADS=4
    healthcheck:
      test: ["CMD-SHELL", "python -c 'import urllib.request; urllib.request.urlopen(\"http://localhost:8080/.well-known/ready\").read()' || exit 1"]
      interval: 45s
      timeout: 30s
      retries: 25
      start_period: 600s
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: '8G'
        reservations:
          memory: '4G'

  vectorization_service:
    build: ${SRC_ROOT}/vectorization
    container_name: vectorization_service
    ports:
      - "8003:8003"
    volumes:
      - ${SRC_ROOT}/vectorization:/app
    environment:
      - WEAVIATE_URL=${WEAVIATE_URL}
      - DATABASE_URL=${DATABASE_URL}
    depends_on:
      weaviate_service:
        condition: service_healthy
    command: python -m uvicorn main:app --host 0.0.0.0 --port 8000 --reload

  
  data_retrieval_service:
    build: ${SRC_ROOT}/data_retrieval
    container_name: data_retrieval_service
    ports:
      - "8002:8002"
    volumes:
      - ${SRC_ROOT}/data_retrieval:/app
      - ${DATA_ROOT}/raw_data:/app/raw_data
      - ${DATA_ROOT}/quelle.txt:/app/quelle.txt
    environment:
      - DATABASE_URL=${DATABASE_URL}
    depends_on:
      database_service:
        condition: service_healthy
    command: python main.py
    deploy:
      resources:
        limits:
          cpus: '1.0'

  data_processing_service:
    # profiles: ["processing"]
    build: ${SRC_ROOT}/data_processing
    container_name: data_processing_service
    ports:
      - "8001:8001"
    volumes:
      - ${SRC_ROOT}/data_processing:/app
      - ${DATA_ROOT}/raw_data:/app/raw_data
      - ${DATA_ROOT}/processed_data:/app/processed_data
      - ${DATA_ROOT}/images:/app/images
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - WEAVIATE_URL=${WEAVIATE_URL}
    depends_on:
      data_retrieval_service:
        condition: service_started
      database_service:
        condition: service_healthy
      weaviate_service:
        condition: service_healthy
      weaviate_transformer_service:
        condition: service_healthy
    command: python main.py
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: '4G'
        reservations:
          cpus: '2'
          memory: '3G'
  
  content_extraction_service:
  build: ${SRC_ROOT}/content_extraction
  container_name: content_extraction_service
  ports:
    - "8004:8004"
  volumes:
    - ${SRC_ROOT}/content_extraction:/app
  environment:
    - DATABASE_URL=${DATABASE_URL}
    - DATA_PROCESSING_URL=http://data_processing_service:8001
    - API_SERVICE_URL=http://api_service:8000
  depends_on:
    database_service:
      condition: service_healthy
  command: python -m uvicorn main:app --host 0.0.0.0 --port 8004 --reload
  deploy:
    resources:
      limits:
        cpus: '1.0'
        memory: '2G'
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:8004/health"]
    interval: 30s
    timeout: 10s
    retries: 3

  pgadmin_service:
    image: dpage/pgadmin4:latest
    container_name: pgadmin_service
    ports:
      - "5050:80"
    environment:
      - PGADMIN_DEFAULT_EMAIL=${PGADMIN_DEFAULT_EMAIL}
      - PGADMIN_DEFAULT_PASSWORD=${POSTGRES_PASSWORD}
      - PGADMIN_CONFIG_SERVER_MODE=False
      - PGADMIN_CONFIG_MASTER_PASSWORD_REQUIRED=False
    volumes:
      - ${PGADMIN_VOLUMES}
    depends_on:
      database_service:
        condition: service_healthy
    user: "5050:5050"
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
  
  weaviate_console:
    image: semitechnologies/weaviate-console
    container_name: weaviate_console
    ports:
      - "3001:3000"
    environment:
      - WEAVIATE_HOST=weaviate_service:8080
    depends_on:
      weaviate_service:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: '256M'
  
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
    
  



### /source /mnt/gigabyte1tb/SDG/sdg_root/requirements-security.txt
# /sdg_root/requirements-security.txt
cryptography>=41.0.0
keyring>=24.0.0
bcrypt>=4.0.0
PyJWT[crypto]>=2.8.0
pydantic[email]>=2.0.0
python-multipart>=0.0.6
slowapi>=0.1.9  # Rate limiting


### /source /mnt/gigabyte1tb/SDG/sdg_root/requirements.txt
fastapi
uvicorn[standard]
python-dotenv


### /source /mnt/gigabyte1tb/SDG/sdg_root/scripts/concat_code.py
import os
from pathlib import Path

import os
from pathlib import Path

def write_files_with_directory_exclusion(folder_config, output_base_path):
    """
    Schreibt Dateien in Output-Files mit flexibler Verzeichnis-Ausschließung.
    
    Args:
        folder_config (dict): Konfiguration pro Ordner
            {ordnerpfad: {
                'files': ['specific1.py', 'specific2.py'] oder 'all',
                'extensions': ['.py', '.txt'] oder 'all',
                'output_name': 'custom_name.txt',
                'recursive': True/False,
                'exclude_dirs': ['__pycache__', '.git', 'node_modules'],  # NEU!
                'exclude_patterns': ['*.pyc', '*.log'],  # NEU!
                'exclude_extensions': ['.exe', '.bin']
            }}
        output_base_path (str): Basis-Pfad für alle Output-Files
    """
    
    for folder, config in folder_config.items():
        # Output-Datei bestimmen
        output_filename = config.get('output_name', f"{os.path.basename(folder)}_filtered.txt")
        output_path = os.path.join(output_base_path, output_filename)
        
        # Output-Verzeichnis erstellen
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as outfile:
            files_to_process = []
            exclude_dirs = config.get('exclude_dirs', [])
            exclude_patterns = config.get('exclude_patterns', [])
            
            # Rekursive Suche mit Verzeichnis-Ausschluss
            def get_files_with_exclusion(root_path):
                result_files = []
                
                for root, dirs, files in os.walk(root_path):
                    # Verzeichnisse ausschließen (modifiziert dirs in-place)
                    dirs[:] = [d for d in dirs if not should_exclude_directory(d, exclude_dirs, root)]
                    
                    for file in files:
                        file_path = os.path.join(root, file)
                        if not should_exclude_file(file_path, exclude_patterns):
                            result_files.append(file_path)
                
                return result_files
            
            # Bestimme welche Dateien verarbeitet werden sollen
            if config.get('recursive', False):
                if config['files'] == 'all':
                    # Alle Dateien rekursiv mit Ausschlüssen
                    files_to_process = get_files_with_exclusion(folder)
                else:
                    # Spezifische Dateien rekursiv suchen
                    all_files = get_files_with_exclusion(folder)
                    specific_files = set(config['files'])
                    files_to_process = [f for f in all_files if os.path.basename(f) in specific_files]
            else:
                # Nur Hauptordner (ohne Rekursion)
                if config['files'] == 'all':
                    files_to_process = [os.path.join(folder, f) for f in os.listdir(folder) 
                                     if os.path.isfile(os.path.join(folder, f)) 
                                     and not should_exclude_file(os.path.join(folder, f), exclude_patterns)]
                else:
                    for filename in config['files']:
                        file_path = os.path.join(folder, filename)
                        if os.path.exists(file_path) and not should_exclude_file(file_path, exclude_patterns):
                            files_to_process.append(file_path)
            
            # Extension-Filter anwenden
            if 'extensions' in config and config['extensions'] != 'all':
                allowed_extensions = [ext.lower() for ext in config['extensions']]
                files_to_process = [f for f in files_to_process 
                                  if Path(f).suffix.lower() in allowed_extensions]
            
            # Ausschließen bestimmter Dateiendungen
            if 'exclude_extensions' in config:
                excluded_extensions = [ext.lower() for ext in config['exclude_extensions']]
                files_to_process = [f for f in files_to_process 
                                  if Path(f).suffix.lower() not in excluded_extensions]
            
            # Dateien sortieren
            files_to_process.sort()
            
            # Jede Datei verarbeiten
            for file_path in files_to_process:
                if os.path.exists(file_path) and os.path.isfile(file_path):
                    outfile.write(f"### /source {file_path}\n")
                    
                    if is_text_file(file_path):
                        try:
                            with open(file_path, 'r', encoding='utf-8') as infile:
                                content = infile.read()
                                outfile.write(content)
                        except UnicodeDecodeError:
                            try:
                                with open(file_path, 'r', encoding='latin-1') as infile:
                                    content = infile.read()
                                    outfile.write(content)
                            except Exception as e:
                                outfile.write(f"# Error reading file {file_path}: {e}\n")
                    else:
                        file_size = os.path.getsize(file_path)
                        outfile.write(f"# Binary file (Size: {file_size} bytes) - Content not displayed\n")
                    
                    outfile.write('\n\n')
            
            print(f"✓ {len(files_to_process)} Dateien geschrieben nach: {output_path}")
            if exclude_dirs:
                print(f"  → Ausgeschlossene Verzeichnisse: {exclude_dirs}")

def should_exclude_directory(dirname, exclude_dirs, current_path):
    """Prüft ob ein Verzeichnis ausgeschlossen werden soll"""
    # Einfacher Name-Match
    if dirname in exclude_dirs:
        return True
    
    # Pattern-Match (z.B. für versteckte Ordner)
    import fnmatch
    for pattern in exclude_dirs:
        if fnmatch.fnmatch(dirname, pattern):
            return True
    
    # Pfad-basierte Ausschlüsse
    full_path = os.path.join(current_path, dirname)
    for exclude_pattern in exclude_dirs:
        if exclude_pattern.startswith('/') and exclude_pattern in full_path:
            return True
    
    return False

def should_exclude_file(filepath, exclude_patterns):
    """Prüft ob eine Datei aufgrund von Patterns ausgeschlossen werden soll"""
    import fnmatch
    filename = os.path.basename(filepath)
    
    for pattern in exclude_patterns:
        if fnmatch.fnmatch(filename, pattern) or fnmatch.fnmatch(filepath, pattern):
            return True
    return False

def is_text_file(file_path):
    """Prüft ob eine Datei als Text lesbar ist"""
    text_extensions = {
        '.txt', '.py', '.js', '.html', '.css', '.md', '.json', '.xml', '.yaml', '.yml',
        '.conf', '.cfg', '.ini', '.log', '.csv', '.sql', '.sh', '.bat', '.ps1',
        '.dockerfile', '.gitignore', '.env', '.properties', '.toml', '.requirements'
    }
    
    if Path(file_path).suffix.lower() in text_extensions:
        return True
    
    try:
        with open(file_path, 'rb') as f:
            chunk = f.read(1024)
            if b'\0' in chunk:
                return False
            try:
                chunk.decode('utf-8')
                return True
            except UnicodeDecodeError:
                return False
    except:
        return False

# BEISPIEL-VERWENDUNG mit Verzeichnis-Ausschluss:
if __name__ == "__main__":
    
    folder_config = {
        # SDG-Projekt komplett, aber ohne unnötige Ordner
        '/mnt/gigabyte1tb/SDG': {
            'files': 'all',
            'extensions': ['.py', '.yml', '.yaml', '.txt', '.md', '.env', '.dockerfile'],
            'output_name': 'sdg_project_clean.txt',
            'recursive': True,
            'exclude_dirs': [
                'Book',
                'oss20b',
                'threads',
                'data', 
                '__pycache__',     # Python Cache
                '.git',            # Git Repository 
                '.pytest_cache',   # Pytest Cache
                'node_modules',    # Node.js Dependencies
                '.venv',           # Virtual Environment
                'venv',            # Virtual Environment
                '.idea',           # PyCharm Files
                '.vscode',         # VS Code Files
                'logs',            # Log-Verzeichnisse
                'temp',            # Temporäre Dateien
                '.*'               # Alle versteckten Ordner
            ],
            'exclude_patterns': [
                '*.pyc',           # Python Compiled Files
                '*.pyo',           # Python Optimized Files  
                '*.log',           # Log Files
                '*.tmp',           # Temporary Files
                '.DS_Store',       # macOS Files
                'Thumbs.db'        # Windows Thumbnails
            ],
            'exclude_extensions': ['.exe', '.dll', '.so', '.dylib', '.bin']
        },
        
        # Nur Source-Code, sehr restriktiv
        # '/home/user/sdg_project/src': {
        #     'files': 'all',
        #     'extensions': ['.py'],
        #     'output_name': 'source_code_only.txt',
        #     'recursive': True,
        #     'exclude_dirs': ['__pycache__', 'tests', '.pytest_cache'],
        #     'exclude_patterns': ['test_*.py', '*_test.py']
        # }
    }




output_base_path = '/mnt/gigabyte1tb/SDG/sdg_root'
write_files_with_directory_exclusion(folder_config, output_base_path)

print("Fertig! Output-File erzeugt.")


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/api/API_Service.txt
API_Service.txt

API Service (FastAPI)
Dieser Service stellt die Verbindung zwischen dem Frontend der Plattform und den internen Diensten her.

Aufgaben:

Frontend-Kommunikation: Empfängt Anfragen von der Benutzeroberfläche (z. B. Abfragen für interaktive Dashboards oder Szenarien).

Abfrage-Verwaltung: Verarbeitet die Anfragen und leitet sie an die entsprechenden Backend-Services weiter. Beispielsweise würde eine Suchanfrage an den Database Service geleitet, um relevante Vektoren und Metadaten abzurufen.

Ergebnis-Aggregation: Konsolidiert die Ergebnisse aus den verschiedenen Diensten und bereitet sie für die Ausgabe an das Frontend auf.


Sicherheit: Stellt sicher, dass alle Datenabfragen und -transaktionen sicher sind (z. B. Authentifizierung, Autorisierung, DSGVO-Konformität )


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/api/database.py
import os
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from sqlalchemy.pool import StaticPool
from .core.secrets_manager import secrets_manager

import logging

logger = logging.getLogger(__name__)


def get_database_url():
    """Get database URL with encrypted credentials"""
    try:
        db_host = os.environ.get("DB_HOST", "database_service")
        db_port = os.environ.get("DB_PORT", "5432")
        db_name = secrets_manager.get_secret("POSTGRES_DB")
        db_user = secrets_manager.get_secret("POSTGRES_USER")
        db_password = secrets_manager.get_secret("POSTGRES_PASSWORD")
        
        return f"postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}"
    except Exception as e:
        logger.error(f"Error constructing database URL: {e}")
        raise

DATABASE_URL = get_database_url()

# Add connection security settings
engine_kwargs = {
    "pool_pre_ping": True,
    "pool_recycle": 300,
    "pool_size": 10,
    "max_overflow": 20,
    "connect_args": {
        "sslmode": "require",  # Require SSL
        "options": "-c default_transaction_isolation=serializable"
    }
}

if DATABASE_URL.startswith("sqlite"):
    engine_kwargs.update({
        "poolclass": StaticPool,
        "connect_args": {"check_same_thread": False}
    })

engine = create_engine(DATABASE_URL, **engine_kwargs)

SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

def get_db():
    """Database session dependency with proper error handling"""
    db = SessionLocal()
    try:
        yield db
    except Exception as e:
        logger.error(f"Database session error: {e}")
        db.rollback()
        raise
    finally:
        db.close()

def check_database_health():
    """Check if database is accessible"""
    try:
        with engine.connect() as conn:
            conn.execute("SELECT 1")
        return True
    except Exception as e:
        logger.error(f"Database health check failed: {e}")
        return False


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/api/main.py
from fastapi import FastAPI, Depends, HTTPException
from sqlalchemy.orm import Session
from typing import List
import models, schemas
from database import get_db

app = FastAPI()

def read_root():
    return {"message": "API Service is running!"}

# --- CRUD Endpunkte für Articles ---

@app.post("/articles/", response_model=schemas.Article)
def create_article(article: schemas.ArticleCreate, db: Session = Depends(get_db)):
    db_article = models.Article(**article.dict())
    db.add(db_article)
    db.commit()
    db.refresh(db_article)
    return db_article

@app.get("/articles/", response_model=List[schemas.Article])
def read_articles(db: Session = Depends(get_db)):
    articles = db.query(models.Article).all()
    return articles

@app.get("/articles/{article_id}", response_model=schemas.Article)
def read_article(article_id: int, db: Session = Depends(get_db)):
    db_article = db.query(models.Article).filter(models.Article.id == article_id).first()
    if db_article is None:
        raise HTTPException(status_code=404, detail="Article not found")
    return db_article

@app.get("/articles/{article_id}/chunks")
async def get_article_chunks(
    article_id: int, 
    sdg_filter: Optional[str] = None,
    limit: int = 10,
    db: Session = Depends(get_db)
):
    article = db.query(models.Article).filter(models.Article.id == article_id).first()
    if not article:
        raise HTTPException(status_code=404, detail="Article not found")
    

    query = db.query(models.ArticleChunk).filter(models.ArticleChunk.article_id == article_id)
    
    if sdg_filter:
        query = query.filter(models.ArticleChunk.sdg_section == sdg_filter)
    
    chunks = query.order_by(models.ArticleChunk.chunk_order).limit(limit).all()
    
    return {
        "article_id": article_id,
        "total_chunks": len(chunks),
        "chunks": chunks
    }

@app.get("/search/sdg/{sdg_id}/chunks")
async def search_chunks_by_sdg(
    sdg_id: int,
    query: Optional[str] = None,
    limit: int = 5,
    db: Session = Depends(get_db)
):
    sdg = db.query(models.Sdg).filter(models.Sdg.id == sdg_id).first()
    if not sdg:
        raise HTTPException(status_code=404, detail="SDG not found")
    
    chunks_query = db.query(models.ArticleChunk).join(models.Article).filter(
        models.Article.sdg_id == sdg_id
    )
    
    if query:
        chunks_query = chunks_query.filter(models.ArticleChunk.text.contains(query))
    
    chunks = chunks_query.limit(limit).all()
    
    return {
        "sdg_id": sdg_id,
        "sdg_name": sdg.name,
        "query": query,
        "results": chunks
    }

@app.get("/articles/{article_id}/summary")
async def get_article_summary(
    article_id: int,
    max_chunks: int = 5,
    db: Session = Depends(get_db)
):
    article = db.query(models.Article).filter(models.Article.id == article_id).first()
    if not article:
        raise HTTPException(status_code=404, detail="Article not found")
    
    # Get most relevant chunks (ordered by importance/SDG relevance)
    chunks = db.query(models.ArticleChunk).filter(
        models.ArticleChunk.article_id == article_id
    ).order_by(models.ArticleChunk.chunk_order).limit(max_chunks).all()
    
    if not chunks:
        raise HTTPException(status_code=404, detail="No chunks found for article")
    
    # Generate summary from chunks
    summary_text = " ".join([chunk.text[:200] + "..." for chunk in chunks])
    
    # Extract SDG information
    sdg_sections = list(set([chunk.sdg_section for chunk in chunks if chunk.sdg_section]))
    
    return {
        "article_id": article_id,
        "article_title": article.title,
        "summary": summary_text,
        "sdg_sections": sdg_sections,
        "chunk_count": len(chunks),
        "generated_at": "2025-08-22T15:29:00Z"
    }

@app.post("/images/", response_model=schemas.ImageBase)
def create_image(image: schemas.ImageBase, db: Session = Depends(get_db)):
    db_image = models.Image(**image.dict())
    db.add(db_image)
    db.commit()
    db.refresh(db_image)
    return db_image

@app.get("/images/{article_id}", response_model=List[schemas.ImageBase])
def get_images(article_id: int, db: Session = Depends(get_db)):
    images = db.query(models.Image).filter(models.Image.article_id == article_id).all()
    return images

# --- CRUD Endpunkte für SDGs ---

@app.post("/sdgs/", response_model=schemas.Sdg)
def create_sdg(sdg: schemas.SdgCreate, db: Session = Depends(get_db)):
    db_sdg = models.Sdg(**sdg.dict())
    db.add(db_sdg)
    db.commit()
    db.refresh(db_sdg)
    return db_sdg

@app.get("/sdgs/", response_model=List[schemas.Sdg])
def read_sdgs(db: Session = Depends(get_db)):
    sdgs = db.query(models.Sdg).all()
    return sdgs

# --- CRUD Endpunkte für Actors ---

@app.post("/actors/", response_model=schemas.Actor)
def create_actor(actor: schemas.ActorCreate, db: Session = Depends(get_db)):
    db_actor = models.Actor(**actor.dict())
    db.add(db_actor)
    db.commit()
    db.refresh(db_actor)
    return db_actor

@app.get("/actors/", response_model=List[schemas.Actor])
def read_actors(db: Session = Depends(get_db)):
    actors = db.query(models.Actor).all()
    return actors


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/api/migrate_existing_schema.py
"""
Migration script to enhance existing schema without breaking changes
"""
import logging
from sqlalchemy import create_engine, text, inspect, MetaData, Table, Column, String, Integer, Float, Boolean, DateTime, JSON
from sqlalchemy.sql import func
import os

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

DATABASE_URL = os.environ.get("DATABASE_URL")
engine = create_engine(DATABASE_URL)

def migrate_existing_schema():

    logger.info("Starting schema migration...")
    
    with engine.begin() as conn:
        
        try:
            conn.execute(text("""
                ALTER TABLE sdgs 
                ADD COLUMN IF NOT EXISTS goal_number INTEGER,
                ADD COLUMN IF NOT EXISTS name_de VARCHAR,
                ADD COLUMN IF NOT EXISTS name_fr VARCHAR,
                ADD COLUMN IF NOT EXISTS name_es VARCHAR,
                ADD COLUMN IF NOT EXISTS name_zh VARCHAR,
                ADD COLUMN IF NOT EXISTS name_hi VARCHAR,
                ADD COLUMN IF NOT EXISTS description_de TEXT,
                ADD COLUMN IF NOT EXISTS description_fr TEXT,
                ADD COLUMN IF NOT EXISTS description_es TEXT,
                ADD COLUMN IF NOT EXISTS description_zh TEXT,
                ADD COLUMN IF NOT EXISTS description_hi TEXT,
                ADD COLUMN IF NOT EXISTS color_hex VARCHAR(7),
                ADD COLUMN IF NOT EXISTS icon_url VARCHAR(500),
                ADD COLUMN IF NOT EXISTS priority_weight FLOAT DEFAULT 1.0,
                ADD COLUMN IF NOT EXISTS created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                ADD COLUMN IF NOT EXISTS updated_at TIMESTAMP WITH TIME ZONE
            """))
            logger.info("✅ Enhanced SDGs table")
        except Exception as e:
            logger.warning(f"SDGs table enhancement: {e}")

        try:
            conn.execute(text("""
                ALTER TABLE articles 
                ADD COLUMN IF NOT EXISTS summary TEXT,
                ADD COLUMN IF NOT EXISTS sdg_confidence FLOAT DEFAULT 0.0,
                ADD COLUMN IF NOT EXISTS publication_date TIMESTAMP WITH TIME ZONE,
                ADD COLUMN IF NOT EXISTS country_code VARCHAR(3),
                ADD COLUMN IF NOT EXISTS language VARCHAR(5) DEFAULT 'en',
                ADD COLUMN IF NOT EXISTS word_count INTEGER,
                ADD COLUMN IF NOT EXISTS readability_score FLOAT,
                ADD COLUMN IF NOT EXISTS content_quality_score FLOAT DEFAULT 0.0,
                ADD COLUMN IF NOT EXISTS has_embeddings BOOLEAN DEFAULT FALSE,
                ADD COLUMN IF NOT EXISTS embedding_model VARCHAR(100),
                ADD COLUMN IF NOT EXISTS embedding_dimension INTEGER,
                ADD COLUMN IF NOT EXISTS updated_at TIMESTAMP WITH TIME ZONE,
                ADD COLUMN IF NOT EXISTS processed_at TIMESTAMP WITH TIME ZONE
            """))
            logger.info("✅ Enhanced Articles table")
        except Exception as e:
            logger.warning(f"Articles table enhancement: {e}")

        try:
            conn.execute(text("""
                ALTER TABLE article_chunks 
                ADD COLUMN IF NOT EXISTS chunk_order INTEGER,
                ADD COLUMN IF NOT EXISTS sdg_relevance_scores JSON,
                ADD COLUMN IF NOT EXISTS confidence_score FLOAT DEFAULT 0.0,
                ADD COLUMN IF NOT EXISTS has_embedding BOOLEAN DEFAULT FALSE,
                ADD COLUMN IF NOT EXISTS embedding_hash VARCHAR(64)
            """))
            logger.info("✅ Enhanced ArticleChunks table")
        except Exception as e:
            logger.warning(f"ArticleChunks table enhancement: {e}")

        try:
            conn.execute(text("""
                ALTER TABLE actors 
                ADD COLUMN IF NOT EXISTS region VARCHAR(100),
                ADD COLUMN IF NOT EXISTS is_active BOOLEAN DEFAULT TRUE,
                ADD COLUMN IF NOT EXISTS created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
            """))
            logger.info("✅ Enhanced Actors table")
        except Exception as e:
            logger.warning(f"Actors table enhancement: {e}")

        
        try:
            conn.execute(text("""
                CREATE TABLE IF NOT EXISTS sdg_interlinkages (
                    id SERIAL PRIMARY KEY,
                    from_sdg_id INTEGER REFERENCES sdgs(id),
                    to_sdg_id INTEGER REFERENCES sdgs(id),
                    relationship_type VARCHAR(50) NOT NULL,
                    strength FLOAT NOT NULL CHECK (strength >= 0.0 AND strength <= 1.0),
                    evidence_level VARCHAR(20) DEFAULT 'medium',
                    source VARCHAR(200),
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
                )
            """))
            logger.info("✅ Created SDG Interlinkages table")
        except Exception as e:
            logger.warning(f"SDG Interlinkages table creation: {e}")

        
        try:
            conn.execute(text("""
                CREATE TABLE IF NOT EXISTS articles_sdg_targets (
                    article_id INTEGER REFERENCES articles(id) ON DELETE CASCADE,
                    sdg_id INTEGER REFERENCES sdgs(id) ON DELETE CASCADE,
                    confidence_score FLOAT DEFAULT 0.0,
                    PRIMARY KEY (article_id, sdg_id)
                )
            """))
            logger.info("✅ Created Articles-SDGs many-to-many table")
        except Exception as e:
            logger.warning(f"Articles-SDGs table creation: {e}")

        indexes = [
            "CREATE INDEX IF NOT EXISTS idx_articles_sdg_id ON articles(sdg_id)",
            "CREATE INDEX IF NOT EXISTS idx_articles_region ON articles(region)",
            "CREATE INDEX IF NOT EXISTS idx_articles_publication_year ON articles(publication_year)",
            "CREATE INDEX IF NOT EXISTS idx_articles_language ON articles(language)",
            "CREATE INDEX IF NOT EXISTS idx_articles_has_embeddings ON articles(has_embeddings)",
            "CREATE INDEX IF NOT EXISTS idx_sdg_progress_year ON sdg_progress(year)",
            "CREATE INDEX IF NOT EXISTS idx_chunks_article_order ON article_chunks(article_id, chunk_order)",
            "CREATE INDEX IF NOT EXISTS idx_sdgs_goal_number ON sdgs(goal_number)"
        ]
        
        for index_sql in indexes:
            try:
                conn.execute(text(index_sql))
            except Exception as e:
                logger.warning(f"Index creation: {e}")
        
        logger.info("✅ Created performance indexes")

        try:
            sdg_updates = [
                ("No Poverty", 1, "#E5243B"),
                ("Zero Hunger", 2, "#DDA63A"),
                ("Good Health and Well-being", 3, "#4C9F38"),
                ("Quality Education", 4, "#C5192D"),
                ("Gender Equality", 5, "#FF3A21"),
                ("Clean Water and Sanitation", 6, "#26BDE2"),
                ("Affordable and Clean Energy", 7, "#FCC30B"),
                ("Decent Work and Economic Growth", 8, "#A21942"),
                ("Industry, Innovation and Infrastructure", 9, "#FD6925"),
                ("Reduced Inequalities", 10, "#DD1367"),
                ("Sustainable Cities and Communities", 11, "#FD9D24"),
                ("Responsible Consumption and Production", 12, "#BF8B2E"),
                ("Climate Action", 13, "#3F7E44"),
                ("Life Below Water", 14, "#0A97D9"),
                ("Life on Land", 15, "#56C02B"),
                ("Peace, Justice and Strong Institutions", 16, "#00689D"),
                ("Partnerships for the Goals", 17, "#19486A")
            ]
            
            for name, goal_number, color in sdg_updates:
                conn.execute(text("""
                    UPDATE sdgs 
                    SET goal_number = :goal_number, color_hex = :color
                    WHERE name ILIKE :name AND goal_number IS NULL
                """), {"name": f"%{name}%", "goal_number": goal_number, "color": color})
            
            logger.info("✅ Updated existing SDG data")
        except Exception as e:
            logger.warning(f"SDG data update: {e}")
        
            conn.execute(text("""
                    CREATE TABLE IF NOT EXISTS sdg_targets (
                        target_id VARCHAR(10) PRIMARY KEY,
                        goal_id INTEGER REFERENCES sdgs(id) ON DELETE CASCADE,
                        title_en TEXT NOT NULL,
                        title_de TEXT,
                        title_fr TEXT,
                        title_es TEXT,
                        title_zh TEXT,
                        title_hi TEXT,
                        description TEXT,
                        description_de TEXT,
                        description_fr TEXT,
                        description_es TEXT,
                        description_zh TEXT,
                        description_hi TEXT,
                        target_type VARCHAR(50),
                        deadline_year INTEGER DEFAULT 2030,
                        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                        updated_at TIMESTAMP WITH TIME ZONE
                    );
                """))
                

            conn.execute(text("""
                CREATE TABLE IF NOT EXISTS sdg_indicators (
                    indicator_id VARCHAR(20) PRIMARY KEY,
                    target_id VARCHAR(10) REFERENCES sdg_targets(target_id) ON DELETE CASCADE,
                    title_en TEXT NOT NULL,
                    title_de TEXT,
                    title_fr TEXT,
                    title_es TEXT,
                    title_zh TEXT,
                    title_hi TEXT,
                    unit_of_measurement TEXT,
                    data_source TEXT,
                    methodology TEXT,
                    tier_classification VARCHAR(10),
                    custodian_agency TEXT,
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                    updated_at TIMESTAMP WITH TIME ZONE
                );
            """))
            
            # Create indexes
            conn.execute(text("CREATE INDEX IF NOT EXISTS idx_sdg_targets_goal_id ON sdg_targets(goal_id);"))
            conn.execute(text("CREATE INDEX IF NOT EXISTS idx_sdg_indicators_target_id ON sdg_indicators(target_id);"))
                
            logger.info("✅ Created SDG targets and indicators tables")
            
        except Exception as e:
            logger.error(f"Error creating SDG targets/indicators: {e}")

    logger.info("Schema migration completed successfully!")



if __name__ == "__main__":
    migrate_existing_schema()


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/api/models.py
import os

from sqlalchemy import create_engine, Column, Integer, String, Text, Float, ForeignKey, DateTime, Table, JSON, Boolean

from sqlalchemy.orm import relationship, sessionmaker, declarative_base

from sqlalchemy.sql import func

from datetime import datetime

DATABASE_URL = os.environ.get("DATABASE_URL")

engine = create_engine(DATABASE_URL, pool_pre_ping=True)

Base = declarative_base()


articles_tags = Table(
    'articles_tags',
    Base.metadata,
    Column('article_id', Integer, ForeignKey('articles.id')),
    Column('tag_id', Integer, ForeignKey('tags.id'))
)


articles_ai_topics = Table(
    'articles_ai_topics',
    Base.metadata,
    Column('article_id', Integer, ForeignKey('articles.id')),
    Column('ai_topic_id', Integer, ForeignKey('ai_topics.id'))
)

articles_sdg_targets = Table(
    'articles_sdg_targets',
    Base.metadata,
    Column('article_id', Integer, ForeignKey('articles.id'), primary_key=True),
    Column('sdg_id', Integer, ForeignKey('sdgs.id'), primary_key=True),
    Column('confidence_score', Float, default=0.0)
)

class User(Base):
    __tablename__ = "users"
    id = Column(Integer, primary_key=True, index=True)
    username = Column(String, unique=True, index=True)
    email = Column(String, unique=True, index=True)
    hashed_password = Column(String)

class Sdg(Base):
    __tablename__ = "sdgs"
    id = Column(Integer, primary_key=True, index=True)
    name = Column(String, unique=True, index=True)
    name_de = Column(String)  
    name_fr = Column(String)  
    name_es = Column(String)  
    name_zh = Column(String)  
    name_hi = Column(String)
    description = Column(Text)
    description_de = Column(Text) 
    description_fr = Column(Text)
    description_es = Column(Text)
    description_zh = Column(Text)
    description_hi = Column(Text)
    color_hex = Column(String(7))  
    icon_url = Column(String(500)) 
    priority_weight = Column(Float, default=1.0) 
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), onupdate=func.now())
    
    progress = relationship("SdgProgress", back_populates="sdg")
    articles_multi = relationship("Article", secondary=articles_sdg_targets, back_populates="sdgs_multi")
class SDGTarget(Base):
    __tablename__ = "sdg_targets"
    target_id = Column(String(10), primary_key=True)  # "1.1", "1.2", etc.
    goal_id = Column(Integer, ForeignKey("sdgs.id"))
    title_en = Column(Text, nullable=False)
    title_de = Column(Text)
    title_fr = Column(Text)
    title_es = Column(Text)
    title_zh = Column(Text)
    title_hi = Column(Text)

    description = Column(Text)
    description_de = Column(Text)
    description_fr = Column(Text)
    description_es = Column(Text)
    description_zh = Column(Text)
    description_hi = Column(Text)

    target_type = Column(String(50)) 
    deadline_year = Column(Integer, default=2030)
    
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), onupdate=func.now())
    
    # Relationships
    goal = relationship("Sdg", back_populates="targets")
    indicators = relationship("SDGIndicator", back_populates="target")
class SDGIndicator(Base):
    __tablename__ = "sdg_indicators"
    indicator_id = Column(String(20), primary_key=True)
    target_id = Column(String(10), ForeignKey("sdg_targets.target_id"))
    title_en = Column(Text, nullable=False)
    title_de = Column(Text)
    title_fr = Column(Text)
    title_es = Column(Text)
    title_zh = Column(Text)
    title_hi = Column(Text)
    
    unit_of_measurement = Column(Text)
    data_source = Column(Text)
    methodology = Column(Text)
    tier_classification = Column(String(10))
    custodian_agency = Column(Text)
    
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), onupdate=func.now())

    target = relationship("SDGTarget", back_populates="indicators")


class Actor(Base):
    __tablename__ = "actors"
    id = Column(Integer, primary_key=True, index=True)
    name = Column(String, unique=True, index=True)
    type = Column(String)
    country_code = Column(String(3), index=True) 
    region = Column(String(100), index=True) 
    is_active = Column(Boolean, default=True) 
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    progress = relationship("SdgProgress", back_populates="actor")

class AiTopic(Base):
    __tablename__ = "ai_topics"
    id = Column(Integer, primary_key=True, index=True)
    name = Column(String, unique=True, index=True)
    description = Column(Text) 
    category = Column(String(100), index=True) 
    sdg_relevance = Column(JSON) 
    maturity_level = Column(String(50)) 
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    articles = relationship("Article", secondary=articles_ai_topics, back_populates="ai_topics")

class Article(Base):
    __tablename__ = "articles"
    id = Column(Integer, primary_key=True, index=True)
    title = Column(String, index=True)
    content_original = Column(Text)
    content_english = Column(Text)
    summary = Column(Text)
    keywords = Column(Text)
    sdg_id = Column(Integer, ForeignKey("sdgs.id"), index=True)
    sdg_confidence = Column(Float, default=0.0)
    
    authors = Column(Text)
    publication_year = Column(Integer)
    publication_date = Column(DateTime(timezone=True), index=True)
    publisher = Column(String)
    doi = Column(String, unique=True)
    isbn = Column(String, unique=True)
    region = Column(Text(100), index=True)
    country_code = Column(String(3), index=True) 
    language = Column(String(5), default="en", index=True) 
    
    context = Column(String)
    study_type = Column(String, index=True)
    research_methods = Column(String)
    data_sources = Column(String)
    funding = Column(Text)
    funding_info = Column(String)
    bias_indicators = Column(String)
    abstract_original = Column(Text)
    abstract_english = Column(Text)
    relevance_questions = Column(String)
    source_url = Column(String(2000), unique=True, index=True)
    availability = Column(String)
    
    citation_count = Column(Integer, default=0)
    impact_metrics = Column(JSON)
    impact_factor = Column(Float)
    policy_impact = Column(String)

    has_embeddings = Column(Boolean, default=False, index=True)
    embedding_model = Column(String(100))
    embedding_dimension = Column(Integer)
    
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime(timezone=True), onupdate=func.now())
    processed_at = Column(DateTime(timezone=True))
    
    tags = relationship("Tag", secondary=articles_tags, back_populates="articles")
    ai_topics = relationship("AiTopic", secondary=articles_ai_topics, back_populates="articles")
    image_paths = relationship("Image", back_populates="article")
    chunks = relationship("ArticleChunk", back_populates="article", cascade="all, delete-orphan")
    sdgs_multi = relationship("Sdg", secondary=articles_sdg_targets, back_populates="articles_multi")
    primary_sdg = relationship("Sdg", foreign_keys=[sdg_id])


class ArticleChunk(Base):
    __tablename__ = "article_chunks"
    id = Column(Integer, primary_key=True, index=True, index=True)
    article_id = Column(Integer, ForeignKey("articles.id"))
    chunk_id = Column(Integer)
    chunk_order = Column(Integer, index=True)
    text = Column(Text)
    chunk_length = Column(Integer)
    sdg_section = Column(String, index=True)
    sub_section_id = Column(Integer)
    sdg_relevance_scores = Column(JSON) 
    confidence_score = Column(Float, default=0.0)
    has_embedding = Column(Boolean, default=False)
    embedding_hash = Column(String(64))
    created_at = Column(DateTime, default=datetime.utcnow)
    article = relationship("Article", back_populates="chunks")

class SdgProgress(Base):
    __tablename__ = "sdg_progress"
    id = Column(Integer, primary_key=True, index=True)
    actor_id = Column(Integer, ForeignKey("actors.id"), index=True)
    sdg_id = Column(Integer, ForeignKey("sdgs.id"), index=True)
    score = Column(Float)
    year = Column(Integer)
    progress_status = Column(String(50)) 
    trend_direction = Column(String(20)) 
    data_quality = Column(String(20), default="medium") 
    data_sources = Column(JSON) 
    confidence_level = Column(Float, default=0.5) 
    
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), onupdate=func.now())
    actor = relationship("Actor", back_populates="progress")
    sdg = relationship("Sdg", back_populates="progress")

class Tag(Base):
    __tablename__ = "tags"
    id = Column(Integer, primary_key=True, index=True)
    name = Column(String, unique=True, index=True)
    category = Column(String(100), index=True) 
    usage_count = Column(Integer, default=0)
    articles = relationship("Article", secondary=articles_tags, back_populates="articles")


class Image(Base):
    __tablename__ = "images"
    id = Column(Integer, primary_key=True, index=True)
    article_id = Column(Integer, ForeignKey('articles.id'), index=True)
    original_path = Column(String(500))
    ocr_text = Column(Text)
    page = Column(Integer)
    caption = Column(Text)
    sdg_tags = Column(JSON)
    ai_tags = Column(Text)
    image_type = Column(String)
    file_size = Column(Integer)
    width = Column(Integer)
    height = Column(Integer)
    format = Column(String(10))
    processed_at = Column(DateTime(timezone=True))
    article = relationship("Article", back_populates="image_paths")

class SdgInterlinkage(Base):
    """SDG goal interlinkages"""
    __tablename__ = "sdg_interlinkages"
    
    id = Column(Integer, primary_key=True, index=True)
    from_sdg_id = Column(Integer, ForeignKey("sdgs.id"), index=True)
    to_sdg_id = Column(Integer, ForeignKey("sdgs.id"), index=True)
    relationship_type = Column(String(50)) 
    strength = Column(Float, nullable=False) 
    evidence_level = Column(String(20), default="medium")
    source = Column(String(200))
    
    from_sdg = relationship("Sdg", foreign_keys=[from_sdg_id])
    to_sdg = relationship("Sdg", foreign_keys=[to_sdg_id])


Index('idx_articles_sdg_region_year', Article.sdg_id, Article.region, Article.publication_year)
Index('idx_articles_language_quality', Article.language, Article.content_quality_score)
Index('idx_articles_has_embeddings', Article.has_embeddings)
Index('idx_chunks_article_order', ArticleChunk.article_id, ArticleChunk.chunk_order)
Index('idx_progress_actor_sdg_year', SdgProgress.actor_id, SdgProgress.sdg_id, SdgProgress.year)

Base.metadata.create_all(bind=engine)


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/api/requirements.txt
fastapi
uvicorn[standard]
python-dotenv
pydantic
sqlalchemy[asyncio]
psycopg2-binaryfastapi==0.104.1
uvicorn[standard]==0.24.0
python-dotenv==1.0.0
pydantic==2.5.0
sqlalchemy[asyncio]==2.0.23
psycopg2-binary==2.9.7
alembic==1.13.1
redis==5.0.1
celery==5.3.4
python-multipart==0.0.6
python-jose[cryptography]==3.3.0
passlib[bcrypt]==1.7.4
python-dateutil==2.8.2


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/api/schemas.py
from pydantic import BaseModel
from datetime import datetime
from typing import Optional, List

class AiTopicBase(BaseModel):
    name: str

class AiTopicCreate(AiTopicBase):
    pass

class AiTopic(AiTopicBase):
    id: int

    class Config:
        from_attributes = True

class TagBase(BaseModel):
    name: str

class TagCreate(TagBase):
    pass

class Tag(TagBase):
    id: int

    class Config:
        from_attributes = True

class SdgBase(BaseModel):
    name: str
    description: str

class SdgCreate(SdgBase):
    pass

class Sdg(SdgBase):
    id: int

    class Config:
        from_attributes = True

class ActorBase(BaseModel):
    name: str
    type: str
    country_code: Optional[str] = None

class ActorCreate(ActorBase):
    pass

class Actor(ActorBase):
    id: int

    class Config:
        from_attributes = True

class SdgProgressBase(BaseModel):
    actor_id: int
    sdg_id: int
    score: float
    year: int

class SdgProgress(SdgProgressBase):
    id: int

    class Config:
        from_attributes = True

class ArticleBase(BaseModel):
    title: str
    content_original: Optional[str]
    content_english: Optional[str]
    keywords: Optional[str]
    sdg_id: Optional[int]
    authors: Optional[str]
    publication_year: Optional[int]
    publisher: Optional[str]
    doi: Optional[str]
    isbn: Optional[str]
    region: Optional[str]
    context: Optional[str]
    study_type: Optional[str]
    research_methods: Optional[str]
    data_sources: Optional[str]
    funding: Optional[str]
    funding_info: Optional[str]
    bias_indicators: Optional[str]
    abstract_original: Optional[str]
    abstract_english: Optional[str]
    relevance_questions: Optional[str]
    source_url: Optional[str]
    availability: Optional[str]
    citation_count: Optional[int]
    impact_metrics: Optional[dict]
    impact_factor: Optional[float]
    policy_impact: Optional[str]
    tags: Optional[List[str]] = []
    ai_topics: Optional[List[str]] = []

class ImageBase(BaseModel):
    article_id: int
    original_path: str
    ocr_text: Optional[str]
    page: Optional[int]
    caption: Optional[str]
    sdg_tags: Optional[dict]
    ai_tags: Optional[str]
    image_type: Optional[str]

class ArticleCreate(ArticleBase):
    pass

class Article(ArticleBase):
    id: int
    created_at: datetime
    tags: List[Tag] = []
    ai_topics: List[AiTopic] = []

class ArticleChunkBase(BaseModel):
    chunk_order: int
    text: str
    chunk_length: Optional[int] = None
    sdg_section: Optional[str] = None
    confidence_score: Optional[float] = 0.0

class ArticleChunkCreate(ArticleChunkBase):
    article_id: int

class ArticleChunk(ArticleChunkBase):
    id: int
    article_id: int
    created_at: datetime
    
    class Config:
        from_attributes = True

class Article(ArticleBase):
    id: int
    created_at: datetime
    tags: List[Tag] = []
    ai_topics: List[AiTopic] = []
    chunks: List[ArticleChunk] = []  # NEW LINE
    
class SDGTargetBase(BaseModel):
    target_id: str
    goal_id: int
    title_en: str
    title_de: Optional[str] = None
    title_fr: Optional[str] = None
    title_es: Optional[str] = None
    title_zh: Optional[str] = None
    title_hi: Optional[str] = None

class SDGTargetCreate(SDGTargetBase):
    pass

class SDGTarget(SDGTargetBase):
    created_at: datetime
    updated_at: Optional[datetime] = None
    
    class Config:
        from_attributes = True

class SDGIndicatorBase(BaseModel):
    indicator_id: str
    target_id: str
    title_en: str
    title_de: Optional[str] = None
    title_fr: Optional[str] = None
    title_es: Optional[str] = None
    title_zh: Optional[str] = None
    title_hi: Optional[str] = None

class SDGIndicator(SDGIndicatorBase):
    created_at: datetime
    
class Config:
    from_attributes = True




### /source /mnt/gigabyte1tb/SDG/sdg_root/src/auth/config.py
# /sdg_root/src/core/config.py
import os
from enum import Enum
from pydantic import BaseSettings

class Environment(str, Enum):
    DEVELOPMENT = "development"
    STAGING = "staging"
    PRODUCTION = "production"

class Settings(BaseSettings):
    environment: Environment = Environment.DEVELOPMENT
    debug: bool = False
    
    # Security settings
    allowed_origins: list = []
    jwt_algorithm: str = "RS256"
    password_min_length: int = 8
    
    # Rate limiting
    rate_limit_per_minute: int = 60
    auth_rate_limit_per_minute: int = 5
    
    # File upload limits
    max_file_size_mb: int = 50
    allowed_file_types: list = [".pdf", ".txt", ".docx", ".csv"]
    
    class Config:
        env_file = ".env"
        case_sensitive = False

settings = Settings()


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/auth/jwt_manager.py
# /sdg_root/src/auth/jwt_manager.py
import os
import jwt
import secrets
import logging
from datetime import datetime, timedelta
from typing import Dict, Any, Optional
from cryptography.hazmat.primitives import serialization
from cryptography.hazmat.primitives.asymmetric import rsa
from ..core.secrets_manager import secrets_manager

logger = logging.getLogger(__name__)

class JWTManager:
    def __init__(self):
        self.algorithm = "RS256"  # Use RSA instead of HMAC for better security
        self.access_token_expire_minutes = 30
        self.refresh_token_expire_days = 7
        
        # Load or generate key pair
        self.private_key, self.public_key = self._load_or_generate_keys()
    
    def _load_or_generate_keys(self):
        """Load existing RSA key pair or generate new ones"""
        try:
            # Try to load existing keys from secrets manager
            private_key_pem = secrets_manager.get_secret("JWT_PRIVATE_KEY")
            public_key_pem = secrets_manager.get_secret("JWT_PUBLIC_KEY")
            
            private_key = serialization.load_pem_private_key(
                private_key_pem.encode(),
                password=None
            )
            public_key = serialization.load_pem_public_key(public_key_pem.encode())
            
            logger.info("Loaded existing JWT key pair")
            return private_key, public_key
            
        except Exception as e:
            logger.warning(f"Could not load existing keys: {e}. Generating new ones.")
            return self._generate_new_keys()
    
    def _generate_new_keys(self):
        """Generate new RSA key pair"""
        try:
            # Generate private key
            private_key = rsa.generate_private_key(
                public_exponent=65537,
                key_size=2048
            )
            
            # Get public key
            public_key = private_key.public_key()
            
            # Serialize keys
            private_pem = private_key.private_bytes(
                encoding=serialization.Encoding.PEM,
                format=serialization.PrivateFormat.PKCS8,
                encryption_algorithm=serialization.NoEncryption()
            ).decode()
            
            public_pem = public_key.public_bytes(
                encoding=serialization.Encoding.PEM,
                format=serialization.PublicFormat.SubjectPublicKeyInfo
            ).decode()
            
            # Store in environment variables (encrypted)
            if os.environ.get('ENVIRONMENT') == 'production':
                # In production, these should be stored securely
                logger.info("New JWT keys generated. Store them securely!")
                print(f"JWT_PRIVATE_KEY_ENCRYPTED={secrets_manager.encrypt_secret(private_pem)}")
                print(f"JWT_PUBLIC_KEY_ENCRYPTED={secrets_manager.encrypt_secret(public_pem)}")
            
            logger.info("Generated new JWT key pair")
            return private_key, public_key
            
        except Exception as e:
            logger.error(f"Error generating JWT keys: {e}")
            raise
    
    def create_access_token(self, data: Dict[str, Any]) -> str:
        """Create JWT access token with RSA signing"""
        try:
            payload = data.copy()
            expire = datetime.utcnow() + timedelta(minutes=self.access_token_expire_minutes)
            
            payload.update({
                "exp": expire,
                "iat": datetime.utcnow(),
                "type": "access",
                "jti": secrets.token_urlsafe(16)  # Unique token ID
            })
            
            token = jwt.encode(payload, self.private_key, algorithm=self.algorithm)
            return token
            
        except Exception as e:
            logger.error(f"Error creating access token: {e}")
            raise
    
    def create_refresh_token(self, data: Dict[str, Any]) -> str:
        """Create JWT refresh token"""
        try:
            payload = data.copy()
            expire = datetime.utcnow() + timedelta(days=self.refresh_token_expire_days)
            
            payload.update({
                "exp": expire,
                "iat": datetime.utcnow(),
                "type": "refresh",
                "jti": secrets.token_urlsafe(16)
            })
            
            token = jwt.encode(payload, self.private_key, algorithm=self.algorithm)
            return token
            
        except Exception as e:
            logger.error(f"Error creating refresh token: {e}")
            raise
    
    def verify_token(self, token: str, token_type: str = "access") -> Dict[str, Any]:
        """Verify and decode JWT token"""
        try:
            payload = jwt.decode(
                token, 
                self.public_key, 
                algorithms=[self.algorithm],
                options={"require": ["exp", "iat", "type", "jti"]}
            )
            
            # Verify token type
            if payload.get("type") != token_type:
                raise jwt.InvalidTokenError(f"Expected {token_type} token")
            
            return payload
            
        except jwt.ExpiredSignatureError:
            logger.warning("Token has expired")
            raise
        except jwt.InvalidTokenError as e:
            logger.warning(f"Invalid token: {e}")
            raise
        except Exception as e:
            logger.error(f"Error verifying token: {e}")
            raise

# Global JWT manager
jwt_manager = JWTManager()


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/auth/main.py
# /sdg_root/src/auth/main.py

from fastapi import FastAPI, Depends, HTTPException, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional, Dict, Any
import bcrypt
import os
from datetime import datetime, timedelta
import logging
import secrets
from .jwt_manager import jwt_manager
from ..core.secrets_manager import secrets_manager

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Configuration
SECRET_KEY = os.environ.get("SECRET_KEY", "your-secret-key-change-in-production")
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 30
REFRESH_TOKEN_EXPIRE_DAYS = 7

app = FastAPI(
    title="SDG Auth Service",
    description="Authentication and authorization service for SDG AI Pipeline",
    version="1.0.0"
)

allowed_origins = os.environ.get("ALLOWED_ORIGINS", "").split(",")
if not allowed_origins or allowed_origins == [""]:
    if os.environ.get("ENVIRONMENT") == "development":
        allowed_origins = ["http://localhost:3000", "http://localhost:8080"]
    else:
        raise ValueError("ALLOWED_ORIGINS must be set in production")


app.add_middleware(
    CORSMiddleware,
    allow_origins=allowed_origins,
    allow_credentials=True,
    allow_methods=["GET", "POST"], 
    allow_headers=["Authorization", "Content-Type"],
)

security = HTTPBearer()

# Pydantic models
class UserLogin(BaseModel):
    username: str
    password: str

    class Config:
        str_strip_whitespace = True
        min_anystr_length = 1
        max_anystr_length = 100

class UserCreate(BaseModel):
    username: str
    password: str
    email: str
    role: str = "user"

class Token(BaseModel):
    access_token: str
    refresh_token: str
    token_type: str = "bearer"
    expires_in: int

class TokenData(BaseModel):
    username: Optional[str] = None
    role: Optional[str] = None

# Simple in-memory user store (replace with database in production)
users_db = {
    "admin": {
        "username": "admin",
        "hashed_password": bcrypt.hashpw(
            secrets_manager.get_secret("ADMIN_PASSWORD").encode(), 
            bcrypt.gensalt()
        ).decode(),
        "email": "admin@sdg-pipeline.org",
        "role": "admin",
        "is_active": True,
        "created_at": datetime.utcnow(),
        "last_login": None
    }
}

def verify_password(plain_password: str, hashed_password: str) -> bool:
    """Verify password against hash."""
    try:
        return bcrypt.checkpw(plain_password.encode(), hashed_password.encode())
    except Exception as e:
        logger.error(f"Password verification error: {e}")
        return False

def authenticate_user(username: str, password: str) -> Optional[Dict[str, Any]]:
    """Authenticate user with rate limiting"""
    user = users_db.get(username)
    if user and user.get("is_active") and verify_password(password, user["hashed_password"]):
        # Update last login
        user["last_login"] = datetime.utcnow()
        return user
    return None


def create_access_token(data: dict, expires_delta: Optional[timedelta] = None) -> str:
    """Create JWT access token."""
    to_encode = data.copy()
    if expires_delta:
        expire = datetime.utcnow() + expires_delta
    else:
        expire = datetime.utcnow() + timedelta(minutes=15)
    
    to_encode.update({"exp": expire, "type": "access"})
    return jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)

def create_refresh_token(data: dict) -> str:
    """Create JWT refresh token."""
    to_encode = data.copy()
    expire = datetime.utcnow() + timedelta(days=REFRESH_TOKEN_EXPIRE_DAYS)
    to_encode.update({"exp": expire, "type": "refresh"})
    return jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)

async def get_current_user(credentials: HTTPAuthorizationCredentials = Depends(security)) -> Dict[str, Any]:
    """Get current user from JWT token."""
    token = credentials.credentials
    credentials_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Could not validate credentials",
        headers={"WWW-Authenticate": "Bearer"},
    )
    
    try:
        token = credentials.credentials
        payload = jwt_manager.verify_token(token, "access")
        
        username = payload.get("sub")
        if username is None:
            raise credentials_exception
        
        user = users_db.get(username)
        if user is None or not user.get("is_active"):
            raise credentials_exception
        
        return user
        
    except wt.PyJWTError:
        raise credentials_exception

async def get_admin_user(current_user: dict = Depends(get_current_user)) -> Dict[str, Any]:
    """Ensure current user has admin role."""
    if current_user.get("role") != "admin":
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Not enough permissions"
        )
    return current_user

@app.post("/auth/login", response_model=Token)
async def login(user_credentials: UserLogin):
    """Secure user authentication"""
    user = authenticate_user(user_credentials.username, user_credentials.password)
    if not user:
        # Add delay to prevent timing attacks
        await asyncio.sleep(1)
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Incorrect username or password"
        )
    
    # Create tokens
    token_data = {"sub": user["username"], "role": user["role"]}
    access_token = jwt_manager.create_access_token(token_data)
    refresh_token = jwt_manager.create_refresh_token(token_data)
    
    logger.info(f"User {user['username']} logged in successfully")
    
    return {
        "access_token": access_token,
        "refresh_token": refresh_token,
        "token_type": "bearer",
        "expires_in": jwt_manager.access_token_expire_minutes * 60
    }
    

@app.post("/auth/refresh", response_model=Token)
async def refresh_token(refresh_token: str):
    """Refresh access token using refresh token."""
    credentials_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Could not validate credentials",
    )
    
    try:
        payload = jwt.decode(refresh_token, SECRET_KEY, algorithms=[ALGORITHM])
        username: str = payload.get("sub")
        token_type: str = payload.get("type")
        
        if username is None or token_type != "refresh":
            raise credentials_exception
            
        user = users_db.get(username)
        if user is None:
            raise credentials_exception
        
        access_token_expires = timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
        new_access_token = create_access_token(
            data={"sub": user["username"], "role": user["role"]},
            expires_delta=access_token_expires
        )
        new_refresh_token = create_refresh_token(
            data={"sub": user["username"], "role": user["role"]}
        )
        
        return {
            "access_token": new_access_token,
            "refresh_token": new_refresh_token,
            "token_type": "bearer",
            "expires_in": ACCESS_TOKEN_EXPIRE_MINUTES * 60
        }
        
    except jwt.PyJWTError:
        raise credentials_exception

@app.get("/auth/me")
async def get_current_user_info(current_user: dict = Depends(get_current_user)):
    """Get current user information."""
    return {
        "username": current_user["username"],
        "email": current_user["email"],
        "role": current_user["role"],
        "is_active": current_user["is_active"]
    }

@app.post("/auth/register", response_model=dict)
async def register_user(user: UserCreate, current_user: dict = Depends(get_admin_user)):
    """Register new user (admin only)."""
    if user.username in users_db:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Username already registered"
        )
    
    hashed_password = get_password_hash(user.password)
    users_db[user.username] = {
        "username": user.username,
        "hashed_password": hashed_password,
        "email": user.email,
        "role": user.role,
        "is_active": True
    }
    
    logger.info(f"New user registered: {user.username} with role {user.role}")
    return {"message": f"User {user.username} created successfully"}

@app.get("/health")
async def health_check():
    """Secure health check"""
    return {
        "status": "healthy",
        "service": "SDG Auth Service",
        "version": "1.0.0",
        "timestamp": datetime.utcnow().isoformat()
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="0.0.0.0", port=8005, reload=True)


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/content_extraction/config/__init__.py


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/content_extraction/config/extraction_rules.yml


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/content_extraction/config/sources.yml


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/content_extraction/extractors/__init__.py


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/content_extraction/extractors/base_extractor.py
"""
Base extractor class for all content extraction sources
"""
import logging
from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from datetime import datetime
import asyncio
import aiohttp
from urllib.parse import urljoin, urlparse
import hashlib

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ExtractedContent:
    """Standard format for extracted content"""
    title: str
    content: str
    summary: Optional[str] = None
    url: str = ""
    source_type: str = ""
    language: str = "en"
    region: str = ""
    extracted_at: datetime = None
    metadata: Dict[str, Any] = None
    quality_score: float = 0.0
    sdg_relevance: List[int] = None
    
    def __post_init__(self):
        if self.extracted_at is None:
            self.extracted_at = datetime.utcnow()
        if self.metadata is None:
            self.metadata = {}
        if self.sdg_relevance is None:
            self.sdg_relevance = []
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "title": self.title,
            "content": self.content,
            "summary": self.summary,
            "url": self.url,
            "source_type": self.source_type,
            "language": self.language,
            "region": self.region,
            "extracted_at": self.extracted_at.isoformat(),
            "metadata": self.metadata,
            "quality_score": self.quality_score,
            "sdg_relevance": self.sdg_relevance
        }
    
    def content_hash(self) -> str:
        """Generate content hash for deduplication"""
        content_string = f"{self.title}{self.content}{self.url}"
        return hashlib.md5(content_string.encode()).hexdigest()

class BaseExtractor(ABC):
    """
    Abstract base class for all content extractors
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.session = None
        self.retry_attempts = config.get("retry_attempts", 3)
        self.retry_delay = config.get("retry_delay", 1.0)
        self.timeout = config.get("timeout", 30)
        self.user_agent = config.get("user_agent", 
            "SDG-Pipeline-Bot/1.0 (+https://sdg-pipeline.org/bot)")
        
    async def __aenter__(self):
        """Async context manager entry"""
        connector = aiohttp.TCPConnector(limit=100, limit_per_host=10)
        timeout = aiohttp.ClientTimeout(total=self.timeout)
        
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers={"User-Agent": self.user_agent}
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        if self.session:
            await self.session.close()
    
    @abstractmethod
    async def extract(self, source_url: str, **kwargs) -> List[ExtractedContent]:
        """Extract content from source"""
        pass
    
    @abstractmethod
    def validate_source(self, source_url: str) -> bool:
        """Validate if source URL is supported"""
        pass
    
    async def fetch_with_retry(self, url: str, **kwargs) -> Optional[aiohttp.ClientResponse]:
        """Fetch URL with retry logic"""
        for attempt in range(self.retry_attempts):
            try:
                async with self.session.get(url, **kwargs) as response:
                    if response.status == 200:
                        return response
                    elif response.status in [429, 503, 504]:  # Rate limiting or server errors
                        wait_time = self.retry_delay * (2 ** attempt)
                        logger.warning(f"Rate limited on {url}, waiting {wait_time}s")
                        await asyncio.sleep(wait_time)
                    else:
                        logger.warning(f"HTTP {response.status} for {url}")
                        return None
            except asyncio.TimeoutError:
                logger.warning(f"Timeout on attempt {attempt + 1} for {url}")
                if attempt < self.retry_attempts - 1:
                    await asyncio.sleep(self.retry_delay * (2 ** attempt))
            except Exception as e:
                logger.error(f"Error fetching {url}: {e}")
                if attempt < self.retry_attempts - 1:
                    await asyncio.sleep(self.retry_delay)
        
        return None
    
    def estimate_quality_score(self, content: ExtractedContent) -> float:
        """Estimate content quality score"""
        score = 0.0
        
        # Title quality (0-0.2)
        if content.title and len(content.title.strip()) > 10:
            score += 0.2
        
        # Content length (0-0.3)
        content_length = len(content.content.strip())
        if content_length > 500:
            score += 0.3
        elif content_length > 200:
            score += 0.15
        
        # Has URL (0-0.1)
        if content.url and content.url.startswith(('http://', 'https://')):
            score += 0.1
        
        # Has metadata (0-0.2)
        if content.metadata and len(content.metadata) > 2:
            score += 0.2
        
        # Language detection (0-0.1)
        if content.language and content.language != "unknown":
            score += 0.1
        
        # SDG relevance (0-0.1)
        if content.sdg_relevance and len(content.sdg_relevance) > 0:
            score += 0.1
        
        return min(score, 1.0)
    
    async def process_batch(self, urls: List[str], **kwargs) -> List[ExtractedContent]:
        """Process multiple URLs concurrently"""
        semaphore = asyncio.Semaphore(self.config.get("concurrent_requests", 5))
        
        async def extract_single(url: str) -> List[ExtractedContent]:
            async with semaphore:
                try:
                    return await self.extract(url, **kwargs)
                except Exception as e:
                    logger.error(f"Error extracting {url}: {e}")
                    return []
        
        tasks = [extract_single(url) for url in urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Flatten results and filter exceptions
        extracted_content = []
        for result in results:
            if isinstance(result, list):
                extracted_content.extend(result)
        
        return extracted_content


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/content_extraction/extractors/chatgpt_extractor.py
"""
ChatGPT HTML Result Extractor (Non-API)
Parses ChatGPT web interface results and extracts content with follow-up links
"""
import logging
from typing import List, Dict, Any, Optional
import re
import asyncio
from bs4 import BeautifulSoup, Comment
from urllib.parse import urljoin, urlparse
from datetime import datetime

from .base_extractor import BaseExtractor, ExtractedContent

logger = logging.getLogger(__name__)

class ChatGPTExtractor(BaseExtractor):
    """
    ChatGPT web interface result extractor
    Parses ChatGPT conversation HTML and extracts insights with source links
    """
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.chatgpt_selectors = self._load_chatgpt_selectors()
        self.link_keywords = self._load_link_keywords()
    
    def _load_chatgpt_selectors(self) -> Dict[str, List[str]]:
        """Load ChatGPT-specific HTML selectors"""
        return {
            "message_containers": [
                "div[data-message-author-role='assistant']",
                ".message-content",
                ".markdown",
                "[data-testid='conversation-turn-message']",
                ".prose"
            ],
            "user_messages": [
                "div[data-message-author-role='user']",
                ".user-message",
                "[data-testid='user-message']"
            ],
            "assistant_messages": [
                "div[data-message-author-role='assistant']", 
                ".assistant-message",
                "[data-testid='assistant-message']"
            ],
            "code_blocks": [
                "pre code",
                ".code-block", 
                "pre"
            ],
            "citations": [
                ".citation",
                "sup a",
                "[data-citation]"
            ]
        }
    
    def _load_link_keywords(self) -> Dict[str, List[str]]:
        """Keywords to identify valuable links"""
        return {
            "download_links": [
                "download", "pdf", "report", "study", "data", "dataset", 
                "excel", "csv", "doc", "presentation", "whitepaper"
            ],
            "reference_links": [
                "source", "reference", "cite", "study", "research", 
                "article", "paper", "journal", "publication"
            ],
            "official_links": [
                "gov", "un.org", "who.int", "worldbank.org", "oecd.org",
                "europa.eu", "unicef.org", "undp.org", "unesco.org"
            ],
            "sdg_links": [
                "sdg", "sustainable", "development", "goal", "target",
                "indicator", "progress", "agenda"
            ]
        }
    
    def validate_source(self, source_url: str) -> bool:
        """Validate if URL is from ChatGPT interface"""
        chatgpt_domains = [
            'chat.openai.com',
            'chatgpt.com',
            'openai.com/chat'
        ]
        
        parsed = urlparse(source_url.lower())
        return any(domain in parsed.netloc for domain in chatgpt_domains)
    
    async def extract(self, source_url: str, **kwargs) -> List[ExtractedContent]:
        """Extract content from ChatGPT conversation page"""
        try:
            response = await self.fetch_with_retry(source_url)
            if not response:
                return []
            
            html_content = await response.text()
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Remove script and style elements
            self._clean_soup(soup)
            
            # Extract conversation turns
            conversation_data = self._extract_conversation(soup, source_url)
            
            # Extract follow-up links and resources
            extracted_links = self._extract_valuable_links(soup, source_url)
            
            # Create structured content
            extracted_items = []
            
            for turn_data in conversation_data:
                if turn_data['role'] == 'assistant' and len(turn_data['content']) > 100:
                    extracted_content = ExtractedContent(
                        title=self._generate_title(turn_data['content']),
                        content=turn_data['content'],
                        summary=self._create_summary(turn_data['content']),
                        url=source_url,
                        source_type="chatgpt_result",
                        language=kwargs.get('language', 'en'),
                        region=kwargs.get('region', ''),
                        metadata={
                            'conversation_turn': turn_data['turn_number'],
                            'extracted_links': extracted_links,
                            'user_query': turn_data.get('user_query', ''),
                            'extraction_method': 'chatgpt_html_parser',
                            'contains_code': turn_data.get('has_code', False),
                            'contains_citations': turn_data.get('has_citations', False),
                            'response_length': len(turn_data['content'])
                        }
                    )
                    
                    # Detect SDG relevance
                    extracted_content.sdg_relevance = self._detect_sdg_relevance(turn_data['content'])
                    
                    # Calculate quality score
                    extracted_content.quality_score = self._calculate_chatgpt_quality_score(
                        extracted_content, turn_data, extracted_links
                    )
                    
                    extracted_items.append(extracted_content)
            
            return extracted_items
            
        except Exception as e:
            logger.error(f"Error extracting ChatGPT content from {source_url}: {e}")
            return []
    
    def _clean_soup(self, soup: BeautifulSoup):
        """Remove unwanted elements from soup"""
        # Remove scripts, styles, and navigation
        for element in soup(['script', 'style', 'nav', 'header', 'footer']):
            element.decompose()
        
        # Remove comments
        for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):
            comment.extract()
    
    def _extract_conversation(self, soup: BeautifulSoup, source_url: str) -> List[Dict[str, Any]]:
        """Extract conversation turns from ChatGPT interface"""
        conversation_turns = []
        
        # Try different selector strategies
        messages = []
        for selector_group in self.chatgpt_selectors["message_containers"]:
            found_messages = soup.select(selector_group)
            if found_messages:
                messages = found_messages
                break
        
        if not messages:
            # Fallback: look for common conversation patterns
            messages = soup.find_all(['div', 'article'], class_=re.compile(r'(message|conversation|chat|response)'))
        
        current_user_query = ""
        turn_number = 1
        
        for i, message in enumerate(messages):
            # Determine if this is user or assistant message
            message_text = message.get_text(separator=' ', strip=True)
            if len(message_text) < 20:  # Skip very short messages
                continue
            
            # Check message role
            role = self._determine_message_role(message, message_text)
            
            if role == 'user':
                current_user_query = message_text
            elif role == 'assistant':
                # Check for code blocks
                has_code = bool(message.find_all(['pre', 'code']))
                
                # Check for citations or links
                has_citations = bool(message.find_all('a') or message.find_all(['sup', 'cite']))
                
                conversation_turns.append({
                    'turn_number': turn_number,
                    'role': 'assistant',
                    'content': message_text,
                    'user_query': current_user_query,
                    'has_code': has_code,
                    'has_citations': has_citations,
                    'html_element': message
                })
                turn_number += 1
        
        return conversation_turns
    
    def _determine_message_role(self, message_element, message_text: str) -> str:
        """Determine if message is from user or assistant"""
        # Check data attributes first
        role_attr = message_element.get('data-message-author-role')
        if role_attr:
            return role_attr
        
        # Check class names
        class_names = ' '.join(message_element.get('class', [])).lower()
        if any(term in class_names for term in ['user', 'human']):
            return 'user'
        elif any(term in class_names for term in ['assistant', 'ai', 'bot']):
            return 'assistant'
        
        # Heuristic based on content patterns
        if re.match(r'^(what|how|why|can you|please|could you)', message_text.lower()):
            return 'user'
        elif len(message_text) > 200:  # Assistant responses tend to be longer
            return 'assistant'
        
        return 'unknown'
    
    def _extract_valuable_links(self, soup: BeautifulSoup, base_url: str) -> List[Dict[str, Any]]:
        """Extract and categorize valuable links from the page"""
        links = []
        
        # Find all links
        for link in soup.find_all('a', href=True):
            href = link['href']
            link_text = link.get_text(strip=True)
            
            # Skip empty links or javascript
            if not href or href.startswith(('javascript:', '#', 'mailto:')):
                continue
            
            # Convert relative URLs to absolute
            if href.startswith('/'):
                href = urljoin(base_url, href)
            elif not href.startswith(('http://', 'https://')):
                continue
            
            # Categorize link
            link_category = self._categorize_link(href, link_text)
            
            if link_category:  # Only include categorized links
                links.append({
                    'url': href,
                    'text': link_text,
                    'category': link_category,
                    'domain': urlparse(href).netloc,
                    'is_download': self._is_download_link(href, link_text),
                    'sdg_relevance': self._assess_link_sdg_relevance(href, link_text)
                })
        
        # Sort by relevance (SDG relevance + category priority)
        links.sort(key=lambda x: (x['sdg_relevance'], x['is_download']), reverse=True)
        
        return links[:20]  # Limit to most relevant links
    
    def _categorize_link(self, url: str, link_text: str) -> Optional[str]:
        """Categorize link based on URL and text content"""
        url_lower = url.lower()
        text_lower = link_text.lower()
        combined_text = f"{url_lower} {text_lower}"
        
        # Check for official/government sources
        if any(domain in url_lower for domain in self.link_keywords["official_links"]):
            return "official_source"
        
        # Check for download links
        if any(keyword in combined_text for keyword in self.link_keywords["download_links"]):
            return "download_resource"
        
        # Check for reference/research links
        if any(keyword in combined_text for keyword in self.link_keywords["reference_links"]):
            return "research_reference"
        
        # Check for SDG-specific links
        if any(keyword in combined_text for keyword in self.link_keywords["sdg_links"]):
            return "sdg_resource"
        
        # Check file extensions
        if any(ext in url_lower for ext in ['.pdf', '.doc', '.xls', '.ppt', '.csv']):
            return "document_file"
        
        return None
    
    def _is_download_link(self, url: str, link_text: str) -> bool:
        """Check if link is likely a download"""
        download_indicators = [
            '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx',
            '.csv', '.zip', '.tar', '.gz', 'download', 'attachment'
        ]
        
        combined = f"{url.lower()} {link_text.lower()}"
        return any(indicator in combined for indicator in download_indicators)
    
    def _assess_link_sdg_relevance(self, url: str, link_text: str) -> float:
        """Assess how relevant a link is to SDG content"""
        score = 0.0
        combined_text = f"{url.lower()} {link_text.lower()}"
        
        # Official SDG sources get high score
        sdg_domains = ['un.org', 'sdgs.un.org', 'sustainabledevelopment.un.org']
        if any(domain in url.lower() for domain in sdg_domains):
            score += 0.5
        
        # SDG keywords
        sdg_terms = ['sdg', 'sustainable development', 'agenda 2030', 'goal', 'target']
        score += sum(0.1 for term in sdg_terms if term in combined_text)
        
        # Research/data indicators
        research_terms = ['research', 'data', 'report', 'study', 'analysis']
        score += sum(0.05 for term in research_terms if term in combined_text)
        
        return min(score, 1.0)
    
    def _generate_title(self, content: str) -> str:
        """Generate title from ChatGPT response content"""
        # Look for clear topic indicators
        sentences = content.split('.')
        first_sentence = sentences[0].strip() if sentences else content[:100]
        
        # Remove common ChatGPT prefixes
        prefixes_to_remove = [
            "Based on", "According to", "Here's", "Here are", "I'll help you",
            "Let me", "To answer", "In response"
        ]
        
        for prefix in prefixes_to_remove:
            if first_sentence.startswith(prefix):
                # Try to find the actual topic
                remaining = first_sentence[len(prefix):].strip()
                if len(remaining) > 20:
                    first_sentence = remaining
                break
        
        # Clean up and limit length
        title = re.sub(r'^[^a-zA-Z0-9]*', '', first_sentence)
        return title[:150] if len(title) > 150 else title
    
    def _create_summary(self, content: str) -> str:
        """Create summary from ChatGPT response"""
        # Take first 2-3 sentences or up to 300 characters
        sentences = re.split(r'[.!?]', content)
        summary_sentences = []
        char_count = 0
        
        for sentence in sentences[:3]:
            sentence = sentence.strip()
            if sentence and char_count + len(sentence) <= 300:
                summary_sentences.append(sentence)
                char_count += len(sentence)
            else:
                break
        
        summary = '. '.join(summary_sentences)
        return summary + '.' if summary and not summary.endswith('.') else summary
    
    def _detect_sdg_relevance(self, content: str) -> List[int]:
        """Detect SDG relevance in ChatGPT response"""
        sdg_patterns = {
            1: r'\b(poverty|poor|income|wealth|social.protection)\b',
            2: r'\b(hunger|food|nutrition|agriculture|farming)\b',
            3: r'\b(health|healthcare|medical|disease|mortality)\b',
            4: r'\b(education|learning|school|literacy|skills)\b',
            5: r'\b(gender|women|girls|equality|empowerment)\b',
            6: r'\b(water|sanitation|hygiene|clean.water)\b',
            7: r'\b(energy|renewable|electricity|clean.energy)\b',
            8: r'\b(employment|jobs|economic.growth|decent.work)\b',
            9: r'\b(infrastructure|innovation|industry|technology)\b',
            10: r'\b(inequality|inclusion|discrimination|equity)\b',
            11: r'\b(cities|urban|housing|transport|sustainable.cities)\b',
            12: r'\b(consumption|production|waste|recycling|sustainable)\b',
            13: r'\b(climate|carbon|emission|greenhouse|warming)\b',
            14: r'\b(ocean|marine|sea|fisheries|aquatic)\b',
            15: r'\b(forest|biodiversity|ecosystem|wildlife|conservation)\b',
            16: r'\b(peace|justice|institutions|governance|law)\b',
            17: r'\b(partnership|cooperation|global|development.finance)\b'
        }
        
        content_lower = re.sub(r'[^\w\s]', ' ', content.lower())
        relevant_sdgs = []
        
        for sdg_id, pattern in sdg_patterns.items():
            if re.search(pattern, content_lower):
                relevant_sdgs.append(sdg_id)
        
        return relevant_sdgs
    
    def _calculate_chatgpt_quality_score(self, content: ExtractedContent, 
                                       turn_data: Dict[str, Any],
                                       extracted_links: List[Dict[str, Any]]) -> float:
        """Calculate quality score for ChatGPT extracted content"""
        score = 0.0
        
        # Base content quality (0-0.4)
        content_length = len(content.content)
        if content_length > 500:
            score += 0.4
        elif content_length > 200:
            score += 0.2
        
        # Has structured elements (0-0.2)
        if turn_data.get('has_code'):
            score += 0.1
        if turn_data.get('has_citations'):
            score += 0.1
        
        # Link quality (0-0.2)
        if extracted_links:
            high_quality_links = [l for l in extracted_links if l.get('sdg_relevance', 0) > 0.3]
            if len(high_quality_links) >= 3:
                score += 0.2
            elif len(high_quality_links) >= 1:
                score += 0.1
        
        # SDG relevance (0-0.1)
        if len(content.sdg_relevance) >= 2:
            score += 0.1
        elif len(content.sdg_relevance) >= 1:
            score += 0.05
        
        # User query context (0-0.1)
        user_query = turn_data.get('user_query', '')
        if len(user_query) > 20:  # Had meaningful user question
            score += 0.1
        
        return min(score, 1.0)


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/content_extraction/extractors/gemini_extractor.py
"""
Gemini 2.5 content analysis extractor
Processes content through Gemini for SDG analysis
"""
import logging
from typing import List, Dict, Any, Optional
import asyncio
import json
import re
from .base_extractor import BaseExtractor, ExtractedContent

logger = logging.getLogger(__name__)

class GeminiExtractor(BaseExtractor):
    """
    Extractor for Gemini 2.5 analysis results
    Processes existing content through Gemini for enhanced SDG insights
    """
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.gemini_prompts = self._load_gemini_prompts()
        self.sdg_keywords = self._load_sdg_keywords()
    
    def _load_gemini_prompts(self) -> Dict[str, str]:
        """Load Gemini analysis prompts"""
        return {
            "sdg_analysis": """
            Analyze the following text for relevance to the UN Sustainable Development Goals (SDGs).
            
            Text: {content}
            
            Please provide:
            1. Primary SDG goals (1-17) that this content addresses
            2. Confidence score (0-1) for each identified SDG
            3. Key themes and topics
            4. Regional relevance if mentioned
            5. Summary of SDG-related content
            
            Format your response as JSON with the following structure:
            {{
                "sdg_goals": [{{goal_number, confidence_score}}],
                "themes": ["theme1", "theme2"],
                "region": "region_name",
                "summary": "content_summary",
                "quality_indicators": {{
                    "has_data": boolean,
                    "has_citations": boolean,
                    "policy_relevant": boolean
                }}
            }}
            """,
            
            "content_enhancement": """
            Enhance and summarize the following content for SDG research purposes:
            
            Original Content: {content}
            
            Please provide:
            1. A concise summary (200-300 words)
            2. Key findings or insights
            3. Methodology mentioned (if any)
            4. Data sources referenced
            5. Policy implications
            
            Focus on aspects relevant to sustainable development research.
            """,
            
            "quality_assessment": """
            Assess the quality and credibility of this content:
            
            Content: {content}
            Source: {source_url}
            
            Evaluate:
            1. Scientific rigor (0-10)
            2. Data reliability (0-10) 
            3. Bias indicators
            4. Citation quality
            5. Overall credibility score (0-10)
            
            Provide reasoning for each score.
            """
        }
    
    def _load_sdg_keywords(self) -> Dict[int, List[str]]:
        """Load SDG-specific keywords for validation"""
        # This would typically load from your keywords.py file
        return {
            1: ["poverty", "income inequality", "social protection"],
            2: ["hunger", "food security", "nutrition", "agriculture"],
            3: ["health", "mortality", "disease", "healthcare"],
            4: ["education", "learning", "literacy", "skills"],
            5: ["gender", "women", "girls", "equality"],
            6: ["water", "sanitation", "hygiene"],
            7: ["energy", "renewable", "electricity", "clean"],
            8: ["employment", "economic growth", "decent work"],
            9: ["infrastructure", "innovation", "industry"],
            10: ["inequality", "inclusion", "discrimination"],
            11: ["cities", "urban", "housing", "transport"],
            12: ["consumption", "production", "waste", "sustainability"],
            13: ["climate", "greenhouse gas", "adaptation"],
            14: ["ocean", "marine", "fisheries"],
            15: ["forest", "biodiversity", "ecosystem"],
            16: ["peace", "justice", "institutions", "governance"],
            17: ["partnership", "cooperation", "finance"]
        }
    
    def validate_source(self, source_url: str) -> bool:
        """Validate if source is suitable for Gemini analysis"""
        # Accept any content for Gemini analysis
        return True
    
    async def extract(self, content_input: str, **kwargs) -> List[ExtractedContent]:
        """
        Process content through Gemini analysis
        content_input can be raw text, URL, or structured content
        """
        try:
            # Determine input type
            if content_input.startswith(('http://', 'https://')):
                # URL input - fetch content first
                content_text = await self._fetch_url_content(content_input)
                source_url = content_input
            else:
                # Direct text input
                content_text = content_input
                source_url = kwargs.get('source_url', '')
            
            if not content_text or len(content_text.strip()) < 100:
                logger.warning("Content too short for Gemini analysis")
                return []
            
            # Process through Gemini (simulated - replace with actual Gemini API)
            analysis_result = await self._simulate_gemini_analysis(content_text)
            
            # Create enhanced content object
            enhanced_content = ExtractedContent(
                title=self._extract_title(content_text),
                content=content_text,
                summary=analysis_result.get('summary', ''),
                url=source_url,
                source_type="gemini_analysis",
                language=kwargs.get('language', 'en'),
                region=analysis_result.get('region', ''),
                metadata={
                    'gemini_analysis': analysis_result,
                    'processing_method': 'gemini_2.5',
                    'themes': analysis_result.get('themes', []),
                    'quality_indicators': analysis_result.get('quality_indicators', {})
                },
                sdg_relevance=[goal['goal_number'] for goal in analysis_result.get('sdg_goals', [])]
            )
            
            # Calculate quality score
            enhanced_content.quality_score = self._calculate_gemini_quality_score(analysis_result)
            
            return [enhanced_content]
            
        except Exception as e:
            logger.error(f"Error in Gemini extraction: {e}")
            return []
    
    async def _fetch_url_content(self, url: str) -> str:
        """Fetch content from URL for analysis"""
        response = await self.fetch_with_retry(url)
        if response:
            content = await response.text()
            # Basic HTML cleaning
            clean_content = re.sub(r'<[^>]+>', ' ', content)
            clean_content = re.sub(r'\s+', ' ', clean_content).strip()
            return clean_content
        return ""
    
    async def _simulate_gemini_analysis(self, content: str) -> Dict[str, Any]:
        """
        Simulate Gemini 2.5 analysis
        Replace this with actual Gemini API calls
        """
        # Simulate processing delay
        await asyncio.sleep(0.5)
        
        # Simple SDG detection based on keywords
        detected_sdgs = []
        for sdg_id, keywords in self.sdg_keywords.items():
            keyword_matches = sum(1 for keyword in keywords if keyword.lower() in content.lower())
            if keyword_matches > 0:
                confidence = min(keyword_matches / len(keywords), 1.0)
                if confidence > 0.1:  # Minimum confidence threshold
                    detected_sdgs.append({
                        "goal_number": sdg_id,
                        "confidence_score": confidence
                    })
        
        # Generate summary (first 300 chars as simulation)
        summary = content[:300] + "..." if len(content) > 300 else content
        
        # Extract themes (simple word frequency analysis)
        words = re.findall(r'\b\w+\b', content.lower())
        word_freq = {}
        for word in words:
            if len(word) > 4:  # Only meaningful words
                word_freq[word] = word_freq.get(word, 0) + 1
        
        themes = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:5]
        themes = [theme[0] for theme in themes]
        
        # Quality indicators
        quality_indicators = {
            "has_data": any(word in content.lower() for word in ['data', 'statistics', 'research', 'study']),
            "has_citations": any(word in content.lower() for word in ['doi', 'reference', 'citation', 'source']),
            "policy_relevant": any(word in content.lower() for word in ['policy', 'government', 'legislation', 'regulation'])
        }
        
        return {
            "sdg_goals": detected_sdgs,
            "themes": themes,
            "region": self._detect_region(content),
            "summary": summary,
            "quality_indicators": quality_indicators
        }
    
    def _detect_region(self, content: str) -> str:
        """Simple region detection"""
        regions = {
            "EU": ["europe", "european", "eu", "brussels"],
            "USA": ["united states", "america", "usa", "us"],
            "China": ["china", "chinese", "beijing"],
            "India": ["india", "indian", "delhi"],
            "ASEAN": ["asean", "southeast asia", "vietnam", "thailand"],
            "BRICS": ["brics", "brazil", "russia", "south africa"]
        }
        
        content_lower = content.lower()
        for region, keywords in regions.items():
            if any(keyword in content_lower for keyword in keywords):
                return region
        
        return ""
    
    def _extract_title(self, content: str) -> str:
        """Extract or generate title from content"""
        # Simple title extraction - first sentence or first 100 chars
        sentences = content.split('.')
        if sentences and len(sentences[0].strip()) > 10:
            return sentences[0].strip()[:100]
        return content[:100].strip()
    
    def _calculate_gemini_quality_score(self, analysis: Dict[str, Any]) -> float:
        """Calculate quality score based on Gemini analysis"""
        score = 0.0
        
        # SDG relevance (0-0.4)
        sdg_goals = analysis.get('sdg_goals', [])
        if sdg_goals:
            avg_confidence = sum(goal['confidence_score'] for goal in sdg_goals) / len(sdg_goals)
            score += avg_confidence * 0.4
        
        # Quality indicators (0-0.3)
        quality_indicators = analysis.get('quality_indicators', {})
        quality_count = sum(quality_indicators.values())
        score += (quality_count / 3) * 0.3
        
        # Themes richness (0-0.2)
        themes = analysis.get('themes', [])
        if len(themes) >= 3:
            score += 0.2
        elif len(themes) >= 1:
            score += 0.1
        
        # Regional relevance (0-0.1)
        if analysis.get('region'):
            score += 0.1
        
        return min(score, 1.0)
    
    async def analyze_existing_content(self, content_items: List[Dict[str, Any]]) -> List[ExtractedContent]:
        """Analyze existing content items through Gemini"""
        results = []
        
        for item in content_items:
            content_text = item.get('content', '')
            if content_text:
                enhanced = await self.extract(
                    content_text,
                    source_url=item.get('url', ''),
                    language=item.get('language', 'en')
                )
                results.extend(enhanced)
        
        return results


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/content_extraction/extractors/newsletter_extractor.py
"""
Newsletter content extractor
Handles email newsletters and similar formatted content
"""
import logging
from typing import List, Dict, Any, Optional
import re
from bs4 import BeautifulSoup
from .base_extractor import BaseExtractor, ExtractedContent
from .web_extractor import WebExtractor

logger = logging.getLogger(__name__)

class NewsletterExtractor(WebExtractor):
    """
    Newsletter content extractor
    Extends WebExtractor with newsletter-specific logic
    """
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.newsletter_patterns = self._load_newsletter_patterns()
    
    def _load_newsletter_patterns(self) -> Dict[str, List[str]]:
        """Load newsletter-specific patterns"""
        return {
            "newsletter_indicators": [
                "newsletter",
                "weekly update",
                "monthly digest",
                "news digest",
                "bulletin",
                "briefing"
            ],
            "content_sections": [
                ".newsletter-content",
                ".email-content",
                ".digest-content", 
                ".bulletin-content",
                "table[role='presentation']",
                "td.content"
            ],
            "article_sections": [
                ".article-item",
                ".news-item",
                ".digest-item",
                ".story",
                "tr.article",
                "div[style*='border']"
            ]
        }
    
    def validate_source(self, source_url: str) -> bool:
        """Validate if source is newsletter-like"""
        newsletter_indicators = self.newsletter_patterns["newsletter_indicators"]
        url_lower = source_url.lower()
        
        # Check URL for newsletter indicators
        if any(indicator in url_lower for indicator in newsletter_indicators):
            return True
        
        # Also validate as general web content
        return super().validate_source(source_url)
    
    async def extract(self, source_url: str, **kwargs) -> List[ExtractedContent]:
        """Extract newsletter content with multi-article support"""
        try:
            # Use parent class to fetch and clean content
            response = await self.fetch_with_retry(source_url)
            if not response:
                return []
            
            html_content = await response.text()
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Check if this is actually a newsletter
            if not self._is_newsletter_content(soup, source_url):
                # Fall back to regular web extraction
                return await super().extract(source_url, **kwargs)
            
            # Clean soup
            self._clean_soup(soup)
            
            # Extract multiple articles from newsletter
            articles = self._extract_newsletter_articles(soup, source_url)
            
            if not articles:
                # Fall back to single content extraction
                single_content = self._extract_structured_content(soup, source_url)
                if single_content:
                    single_content.source_type = "newsletter"
                    return [single_content]
            
            return articles
            
        except Exception as e:
            logger.error(f"Error extracting newsletter from {source_url}: {e}")
            return []
    
    def _is_newsletter_content(self, soup: BeautifulSoup, source_url: str) -> bool:
        """Determine if content is newsletter-like"""
        # Check URL
        url_lower = source_url.lower()
        newsletter_indicators = self.newsletter_patterns["newsletter_indicators"]
        if any(indicator in url_lower for indicator in newsletter_indicators):
            return True
        
        # Check page content
        page_text = soup.get_text().lower()
        if any(indicator in page_text[:500] for indicator in newsletter_indicators):
            return True
        
        # Check for newsletter-like structure
        # Multiple article-like sections
        article_sections = soup.select('.article, .story, .news-item, .digest-item')
        if len(article_sections) >= 3:
            return True
        
        # Email-like table structure
        if soup.find('table', {'role': 'presentation'}):
            return True
        
        # Newsletter-specific meta tags
        newsletter_meta = soup.find('meta', attrs={
            'name': lambda x: x and 'newsletter' in x.lower() if x else False
        })
        if newsletter_meta:
            return True
        
        return False
    
    def _extract_newsletter_articles(self, soup: BeautifulSoup, source_url: str) -> List[ExtractedContent]:
        """Extract individual articles from newsletter"""
        articles = []
        
        # Try different strategies to find article sections
        article_sections = self._find_article_sections(soup)
        
        newsletter_metadata = self._extract_newsletter_metadata(soup, source_url)
        
        for i, section in enumerate(article_sections):
            article = self._extract_newsletter_article(section, source_url, i, newsletter_metadata)
            if article:
                articles.append(article)
        
        return articles
    
    def _find_article_sections(self, soup: BeautifulSoup) -> List:
        """Find individual article sections in newsletter"""
        sections = []
        
        # Strategy 1: Use newsletter-specific selectors
        for selector in self.newsletter_patterns["article_sections"]:
            found_sections = soup.select(selector)
            if found_sections and len(found_sections) >= 2:
                return found_sections
        
        # Strategy 2: Find repeating patterns
        # Look for multiple divs/sections with similar structure
        potential_sections = []
        
        # Try divs with similar classes
        all_divs = soup.find_all('div', class_=True)
        class_groups = {}
        
        for div in all_divs:
            classes = ' '.join(div.get('class', []))
            if classes:
                class_groups.setdefault(classes, []).append(div)
        
        # Find groups with multiple similar elements
        for class_name, divs in class_groups.items():
            if len(divs) >= 3:  # At least 3 similar sections
                # Check if they contain substantial content
                content_divs = []
                for div in divs:
                    text = div.get_text(strip=True)
                    if len(text) > 100:  # Minimum content length
                        content_divs.append(div)
                
                if len(content_divs) >= 2:
                    potential_sections = content_divs
                    break
        
        # Strategy 3: Table rows (email newsletters often use tables)
        if not potential_sections:
            table_rows = soup.find_all('tr')
            content_rows = []
            
            for row in table_rows:
                text = row.get_text(strip=True)
                if len(text) > 100 and not self._is_header_footer_row(row):
                    content_rows.append(row)
            
            if len(content_rows) >= 2:
                potential_sections = content_rows
        
        # Strategy 4: Paragraphs with headers
        if not potential_sections:
            # Find h2/h3 elements that might be article headers
            headers = soup.find_all(['h2', 'h3', 'h4'])
            for header in headers:
                # Get content after header until next header
                article_content = []
                current = header.next_sibling
                
                while current:
                    if hasattr(current, 'name') and current.name in ['h1', 'h2', 'h3', 'h4']:
                        break
                    if hasattr(current, 'get_text'):
                        text = current.get_text(strip=True)
                        if text:
                            article_content.append(current)
                    current = current.next_sibling
                
                if article_content:
                    # Create a wrapper for this article section
                    wrapper = soup.new_tag('div')
                    wrapper.append(header)
                    for content in article_content:
                        wrapper.append(content)
                    potential_sections.append(wrapper)
        
        return potential_sections[:20]  # Limit number of sections
    
    def _is_header_footer_row(self, row) -> bool:
        """Check if table row is header/footer content"""
        text = row.get_text().lower()
        header_footer_indicators = [
            'unsubscribe', 'privacy policy', 'terms of service',
            'follow us', 'social media', 'contact us', 'newsletter',
            'view in browser', 'email preferences', 'copyright'
        ]
        
        return any(indicator in text for indicator in header_footer_indicators)
    
    def _extract_newsletter_article(self, section, source_url: str, index: int, 
                                  newsletter_metadata: Dict[str, Any]) -> Optional[ExtractedContent]:
        """Extract individual article from newsletter section"""
        try:
            # Extract title
            title = self._extract_section_title(section)
            if not title:
                title = f"Newsletter Article {index + 1}"
            
            # Extract content
            content = section.get_text(separator=' ', strip=True)
            if len(content) < 100:  # Minimum content length
                return None
            
            # Clean content
            content = re.sub(r'\s+', ' ', content).strip()
            
            # Extract links
            links = self._extract_section_links(section, source_url)
            
            # Generate article URL (use first link or base URL with anchor)
            article_url = source_url
            if links:
                article_url = links[0]['url']
            else:
                article_url = f"{source_url}#article-{index + 1}"
            
            # Create summary (first paragraph or truncated content)
            summary = self._extract_section_summary(section, content)
            
            # Combine newsletter metadata with article-specific metadata
            article_metadata = {
                **newsletter_metadata,
                "article_index": index + 1,
                "links": links,
                "section_type": "newsletter_article"
            }
            
            # Detect SDG relevance
            sdg_relevance = self._detect_sdg_keywords_newsletter(content + " " + title)
            
            article = ExtractedContent(
                title=title,
                content=content,
                summary=summary,
                url=article_url,
                source_type="newsletter",
                language=newsletter_metadata.get('language', 'en'),
                region=newsletter_metadata.get('region', ''),
                metadata=article_metadata,
                sdg_relevance=sdg_relevance
            )
            
            # Calculate quality score
            article.quality_score = self._calculate_newsletter_quality_score(article, section)
            
            return article
            
        except Exception as e:
            logger.error(f"Error extracting newsletter article {index}: {e}")
            return None
    
    def _extract_section_title(self, section) -> str:
        """Extract title from newsletter section"""
        # Look for header tags
        for header_tag in ['h1', 'h2', 'h3', 'h4', 'h5']:
            header = section.find(header_tag)
            if header:
                title = header.get_text(strip=True)
                if len(title) > 5:
                    return title[:200]
        
        # Look for bold/strong text at beginning
        bold_elements = section.find_all(['b', 'strong'])
        for bold in bold_elements:
            text = bold.get_text(strip=True)
            if len(text) > 10 and len(text) < 100:
                return text
        
        # Look for first sentence
        text = section.get_text(strip=True)
        sentences = re.split(r'[.!?]', text)
        if sentences and len(sentences[0]) > 10:
            return sentences[0][:200]
        
        return ""
    
    def _extract_section_links(self, section, base_url: str) -> List[Dict[str, str]]:
        """Extract links from newsletter section"""
        links = []
        for link in section.find_all('a', href=True):
            href = link['href']
            text = link.get_text(strip=True)
            
            # Convert relative URLs to absolute
            if href.startswith('/'):
                from urllib.parse import urljoin, urlparse
                base_domain = f"{urlparse(base_url).scheme}://{urlparse(base_url).netloc}"
                href = urljoin(base_domain, href)
            elif not href.startswith(('http://', 'https://')):
                continue
            
            if text and href:
                links.append({
                    'url': href,
                    'text': text,
                    'type': 'article_link'
                })
        
        return links[:5]  # Limit links per article
    
    def _extract_section_summary(self, section, content: str) -> str:
        """Extract or generate summary for newsletter section"""
        # Look for explicit summary/excerpt
        summary_selectors = ['.summary', '.excerpt', '.intro', '.lead']
        for selector in summary_selectors:
            summary_elem = section.select_one(selector)
            if summary_elem:
                summary = summary_elem.get_text(strip=True)
                if len(summary) > 50:
                    return summary[:400]
        
        # Use first paragraph if available
        paragraphs = section.find_all('p')
        if paragraphs:
            first_p = paragraphs[0].get_text(strip=True)
            if len(first_p) > 50:
                return first_p[:400]
        
        # Fallback to truncated content
        return content[:300] + "..." if len(content) > 300 else content
    
    def _extract_newsletter_metadata(self, soup: BeautifulSoup, source_url: str) -> Dict[str, Any]:
        """Extract newsletter-level metadata"""
        metadata = {
            "source_type": "newsletter",
            "extraction_method": "newsletter_extractor"
        }
        
        # Newsletter title
        title_elem = soup.find('title')
        if title_elem:
            metadata['newsletter_title'] = title_elem.get_text(strip=True)
        
        # Newsletter date
        date_selectors = [
            'meta[name="date"]',
            'meta[property="article:published_time"]',
            '.newsletter-date',
            '.issue-date'
        ]
        
        for selector in date_selectors:
            date_elem = soup.select_one(selector)
            if date_elem:
                date_value = date_elem.get('content') or date_elem.get_text(strip=True)
                if date_value:
                    metadata['newsletter_date'] = date_value
                    break
        
        # Newsletter organization/sender
        org_selectors = [
            'meta[name="author"]',
            'meta[property="article:author"]',
            '.newsletter-from',
            '.sender'
        ]
        
        for selector in org_selectors:
            org_elem = soup.select_one(selector)
            if org_elem:
                org_value = org_elem.get('content') or org_elem.get_text(strip=True)
                if org_value:
                    metadata['newsletter_organization'] = org_value
                    break
        
        # Language detection
        html_elem = soup.find('html')
        if html_elem and html_elem.get('lang'):
            metadata['language'] = html_elem['lang'][:2]
        else:
            metadata['language'] = 'en'
        
        # Region detection (basic)
        content_text = soup.get_text()
        metadata['region'] = self._detect_region_from_content(content_text, soup)
        
        return metadata
    
    def _detect_sdg_keywords_newsletter(self, text: str) -> List[int]:
        """Enhanced SDG keyword detection for newsletters"""
        # Newsletter content often has more context, so use enhanced patterns
        enhanced_sdg_patterns = {
            1: r'\b(poverty|poor|low.income|wealth.gap|social.protection|basic.needs)\b',
            2: r'\b(hunger|food.security|malnutrition|agriculture|farming|crop)\b',
            3: r'\b(health|healthcare|medical|disease|mortality|wellbeing|pandemic)\b',
            4: r'\b(education|school|learning|literacy|skills|training|university)\b',
            5: r'\b(gender|women|girls|equality|empowerment|discrimination)\b',
            6: r'\b(water|sanitation|hygiene|drinking.water|clean.water)\b',
            7: r'\b(energy|renewable|solar|wind|electricity|clean.energy|fossil)\b',
            8: r'\b(employment|jobs|economic.growth|decent.work|unemployment)\b',
            9: r'\b(infrastructure|innovation|industry|technology|research|development)\b',
            10: r'\b(inequality|inclusion|discrimination|equity|marginalized)\b',
            11: r'\b(cities|urban|housing|transport|sustainable.cities|smart.city)\b',
            12: r'\b(consumption|production|waste|recycling|sustainable|circular.economy)\b',
            13: r'\b(climate|carbon|emission|greenhouse|global.warming|adaptation)\b',
            14: r'\b(ocean|marine|sea|fisheries|aquatic|coral|plastic.pollution)\b',
            15: r'\b(forest|biodiversity|ecosystem|wildlife|conservation|deforestation)\b',
            16: r'\b(peace|justice|institutions|governance|rule.of.law|corruption)\b',
            17: r'\b(partnership|cooperation|global|development.finance|aid)\b'
        }
        
        text_lower = text.lower()
        text_lower = re.sub(r'[^\w\s]', ' ', text_lower)  # Remove punctuation
        relevant_sdgs = []
        
        for sdg_id, pattern in enhanced_sdg_patterns.items():
            if re.search(pattern, text_lower):
                relevant_sdgs.append(sdg_id)
        
        return relevant_sdgs
    
    def _calculate_newsletter_quality_score(self, article: ExtractedContent, section) -> float:
        """Calculate quality score for newsletter article"""
        score = 0.0
        
        # Base quality score from parent
        base_score = self.estimate_quality_score(article)
        score += base_score * 0.6
        
        # Newsletter-specific quality factors
        
        # Has links (0-0.1)
        links = article.metadata.get('links', [])
        if links:
            score += 0.1
        
        # Content structure (0-0.1)
        # Check if section has proper HTML structure
        if section.find(['p', 'div', 'span']):
            score += 0.1
        
        # Newsletter organization info (0-0.1)
        if article.metadata.get('newsletter_organization'):
            score += 0.1
        
        # SDG relevance bonus (0-0.1)
        if len(article.sdg_relevance) >= 2:
            score += 0.1
        elif len(article.sdg_relevance) >= 1:
            score += 0.05
        
        return min(score, 1.0)


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/content_extraction/extractors/perplexity_extractor.py
"""
Perplexity.ai HTML Result Extractor (Non-API)
Parses Perplexity search results and extracts content with source citations
"""
import logging
from typing import List, Dict, Any, Optional
import re
import asyncio
from bs4 import BeautifulSoup, Comment
from urllib.parse import urljoin, urlparse
from datetime import datetime

from .base_extractor import BaseExtractor, ExtractedContent

logger = logging.getLogger(__name__)

class PerplexityExtractor(BaseExtractor):
    """
    Perplexity.ai search result extractor
    Parses Perplexity response pages and extracts answers with source citations
    """
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.perplexity_selectors = self._load_perplexity_selectors()
        self.citation_patterns = self._load_citation_patterns()
    
    def _load_perplexity_selectors(self) -> Dict[str, List[str]]:
        """Load Perplexity-specific HTML selectors"""
        return {
            "answer_containers": [
                ".answer-content",
                ".response-text", 
                "[data-testid='answer']",
                ".prose-content",
                ".main-answer"
            ],
            "query_containers": [
                ".query-text",
                ".search-query",
                "[data-testid='query']",
                ".user-question"
            ],
            "source_citations": [
                ".citation",
                ".source-link",
                "[data-citation-index]",
                ".reference",
                "sup a"
            ],
            "source_list": [
                ".sources-list",
                ".references-section",
                ".source-references",
                "[data-testid='sources']"
            ],
            "follow_up_questions": [
                ".follow-up-questions",
                ".related-queries",
                ".suggested-questions"
            ]
        }
    
    def _load_citation_patterns(self) -> Dict[str, str]:
        """Load patterns for identifying citations"""
        return {
            "numbered_citation": r'\[(\d+)\]',
            "superscript_citation": r'<sup[^>]*>(\d+)</sup>',
            "parenthetical_citation": r'\((\d+)\)',
            "source_indicator": r'(source|according to|based on|from):\s*(.+?)(?=\.|$)',
        }
    
    def validate_source(self, source_url: str) -> bool:
        """Validate if URL is from Perplexity"""
        perplexity_domains = [
            'perplexity.ai',
            'www.perplexity.ai'
        ]
        
        parsed = urlparse(source_url.lower())
        return any(domain in parsed.netloc for domain in perplexity_domains)
    
    async def extract(self, source_url: str, **kwargs) -> List[ExtractedContent]:
        """Extract content from Perplexity search result page"""
        try:
            response = await self.fetch_with_retry(source_url)
            if not response:
                return []
            
            html_content = await response.text()
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Clean soup
            self._clean_soup(soup)
            
            # Extract query context
            user_query = self._extract_user_query(soup)
            
            # Extract main answer
            answer_content = self._extract_main_answer(soup, source_url)
            
            # Extract source citations and references
            citations = self._extract_citations(soup, source_url)
            
            # Extract follow-up questions
            follow_up_questions = self._extract_follow_up_questions(soup)
            
            if not answer_content:
                return []
            
            # Create extracted content
            extracted_content = ExtractedContent(
                title=self._generate_title(user_query, answer_content['text']),
                content=answer_content['text'],
                summary=self._create_summary(answer_content['text']),
                url=source_url,
                source_type="perplexity_search",
                language=kwargs.get('language', 'en'),
                region=kwargs.get('region', ''),
                metadata={
                    'user_query': user_query,
                    'source_citations': citations,
                    'follow_up_questions': follow_up_questions,
                    'extraction_method': 'perplexity_html_parser',
                    'answer_structure': answer_content.get('structure_type', 'prose'),
                    'citation_count': len(citations),
                    'high_quality_sources': self._count_high_quality_sources(citations)
                }
            )
            
            # Detect SDG relevance
            extracted_content.sdg_relevance = self._detect_sdg_relevance(
                answer_content['text'] + ' ' + user_query
            )
            
            # Calculate quality score
            extracted_content.quality_score = self._calculate_perplexity_quality_score(
                extracted_content, answer_content, citations
            )
            
            return [extracted_content]
            
        except Exception as e:
            logger.error(f"Error extracting Perplexity content from {source_url}: {e}")
            return []
    
    def _clean_soup(self, soup: BeautifulSoup):
        """Remove unwanted elements"""
        for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
            element.decompose()
        
        for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):
            comment.extract()
    
    def _extract_user_query(self, soup: BeautifulSoup) -> str:
        """Extract the user's search query"""
        # Try different selectors for query
        for selector in self.perplexity_selectors["query_containers"]:
            query_elem = soup.select_one(selector)
            if query_elem:
                query_text = query_elem.get_text(strip=True)
                if len(query_text) > 5:
                    return query_text
        
        # Fallback: look for query in page title or meta tags
        title_elem = soup.find('title')
        if title_elem:
            title_text = title_elem.get_text().strip()
            # Remove "Perplexity" branding
            title_text = re.sub(r'\s*-\s*Perplexity.*$', '', title_text, flags=re.IGNORECASE)
            if len(title_text) > 10:
                return title_text
        
        return ""
    
    def _extract_main_answer(self, soup: BeautifulSoup, source_url: str) -> Optional[Dict[str, Any]]:
        """Extract the main answer content"""
        # Try different selectors for answer content
        answer_elem = None
        for selector in self.perplexity_selectors["answer_containers"]:
            found_elem = soup.select_one(selector)
            if found_elem:
                answer_elem = found_elem
                break
        
        if not answer_elem:
            # Fallback: look for largest text block
            text_blocks = soup.find_all(['div', 'p', 'article'], string=True)
            if text_blocks:
                # Find the longest text block
                answer_elem = max(text_blocks, key=lambda x: len(x.get_text(strip=True)))
        
        if not answer_elem:
            return None
        
        # Extract text content
        answer_text = answer_elem.get_text(separator=' ', strip=True)
        
        if len(answer_text) < 50:
            return None
        
        # Analyze structure
        structure_type = self._analyze_answer_structure(answer_elem)
        
        # Extract any embedded lists or structured data
        lists = answer_elem.find_all(['ul', 'ol'])
        tables = answer_elem.find_all('table')
        
        return {
            'text': answer_text,
            'structure_type': structure_type,
            'has_lists': len(lists) > 0,
            'has_tables': len(tables) > 0,
            'word_count': len(answer_text.split())
        }
    
    def _analyze_answer_structure(self, answer_elem) -> str:
        """Analyze the structure of the answer"""
        # Check for lists
        if answer_elem.find_all(['ul', 'ol']):
            return "structured_list"
        
        # Check for tables
        if answer_elem.find_all('table'):
            return "tabular_data"
        
        # Check for multiple paragraphs
        paragraphs = answer_elem.find_all('p')
        if len(paragraphs) > 2:
            return "multi_paragraph"
        
        return "prose"
    
    def _extract_citations(self, soup: BeautifulSoup, base_url: str) -> List[Dict[str, Any]]:
        """Extract source citations and references"""
        citations = []
        
        # Method 1: Find citation links within text
        citation_links = soup.select('a[href*="source"], sup a, [data-citation] a')
        for link in citation_links:
            href = link.get('href')
            if href:
                citation_data = self._process_citation_link(link, href, base_url)
                if citation_data:
                    citations.append(citation_data)
        
        # Method 2: Find sources section
        for selector in self.perplexity_selectors["source_list"]:
            sources_section = soup.select_one(selector)
            if sources_section:
                source_links = sources_section.find_all('a', href=True)
                for link in source_links:
                    href = link['href']
                    citation_data = self._process_citation_link(link, href, base_url)
                    if citation_data:
                        citations.append(citation_data)
        
        # Method 3: Look for numbered references
        numbered_refs = soup.find_all(string=re.compile(r'\[\d+\]'))
        for ref in numbered_refs:
            # Try to find associated link
            parent = ref.parent if ref.parent else None
            if parent:
                link = parent.find('a', href=True)
                if link:
                    citation_data = self._process_citation_link(link, link['href'], base_url)
                    if citation_data:
                        citations.append(citation_data)
        
        # Remove duplicates and sort by relevance
        seen_urls = set()
        unique_citations = []
        for citation in citations:
            if citation['url'] not in seen_urls:
                seen_urls.add(citation['url'])
                unique_citations.append(citation)
        
        # Sort by quality score
        unique_citations.sort(key=lambda x: x.get('quality_score', 0), reverse=True)
        
        return unique_citations[:15]  # Limit to most relevant citations
    
    def _process_citation_link(self, link_elem, href: str, base_url: str) -> Optional[Dict[str, Any]]:
        """Process individual citation link"""
        # Skip invalid links
        if not href or href.startswith(('javascript:', '#', 'mailto:')):
            return None
        
        # Convert relative URLs
        if href.startswith('/'):
            href = urljoin(base_url, href)
        elif not href.startswith(('http://', 'https://')):
            return None
        
        # Extract link text and title
        link_text = link_elem.get_text(strip=True)
        link_title = link_elem.get('title', '')
        
        # Get domain information
        parsed_url = urlparse(href)
        domain = parsed_url.netloc
        
        # Assess source quality
        quality_score = self._assess_source_quality(href, link_text, domain)
        
        # Categorize source type
        source_type = self._categorize_source(href, link_text, domain)
        
        # Check for download potential
        is_download = self._is_potential_download(href, link_text)
        
        return {
            'url': href,
            'text': link_text,
            'title': link_title,
            'domain': domain,
            'source_type': source_type,
            'quality_score': quality_score,
            'is_download': is_download,
            'sdg_relevance': self._assess_citation_sdg_relevance(href, link_text)
        }
    
    def _assess_source_quality(self, url: str, text: str, domain: str) -> float:
        """Assess the quality of a citation source"""
        score = 0.0
        
        # High-quality domains
        quality_domains = [
            'un.org', 'who.int', 'worldbank.org', 'oecd.org', 'unesco.org',
            'unicef.org', 'undp.org', 'wto.org', 'imf.org', 'europa.eu',
            '.gov', '.edu', '.org'
        ]
        
        for quality_domain in quality_domains:
            if quality_domain in domain:
                score += 0.4
                break
        
        # Academic/research indicators
        academic_indicators = ['journal', 'research', 'study', 'paper', 'academic']
        if any(indicator in text.lower() or indicator in url.lower() for indicator in academic_indicators):
            score += 0.2
        
        # Data/statistics indicators
        data_indicators = ['data', 'statistics', 'report', 'dataset']
        if any(indicator in text.lower() or indicator in url.lower() for indicator in data_indicators):
            score += 0.2
        
        # Recent publication indicators
        if re.search(r'20(2[0-5])', url + text):  # 2020-2025
            score += 0.1
        
        # PDF or document indicators (often more substantial)
        if any(ext in url.lower() for ext in ['.pdf', '.doc', '.report']):
            score += 0.1
        
        return min(score, 1.0)
    
    def _categorize_source(self, url: str, text: str, domain: str) -> str:
        """Categorize the type of source"""
        url_lower = url.lower()
        text_lower = text.lower()
        
        # Government/Official
        if any(indicator in domain for indicator in ['.gov', 'un.org', 'who.int', 'worldbank.org']):
            return "official_government"
        
        # Academic/Research
        if any(indicator in text_lower for indicator in ['journal', 'research', 'study', 'academic']):
            return "academic_research"
        
        # News/Media
        if any(indicator in domain for indicator in ['news', 'times', 'post', 'guardian', 'reuters']):
            return "news_media"
        
        # NGO/Organization
        if '.org' in domain:
            return "organization_ngo"
        
        # Data/Statistics
        if any(indicator in text_lower for indicator in ['data', 'statistics', 'dataset']):
            return "data_statistics"
        
        return "general_web"
    
    def _is_potential_download(self, url: str, text: str) -> bool:
        """Check if citation might be a downloadable resource"""
        download_indicators = [
            '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx',
            '.csv', 'download', 'report', 'dataset', 'attachment'
        ]
        
        combined = f"{url.lower()} {text.lower()}"
        return any(indicator in combined for indicator in download_indicators)
    
    def _assess_citation_sdg_relevance(self, url: str, text: str) -> float:
        """Assess SDG relevance of citation"""
        score = 0.0
        combined = f"{url.lower()} {text.lower()}"
        
        # Direct SDG mentions
        sdg_terms = ['sdg', 'sustainable development', 'agenda 2030', 'goal', 'target']
        score += sum(0.2 for term in sdg_terms if term in combined)
        
        # SDG-related organizations
        sdg_orgs = ['un.org', 'undp.org', 'sustainabledevelopment', 'sdgs.un.org']
        if any(org in url.lower() for org in sdg_orgs):
            score += 0.3
        
        return min(score, 1.0)
    
    def _extract_follow_up_questions(self, soup: BeautifulSoup) -> List[str]:
        """Extract follow-up or related questions"""
        questions = []
        
        for selector in self.perplexity_selectors["follow_up_questions"]:
            questions_section = soup.select_one(selector)
            if questions_section:
                # Extract individual questions
                question_elements = questions_section.find_all(['li', 'div', 'p'])
                for elem in question_elements:
                    question_text = elem.get_text(strip=True)
                    if len(question_text) > 10 and question_text.endswith('?'):
                        questions.append(question_text)
        
        return questions[:5]  # Limit to 5 follow-up questions
    
    def _generate_title(self, user_query: str, answer_content: str) -> str:
        """Generate title from query and answer"""
        if user_query and len(user_query) > 5:
            return user_query[:200]
        
        # Fallback to first sentence of answer
        sentences = answer_content.split('.')
        first_sentence = sentences[0].strip() if sentences else answer_content[:100]
        return first_sentence[:150]
    
    def _create_summary(self, content: str) -> str:
        """Create summary from Perplexity answer"""
        # Take first 2-3 sentences
        sentences = re.split(r'[.!?]', content)
        summary_sentences = []
        char_count = 0
        
        for sentence in sentences[:3]:
            sentence = sentence.strip()
            if sentence and char_count + len(sentence) <= 250:
                summary_sentences.append(sentence)
                char_count += len(sentence)
            else:
                break
        
        summary = '. '.join(summary_sentences)
        return summary + '.' if summary and not summary.endswith('.') else summary
    
    def _detect_sdg_relevance(self, content: str) -> List[int]:
        """Detect SDG relevance in content"""
        # Enhanced SDG detection patterns
        sdg_patterns = {
            1: r'\b(poverty|poor|income.inequality|wealth.gap|social.protection|basic.needs)\b',
            2: r'\b(hunger|food.security|malnutrition|agriculture|farming|crop.yield)\b',
            3: r'\b(health|healthcare|medical|disease|mortality|wellbeing|pandemic)\b',
            4: r'\b(education|school|learning|literacy|skills|training|university)\b',
            5: r'\b(gender|women|girls|equality|empowerment|discrimination)\b',
            6: r'\b(water|sanitation|hygiene|drinking.water|clean.water|wastewater)\b',
            7: r'\b(energy|renewable|solar|wind|electricity|clean.energy|fossil)\b',
            8: r'\b(employment|jobs|economic.growth|decent.work|unemployment|labor)\b',
            9: r'\b(infrastructure|innovation|industry|technology|research|development)\b',
            10: r'\b(inequality|inclusion|discrimination|equity|marginalized|income.gap)\b',
            11: r'\b(cities|urban|housing|transport|sustainable.cities|smart.city)\b',
            12: r'\b(consumption|production|waste|recycling|sustainable|circular.economy)\b',
            13: r'\b(climate|carbon|emission|greenhouse|global.warming|adaptation|mitigation)\b',
            14: r'\b(ocean|marine|sea|fisheries|aquatic|coral|plastic.pollution)\b',
            15: r'\b(forest|biodiversity|ecosystem|wildlife|conservation|deforestation)\b',
            16: r'\b(peace|justice|institutions|governance|rule.of.law|corruption)\b',
            17: r'\b(partnership|cooperation|global|development.finance|aid|collaboration)\b'
        }
        
        content_lower = re.sub(r'[^\w\s]', ' ', content.lower())
        relevant_sdgs = []
        
        for sdg_id, pattern in sdg_patterns.items():
            if re.search(pattern, content_lower):
                relevant_sdgs.append(sdg_id)
        
        return relevant_sdgs
    
    def _count_high_quality_sources(self, citations: List[Dict[str, Any]]) -> int:
        """Count high-quality sources in citations"""
        return len([c for c in citations if c.get('quality_score', 0) > 0.6])
    
    def _calculate_perplexity_quality_score(self, content: ExtractedContent,
                                          answer_data: Dict[str, Any],
                                          citations: List[Dict[str, Any]]) -> float:
        """Calculate quality score for Perplexity content"""
        score = 0.0
        
        # Answer quality (0-0.4)
        word_count = answer_data.get('word_count', 0)
        if word_count > 300:
            score += 0.4
        elif word_count > 150:
            score += 0.25
        elif word_count > 75:
            score += 0.1
        
        # Structure quality (0-0.2)
        if answer_data.get('has_lists'):
            score += 0.1
        if answer_data.get('structure_type') in ['structured_list', 'tabular_data']:
            score += 0.1
        
        # Citation quality (0-0.3)
        high_quality_citations = self._count_high_quality_sources(citations)
        if high_quality_citations >= 3:
            score += 0.3
        elif high_quality_citations >= 1:
            score += 0.15
        
        # SDG relevance (0-0.1)
        if len(content.sdg_relevance) >= 2:
            score += 0.1
        elif len(content.sdg_relevance) >= 1:
            score += 0.05
        
        return min(score, 1.0)


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/content_extraction/extractors/rss_extractor.py
"""
RSS Feed content extractor
"""
import logging
from typing import List, Dict, Any, Optional
import asyncio
import feedparser
from datetime import datetime
import re
from .base_extractor import BaseExtractor, ExtractedContent

logger = logging.getLogger(__name__)

class RSSExtractor(BaseExtractor):
    """
    RSS/Atom feed content extractor
    """
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.max_entries = config.get("max_entries_per_feed", 50)
    
    def validate_source(self, source_url: str) -> bool:
        """Validate RSS feed URL"""
        rss_indicators = ['/rss', '/feed', '.rss', '.xml', 'rss.xml', 'feed.xml']
        return any(indicator in source_url.lower() for indicator in rss_indicators)
    
    async def extract(self, source_url: str, **kwargs) -> List[ExtractedContent]:
        """Extract content from RSS feed"""
        try:
            # Fetch RSS feed
            response = await self.fetch_with_retry(source_url)
            if not response:
                return []
            
            feed_content = await response.text()
            
            # Parse feed
            feed = feedparser.parse(feed_content)
            
            if feed.bozo and hasattr(feed, 'bozo_exception'):
                logger.warning(f"RSS parsing issues for {source_url}: {feed.bozo_exception}")
            
            extracted_items = []
            
            # Process feed entries
            for entry in feed.entries[:self.max_entries]:
                content_item = self._process_feed_entry(entry, feed, source_url)
                if content_item:
                    extracted_items.append(content_item)
            
            logger.info(f"Extracted {len(extracted_items)} items from RSS feed: {source_url}")
            return extracted_items
            
        except Exception as e:
            logger.error(f"Error processing RSS feed {source_url}: {e}")
            return []
    
    def _process_feed_entry(self, entry: Any, feed: Any, source_url: str) -> Optional[ExtractedContent]:
        """Process individual RSS feed entry"""
        try:
            # Extract title
            title = getattr(entry, 'title', '').strip()
            if not title:
                return None
            
            # Extract content
            content = self._extract_entry_content(entry)
            if not content or len(content.strip()) < 50:
                return None
            
            # Extract summary
            summary = getattr(entry, 'summary', '')
            if not summary:
                summary = content[:300] + "..." if len(content) > 300 else content
            
            # Extract URL
            entry_url = getattr(entry, 'link', source_url)
            
            # Extract publication date
            pub_date = self._extract_publication_date(entry)
            
            # Extract metadata
            metadata = self._extract_entry_metadata(entry, feed)
            
            # Detect language and region
            language = self._detect_entry_language(entry, feed)
            region = self._detect_entry_region(content, metadata)
            
            content_item = ExtractedContent(
                title=title[:200],  # Limit title length
                content=content,
                summary=summary[:500],  # Limit summary length
                url=entry_url,
                source_type="rss_feed",
                language=language,
                region=region,
                extracted_at=datetime.utcnow(),
                metadata=metadata
            )
            
            # Calculate quality score
            content_item.quality_score = self._calculate_rss_quality_score(content_item, entry)
            
            # Simple SDG relevance detection
            content_item.sdg_relevance = self._detect_sdg_keywords(content)
            
            return content_item
            
        except Exception as e:
            logger.error(f"Error processing RSS entry: {e}")
            return None
    
    def _extract_entry_content(self, entry: Any) -> str:
        """Extract content from RSS entry"""
        # Try different content fields
        content_fields = ['content', 'description', 'summary']
        
        for field in content_fields:
            content_data = getattr(entry, field, None)
            if content_data:
                if isinstance(content_data, list) and content_data:
                    # Handle content list (like Atom feeds)
                    content_text = content_data[0].get('value', '')
                elif isinstance(content_data, dict):
                    content_text = content_data.get('value', '')
                else:
                    content_text = str(content_data)
                
                if content_text:
                    # Clean HTML tags
                    clean_content = re.sub(r'<[^>]+>', ' ', content_text)
                    clean_content = re.sub(r'\s+', ' ', clean_content).strip()
                    
                    if len(clean_content) > 100:
                        return clean_content
        
        return ""
    
    def _extract_publication_date(self, entry: Any) -> Optional[str]:
        """Extract publication date from entry"""
        date_fields = ['published_parsed', 'updated_parsed']
        
        for field in date_fields:
            date_data = getattr(entry, field, None)
            if date_data:
                try:
                    # Convert time struct to ISO format
                    dt = datetime(*date_data[:6])
                    return dt.isoformat()
                except:
                    continue
        
        # Try string fields as fallback
        string_date_fields = ['published', 'updated']
        for field in string_date_fields:
            date_str = getattr(entry, field, '')
            if date_str:
                return date_str
        
        return None
    
    def _extract_entry_metadata(self, entry: Any, feed: Any) -> Dict[str, Any]:
        """Extract metadata from RSS entry and feed"""
        metadata = {
            "feed_title": getattr(feed.feed, 'title', ''),
            "feed_description": getattr(feed.feed, 'description', ''),
            "feed_url": getattr(feed.feed, 'link', ''),
            "entry_id": getattr(entry, 'id', ''),
            "extraction_method": "rss_feed"
        }
        
        # Author information
        author = getattr(entry, 'author', '')
        if author:
            metadata['author'] = author
        
        # Tags/categories
        tags = getattr(entry, 'tags', [])
        if tags:
            tag_list = []
            for tag in tags:
                if isinstance(tag, dict):
                    tag_list.append(tag.get('term', ''))
                else:
                    tag_list.append(str(tag))
            metadata['tags'] = [t for t in tag_list if t]
        
        # Publication date
        pub_date = self._extract_publication_date(entry)
        if pub_date:
            metadata['publication_date'] = pub_date
        
        # Additional fields
        for field in ['rights', 'publisher']:
            value = getattr(entry, field, '')
            if value:
                metadata[field] = value
        
        return metadata
    
    def _detect_entry_language(self, entry: Any, feed: Any) -> str:
        """Detect language from entry or feed"""
        # Check entry language
        entry_lang = getattr(entry, 'language', '')
        if entry_lang:
            return entry_lang[:2]
        
        # Check feed language
        feed_lang = getattr(feed.feed, 'language', '')
        if feed_lang:
            return feed_lang[:2]
        
        # Simple detection based on content
        title = getattr(entry, 'title', '')
        content = self._extract_entry_content(entry)
        combined_text = f"{title} {content}".lower()
        
        # Basic language indicators
        if any(word in combined_text for word in ['the', 'and', 'is', 'in']):
            return 'en'
        elif any(word in combined_text for word in ['der', 'die', 'das', 'und']):
            return 'de'
        elif any(word in combined_text for word in ['le', 'de', 'et', 'dans']):
            return 'fr'
        
        return 'en'  # Default
    
    def _detect_entry_region(self, content: str, metadata: Dict[str, Any]) -> str:
        """Detect region from content and metadata"""
        # Check feed metadata first
        feed_url = metadata.get('feed_url', '')
        if feed_url:
            # Domain-based region detection
            domain_regions = {
                '.eu': 'EU',
                '.gov': 'USA',
                '.cn': 'China',
                '.in': 'India'
            }
            for domain, region in domain_regions.items():
                if domain in feed_url:
                    return region
        
        # Content-based detection
        region_indicators = {
            'EU': ['european', 'europe', 'brussels', 'eu'],
            'USA': ['america', 'united states', 'washington'],
            'China': ['china', 'chinese', 'beijing'],
            'India': ['india', 'indian', 'delhi']
        }
        
        content_lower = content.lower()
        for region, keywords in region_indicators.items():
            if any(keyword in content_lower for keyword in keywords):
                return region
        
        return ""
    
    def _calculate_rss_quality_score(self, content: ExtractedContent, entry: Any) -> float:
        """Calculate quality score for RSS content"""
        score = 0.0
        
        # Title quality (0-0.2)
        if content.title and len(content.title.strip()) > 20:
            score += 0.2
        elif content.title and len(content.title.strip()) > 10:
            score += 0.1
        
        # Content length (0-0.3)
        content_length = len(content.content)
        if content_length > 1000:
            score += 0.3
        elif content_length > 500:
            score += 0.2
        elif content_length > 200:
            score += 0.1
        
        # Has author (0-0.1)
        if content.metadata.get('author'):
            score += 0.1
        
        # Has publication date (0-0.1)
        if content.metadata.get('publication_date'):
            score += 0.1
        
        # Has tags/categories (0-0.1)
        if content.metadata.get('tags'):
            score += 0.1
        
        # Has valid URL (0-0.1)
        if content.url and content.url.startswith(('http://', 'https://')):
            score += 0.1
        
        # SDG relevance (0-0.1)
        if content.sdg_relevance:
            score += 0.1
        
        return min(score, 1.0)
    
    def _detect_sdg_keywords(self, content: str) -> List[int]:
        """Simple SDG keyword detection"""
        sdg_patterns = {
            1: r'\b(poverty|poor|income|wealth|social protection)\b',
            2: r'\b(hunger|food|nutrition|agriculture|farming)\b',
            3: r'\b(health|medical|disease|mortality|healthcare)\b',
            4: r'\b(education|learning|school|literacy|skills)\b',
            5: r'\b(gender|women|girls|equality|empowerment)\b',
            6: r'\b(water|sanitation|hygiene|drinking water)\b',
            7: r'\b(energy|renewable|electricity|clean energy)\b',
            8: r'\b(employment|jobs|economic growth|decent work)\b',
            9: r'\b(infrastructure|innovation|industry|technology)\b',
            10: r'\b(inequality|inclusion|discrimination|equity)\b',
            11: r'\b(cities|urban|housing|transport|sustainable cities)\b',
            12: r'\b(consumption|production|waste|recycling|sustainable)\b',
            13: r'\b(climate|carbon|emission|greenhouse|adaptation)\b',
            14: r'\b(ocean|marine|sea|fisheries|aquatic)\b',
            15: r'\b(forest|biodiversity|ecosystem|wildlife|conservation)\b',
            16: r'\b(peace|justice|institutions|governance|rule of law)\b',
            17: r'\b(partnership|cooperation|global|development finance)\b'
        }
        
        content_lower = content.lower()
        relevant_sdgs = []
        
        for sdg_id, pattern in sdg_patterns.items():
            if re.search(pattern, content_lower):
                relevant_sdgs.append(sdg_id)
        
        return relevant_sdgs
    
    async def extract_multiple_feeds(self, feed_urls: List[str], **kwargs) -> List[ExtractedContent]:
        """Extract content from multiple RSS feeds concurrently"""
        all_content = []
        
        # Process feeds in batches to avoid overwhelming servers
        batch_size = self.config.get("concurrent_feeds", 3)
        
        for i in range(0, len(feed_urls), batch_size):
            batch_urls = feed_urls[i:i + batch_size]
            batch_results = await self.process_batch(batch_urls, **kwargs)
            all_content.extend(batch_results)
            
            # Brief pause between batches
            if i + batch_size < len(feed_urls):
                await asyncio.sleep(1)
        
        logger.info(f"Extracted {len(all_content)} total items from {len(feed_urls)} RSS feeds")
        return all_content


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/content_extraction/extractors/web_extractor.py
"""
General web scraping extractor for various websites
"""
import logging
from typing import List, Dict, Any, Optional
import asyncio
from bs4 import BeautifulSoup, Comment
import re
from urllib.parse import urljoin, urlparse
from .base_extractor import BaseExtractor, ExtractedContent

logger = logging.getLogger(__name__)

class WebExtractor(BaseExtractor):
    """
    General purpose web content extractor
    Handles various website types with smart content detection
    """
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.content_selectors = self._load_content_selectors()
        self.blocked_elements = ['script', 'style', 'nav', 'header', 'footer', 'aside']
    
    def _load_content_selectors(self) -> Dict[str, List[str]]:
        """Load CSS selectors for different website types"""
        return {
            "article": [
                "article", 
                ".article", 
                ".post", 
                ".content", 
                ".main-content",
                ".entry-content",
                "#content",
                ".page-content"
            ],
            "title": [
                "h1",
                ".title", 
                ".article-title", 
                ".post-title",
                "title"
            ],
            "summary": [
                ".summary", 
                ".excerpt", 
                ".abstract", 
                ".lead",
                ".intro"
            ],
            "metadata": [
                ".metadata", 
                ".post-meta", 
                ".article-meta",
                "time[datetime]",
                ".author",
                ".date"
            ]
        }
    
    def validate_source(self, source_url: str) -> bool:
        """Validate if URL is suitable for web scraping"""
        try:
            parsed = urlparse(source_url)
            if not parsed.scheme or not parsed.netloc:
                return False
            
            # Block certain file types
            blocked_extensions = ['.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx']
            if any(source_url.lower().endswith(ext) for ext in blocked_extensions):
                return False
            
            return True
        except:
            return False
    
    async def extract(self, source_url: str, **kwargs) -> List[ExtractedContent]:
        """Extract content from web page"""
        try:
            if not self.validate_source(source_url):
                logger.warning(f"Invalid source URL: {source_url}")
                return []
            
            # Fetch page content
            response = await self.fetch_with_retry(source_url)
            if not response:
                return []
            
            html_content = await response.text()
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Remove unwanted elements
            self._clean_soup(soup)
            
            # Extract structured content
            extracted_content = self._extract_structured_content(soup, source_url)
            
            if extracted_content:
                # Enhance with additional processing
                extracted_content = await self._enhance_content(extracted_content, soup)
                return [extracted_content]
            
            return []
            
        except Exception as e:
            logger.error(f"Error extracting from {source_url}: {e}")
            return []
    
    def _clean_soup(self, soup: BeautifulSoup):
        """Remove unwanted elements from soup"""
        # Remove blocked elements
        for element_name in self.blocked_elements:
            for element in soup.find_all(element_name):
                element.decompose()
        
        # Remove comments
        for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):
            comment.extract()
        
        # Remove elements with no text content
        for element in soup.find_all():
            if not element.get_text(strip=True):
                element.decompose()
    
    def _extract_structured_content(self, soup: BeautifulSoup, source_url: str) -> Optional[ExtractedContent]:
        """Extract structured content from soup"""
        # Extract title
        title = self._extract_title(soup)
        if not title:
            return None
        
        # Extract main content
        content = self._extract_main_content(soup)
        if not content or len(content.strip()) < 100:
            return None
        
        # Extract summary
        summary = self._extract_summary(soup)
        
        # Extract metadata
        metadata = self._extract_metadata(soup, source_url)
        
        # Detect language and region
        language = self._detect_language(soup, content)
        region = self._detect_region_from_content(content, soup)
        
        extracted_content = ExtractedContent(
            title=title,
            content=content,
            summary=summary,
            url=source_url,
            source_type="web_scraping",
            language=language,
            region=region,
            metadata=metadata
        )
        
        # Calculate quality score
        extracted_content.quality_score = self.estimate_quality_score(extracted_content)
        
        return extracted_content
    
    def _extract_title(self, soup: BeautifulSoup) -> str:
        """Extract page title"""
        for selector in self.content_selectors["title"]:
            elements = soup.select(selector)
            if elements:
                title = elements[0].get_text(strip=True)
                if title and len(title) > 10:
                    return title[:200]  # Limit title length
        
        # Fallback to page title
        title_tag = soup.find('title')
        if title_tag:
            return title_tag.get_text(strip=True)[:200]
        
        return ""
    
    def _extract_main_content(self, soup: BeautifulSoup) -> str:
        """Extract main content using multiple strategies"""
        content_parts = []
        
        # Strategy 1: Use content selectors
        for selector in self.content_selectors["article"]:
            elements = soup.select(selector)
            if elements:
                for element in elements:
                    text = element.get_text(separator=' ', strip=True)
                    if len(text) > 200:  # Minimum content length
                        content_parts.append(text)
                        break
                if content_parts:
                    break
        
        # Strategy 2: Find paragraphs with substantial content
        if not content_parts:
            paragraphs = soup.find_all('p')
            paragraph_texts = []
            for p in paragraphs:
                text = p.get_text(strip=True)
                if len(text) > 50:  # Minimum paragraph length
                    paragraph_texts.append(text)
            
            if len(paragraph_texts) >= 3:  # At least 3 substantial paragraphs
                content_parts = paragraph_texts
        
        # Strategy 3: Extract all text as fallback
        if not content_parts:
            body = soup.find('body')
            if body:
                text = body.get_text(separator=' ', strip=True)
                if len(text) > 500:
                    content_parts = [text]
        
        # Combine and clean content
        if content_parts:
            combined_content = '\n\n'.join(content_parts)
            # Clean up whitespace
            combined_content = re.sub(r'\s+', ' ', combined_content).strip()
            return combined_content
        
        return ""
    
    def _extract_summary(self, soup: BeautifulSoup) -> str:
        """Extract summary/excerpt"""
        for selector in self.content_selectors["summary"]:
            elements = soup.select(selector)
            if elements:
                summary = elements[0].get_text(strip=True)
                if summary and len(summary) > 50:
                    return summary[:500]  # Limit summary length
        
        # Fallback: first paragraph or meta description
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc and meta_desc.get('content'):
            return meta_desc['content'][:500]
        
        # First substantial paragraph
        paragraphs = soup.find_all('p')
        for p in paragraphs:
            text = p.get_text(strip=True)
            if len(text) > 100:
                return text[:500]
        
        return ""
    
    def _extract_metadata(self, soup: BeautifulSoup, source_url: str) -> Dict[str, Any]:
        """Extract metadata from page"""
        metadata = {
            "source_domain": urlparse(source_url).netloc,
            "extraction_method": "web_scraping"
        }
        
        # Meta tags
        meta_tags = soup.find_all('meta')
        for tag in meta_tags:
            name = tag.get('name') or tag.get('property')
            content = tag.get('content')
            if name and content:
                metadata[f"meta_{name}"] = content
        
        # Author
        author_selectors = ['.author', '[rel="author"]', '.byline']
        for selector in author_selectors:
            author_elem = soup.select_one(selector)
            if author_elem:
                metadata['author'] = author_elem.get_text(strip=True)
                break
        
        # Publication date
        date_selectors = ['time[datetime]', '.date', '.published']
        for selector in date_selectors:
            date_elem = soup.select_one(selector)
            if date_elem:
                date_text = date_elem.get('datetime') or date_elem.get_text(strip=True)
                if date_text:
                    metadata['publication_date'] = date_text
                    break
        
        # Word count
        content = soup.get_text()
        word_count = len(re.findall(r'\b\w+\b', content))
        metadata['word_count'] = word_count
        
        return metadata
    
    def _detect_language(self, soup: BeautifulSoup, content: str) -> str:
        """Detect content language"""
        # Check HTML lang attribute
        html_tag = soup.find('html')
        if html_tag and html_tag.get('lang'):
            lang = html_tag['lang'][:2]  # Get first 2 characters
            return lang
        
        # Simple language detection based on common words
        language_indicators = {
            'en': ['the', 'and', 'is', 'in', 'to', 'of', 'for'],
            'de': ['der', 'die', 'das', 'und', 'ist', 'in', 'zu'],
            'fr': ['le', 'de', 'et', 'à', 'un', 'il', 'être'],
            'es': ['el', 'de', 'que', 'y', 'a', 'en', 'un'],
            'zh': ['的', '是', '在', '了', '不', '与', '也'],
        }
        
        content_lower = content.lower()
        scores = {}
        
        for lang, indicators in language_indicators.items():
            score = sum(1 for word in indicators if word in content_lower)
            scores[lang] = score
        
        if scores:
            detected_lang = max(scores, key=scores.get)
            if scores[detected_lang] > 2:  # Minimum threshold
                return detected_lang
        
        return 'en'  # Default to English
    
    def _detect_region_from_content(self, content: str, soup: BeautifulSoup) -> str:
        """Detect region from content and metadata"""
        # Check meta tags first
        geo_meta = soup.find('meta', attrs={'name': 'geo.region'})
        if geo_meta:
            return geo_meta.get('content', '')
        
        # Region keywords detection
        region_keywords = {
            "EU": ["european union", "europe", "eu", "brussels", "eurostat"],
            "USA": ["united states", "america", "usa", "washington dc"],
            "China": ["china", "chinese", "beijing", "prc"],
            "India": ["india", "indian", "new delhi", "bharat"],
            "ASEAN": ["asean", "southeast asia", "southeast asian"],
            "BRICS": ["brics", "emerging economies"]
        }
        
        content_lower = content.lower()
        region_scores = {}
        
        for region, keywords in region_keywords.items():
            score = sum(1 for keyword in keywords if keyword in content_lower)
            if score > 0:
                region_scores[region] = score
        
        if region_scores:
            return max(region_scores, key=region_scores.get)
        
        return ""
    
    async def _enhance_content(self, content: ExtractedContent, soup: BeautifulSoup) -> ExtractedContent:
        """Enhance extracted content with additional processing"""
        # Extract links for further processing
        links = []
        for link in soup.find_all('a', href=True):
            href = link['href']
            if href.startswith(('http://', 'https://')):
                links.append({
                    'url': href,
                    'text': link.get_text(strip=True)
                })
        
        content.metadata['internal_links'] = links[:20]  # Limit links
        
        # Extract images
        images = []
        for img in soup.find_all('img', src=True):
            images.append({
                'src': img['src'],
                'alt': img.get('alt', ''),
                'title': img.get('title', '')
            })
        
        content.metadata['images'] = images[:10]  # Limit images
        
        # Simple SDG keyword detection for initial relevance
        content.sdg_relevance = self._detect_sdg_relevance(content.content)
        
        return content
    
    def _detect_sdg_relevance(self, content: str) -> List[int]:
        """Simple SDG relevance detection"""
        # Basic keyword-based SDG detection
        sdg_keywords = {
            1: ["poverty", "poor", "income"],
            2: ["hunger", "food", "agriculture"],
            3: ["health", "medical", "disease"],
            13: ["climate", "carbon", "emission", "greenhouse"],
            # Add more as needed
        }
        
        content_lower = content.lower()
        relevant_sdgs = []
        
        for sdg_id, keywords in sdg_keywords.items():
            if any(keyword in content_lower for keyword in keywords):
                relevant_sdgs.append(sdg_id)
        
        return relevant_sdgs


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/content_extraction/main.py
"""
Content Extraction Service - FastAPI Application
Handles extraction from multiple sources: Gemini, web pages, newsletters, RSS feeds
"""
import logging
from typing import List, Dict, Any, Optional
import asyncio
from datetime import datetime

from fastapi import FastAPI, HTTPException, BackgroundTasks, Depends, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field, HttpUrl
import httpx

from extractors.gemini_extractor import GeminiExtractor
from extractors.web_extractor import WebExtractor
from extractors.newsletter_extractor import NewsletterExtractor
from extractors.rss_extractor import RSSExtractor

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Pydantic models
class ExtractionRequest(BaseModel):
    url: HttpUrl = Field(..., description="Source URL to extract content from")
    source_type: Optional[str] = Field(None, description="Source type hint: web, rss, newsletter, gemini")
    language: Optional[str] = Field("en", description="Expected content language")
    region: Optional[str] = Field(None, description="Expected content region")
    
class BatchExtractionRequest(BaseModel):
    urls: List[HttpUrl] = Field(..., description="List of URLs to extract", max_items=20)
    source_type: Optional[str] = Field(None, description="Source type for all URLs")
    language: Optional[str] = Field("en", description="Expected content language")
    region: Optional[str] = Field(None, description="Expected content region")

class GeminiAnalysisRequest(BaseModel):
    content: str = Field(..., description="Content to analyze with Gemini", min_length=100)
    source_url: Optional[HttpUrl] = Field(None, description="Original source URL")
    language: Optional[str] = Field("en", description="Content language")

class ExtractionResponse(BaseModel):
    success: bool
    extracted_count: int
    content: List[Dict[str, Any]]
    metadata: Dict[str, Any]
    processing_time: float

# Global extractors
extractors = {}

async def initialize_extractors():
    """Initialize extraction services"""
    config = {
        "retry_attempts": 3,
        "retry_delay": 1.0,
        "timeout": 30,
        "concurrent_requests": 5,
        "max_entries_per_feed": 50,
        "user_agent": "SDG-Pipeline-ContentExtractor/1.0"
    }
    
    extractors['web'] = WebExtractor(config)
    extractors['newsletter'] = NewsletterExtractor(config)
    extractors['rss'] = RSSExtractor(config)
    extractors['gemini'] = GeminiExtractor(config)
    
    logger.info("Content extractors initialized")

# FastAPI app
app = FastAPI(
    title="SDG Content Extraction Service",
    description="Microservice for extracting and analyzing content from multiple sources",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.on_event("startup")
async def startup_event():
    """Initialize services on startup"""
    await initialize_extractors()

@app.on_event("shutdown")
async def shutdown_event():
    """Cleanup on shutdown"""
    for extractor in extractors.values():
        if hasattr(extractor, 'session') and extractor.session:
            await extractor.session.close()

# Health check
@app.get("/health")
async def health_check():
    """Service health check"""
    return {
        "status": "healthy",
        "service": "SDG Content Extraction Service",
        "version": "1.0.0",
        "extractors_loaded": len(extractors),
        "available_extractors": list(extractors.keys())
    }

# Single URL extraction
@app.post("/extract", response_model=ExtractionResponse)
async def extract_content(request: ExtractionRequest):
    """Extract content from a single URL"""
    start_time = datetime.now()
    
    try:
        # Determine extractor type
        extractor_type = await determine_extractor_type(str(request.url), request.source_type)
        extractor = extractors.get(extractor_type)
        
        if not extractor:
            raise HTTPException(status_code=400, detail=f"Unsupported source type: {extractor_type}")
        
        # Extract content
        async with extractor:
            extracted_content = await extractor.extract(
                str(request.url),
                language=request.language,
                region=request.region
            )
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        return ExtractionResponse(
            success=True,
            extracted_count=len(extracted_content),
            content=[item.to_dict() for item in extracted_content],
            metadata={
                "source_url": str(request.url),
                "extractor_used": extractor_type,
                "processing_time": processing_time
            },
            processing_time=processing_time
        )
        
    except Exception as e:
        logger.error(f"Error extracting from {request.url}: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# Batch extraction
@app.post("/extract/batch", response_model=ExtractionResponse)
async def extract_batch_content(request: BatchExtractionRequest):
    """Extract content from multiple URLs"""
    start_time = datetime.now()
    
    try:
        all_content = []
        extractor_usage = {}
        
        # Group URLs by extractor type
        url_groups = {}
        for url in request.urls:
            extractor_type = await determine_extractor_type(str(url), request.source_type)
            url_groups.setdefault(extractor_type, []).append(str(url))
        
        # Process each group with appropriate extractor
        for extractor_type, urls in url_groups.items():
            extractor = extractors.get(extractor_type)
            if not extractor:
                logger.warning(f"Skipping unsupported extractor type: {extractor_type}")
                continue
            
            async with extractor:
                batch_content = await extractor.process_batch(
                    urls,
                    language=request.language,
                    region=request.region
                )
                all_content.extend(batch_content)
                extractor_usage[extractor_type] = len(urls)
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        return ExtractionResponse(
            success=True,
            extracted_count=len(all_content),
            content=[item.to_dict() for item in all_content],
            metadata={
                "total_urls": len(request.urls),
                "extractor_usage": extractor_usage,
                "processing_time": processing_time
            },
            processing_time=processing_time
        )
        
    except Exception as e:
        logger.error(f"Error in batch extraction: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# RSS feed extraction
@app.post("/extract/rss")
async def extract_rss_feeds(
    feed_urls: List[HttpUrl],
    max_entries: int = Query(50, description="Maximum entries per feed"),
    language: Optional[str] = Query("en", description="Expected language"),
    region: Optional[str] = Query(None, description="Expected region")
):
    """Extract content from RSS feeds"""
    start_time = datetime.now()
    
    try:
        rss_extractor = extractors['rss']
        
        # Update config for this request
        rss_extractor.max_entries = max_entries
        
        async with rss_extractor:
            all_content = await rss_extractor.extract_multiple_feeds(
                [str(url) for url in feed_urls],
                language=language,
                region=region
            )
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        return ExtractionResponse(
            success=True,
            extracted_count=len(all_content),
            content=[item.to_dict() for item in all_content],
            metadata={
                "feed_count": len(feed_urls),
                "max_entries_per_feed": max_entries,
                "processing_time": processing_time
            },
            processing_time=processing_time
        )
        
    except Exception as e:
        logger.error(f"Error extracting RSS feeds: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# Gemini analysis
@app.post("/analyze/gemini")
async def analyze_with_gemini(request: GeminiAnalysisRequest):
    """Analyze content using Gemini 2.5"""
    start_time = datetime.now()
    
    try:
        gemini_extractor = extractors['gemini']
        
        async with gemini_extractor:
            analyzed_content = await gemini_extractor.extract(
                request.content,
                source_url=str(request.source_url) if request.source_url else "",
                language=request.language
            )
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        return ExtractionResponse(
            success=True,
            extracted_count=len(analyzed_content),
            content=[item.to_dict() for item in analyzed_content],
            metadata={
                "analysis_type": "gemini_2.5",
                "content_length": len(request.content),
                "processing_time": processing_time
            },
            processing_time=processing_time
        )
        
    except Exception as e:
        logger.error(f"Error in Gemini analysis: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# Utility endpoints
@app.get("/extractors")
async def get_available_extractors():
    """Get information about available extractors"""
    return {
        "extractors": {
            name: {
                "type": type(extractor).__name__,
                "description": extractor.__class__.__doc__.strip() if extractor.__class__.__doc__ else ""
            }
            for name, extractor in extractors.items()
        }
    }

@app.post("/validate-url")
async def validate_url(url: HttpUrl, source_type: Optional[str] = None):
    """Validate if URL can be processed by available extractors"""
    try:
        extractor_type = await determine_extractor_type(str(url), source_type)
        extractor = extractors.get(extractor_type)
        
        if not extractor:
            return {
                "valid": False,
                "reason": f"No extractor available for type: {extractor_type}"
            }
        
        is_valid = extractor.validate_source(str(url))
        
        return {
            "valid": is_valid,
            "extractor_type": extractor_type,
            "supported": True
        }
        
    except Exception as e:
        return {
            "valid": False,
            "reason": str(e)
        }

# Integration with existing SDG pipeline
@app.post("/extract-and-forward")
async def extract_and_forward_to_pipeline(
    request: ExtractionRequest,
    background_tasks: BackgroundTasks
):
    """Extract content and forward to data processing service"""
    try:
        # Extract content
        extraction_result = await extract_content(request)
        
        if extraction_result.success:
            # Forward to data processing service in background
            background_tasks.add_task(
                forward_to_processing_service,
                extraction_result.content
            )
        
        return {
            "extraction_success": extraction_result.success,
            "extracted_count": extraction_result.extracted_count,
            "forwarded_to_processing": True
        }
        
    except Exception as e:
        logger.error(f"Error in extract-and-forward: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# Helper functions
async def determine_extractor_type(url: str, hint: Optional[str] = None) -> str:
    """Determine which extractor to use for a URL"""
    url_lower = url.lower()
    
    # Use hint if provided and valid
    if hint and hint in extractors:
        return hint
    
    # Auto-detect based on URL patterns
    if any(pattern in url_lower for pattern in ['/rss', '/feed', '.rss', '.xml']):
        return 'rss'
    elif any(pattern in url_lower for pattern in ['newsletter', 'digest', 'bulletin']):
        return 'newsletter'
    elif url_lower.startswith(('http://', 'https://')):
        return 'web'
    else:
        return 'gemini'  # Default for content analysis

async def forward_to_processing_service(content_items: List[Dict[str, Any]]):
    """Forward extracted content to data processing service"""
    try:
        processing_url = "http://data_processing_service:8001/process-content"
        
        async with httpx.AsyncClient() as client:
            response = await client.post(
                processing_url,
                json={"content_items": content_items},
                timeout=60
            )
            
            if response.status_code == 200:
                logger.info(f"Successfully forwarded {len(content_items)} items to processing service")
            else:
                logger.error(f"Error forwarding to processing service: {response.status_code}")
                
    except Exception as e:
        logger.error(f"Error forwarding content to processing service: {e}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8004,
        reload=True,
        log_level="info"
    )


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/content_extraction/processors/__init__.py


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/content_extraction/processors/content_classifier.py


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/content_extraction/processors/html_processor.py


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/content_extraction/processors/quality_validator.py


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/content_extraction/requirements.txt
fastapi==0.104.1
uvicorn[standard]==0.24.0
aiohttp==3.9.1
beautifulsoup4==4.12.2
feedparser==6.0.10
pydantic==2.5.0
httpx==0.25.2
python-multipart==0.0.6
lxml==4.9.3
html5lib==1.1
python-dateutil==2.8.2
asyncio==3.4.3


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/core/secrets_manger.py
# /sdg_root/src/core/secrets_manager.py
import os
import base64
from cryptography.fernet import Fernet
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC
import keyring
import logging

logger = logging.getLogger(__name__)

class SecretsManager:
    def __init__(self):
        self.key = self._get_or_create_key()
        self.cipher = Fernet(self.key)
    
    def _get_or_create_key(self):
        """Get encryption key from secure storage or create new one"""
        try:
            # Try to get key from system keyring
            key = keyring.get_password("sdg_pipeline", "encryption_key")
            if key:
                return key.encode()
            
            # Generate new key if none exists
            password = os.environ.get('MASTER_PASSWORD', 'default_dev_password').encode()
            salt = os.environ.get('ENCRYPTION_SALT', 'default_salt').encode()
            
            kdf = PBKDF2HMAC(
                algorithm=hashes.SHA256(),
                length=32,
                salt=salt,
                iterations=100000,
            )
            key = base64.urlsafe_b64encode(kdf.derive(password))
            
            # Store in system keyring for production
            if os.environ.get('ENVIRONMENT') == 'production':
                keyring.set_password("sdg_pipeline", "encryption_key", key.decode())
            
            return key
        except Exception as e:
            logger.error(f"Error managing encryption key: {e}")
            raise
    
    def encrypt_secret(self, plaintext: str) -> str:
        """Encrypt a secret value"""
        return self.cipher.encrypt(plaintext.encode()).decode()
    
    def decrypt_secret(self, encrypted: str) -> str:
        """Decrypt a secret value"""
        return self.cipher.decrypt(encrypted.encode()).decode()
    
    def get_secret(self, key: str) -> str:
        """Get decrypted secret from environment"""
        encrypted_value = os.environ.get(f"{key}_ENCRYPTED")
        if encrypted_value:
            return self.decrypt_secret(encrypted_value)
        
        # Fallback to plaintext for development (with warning)
        plaintext_value = os.environ.get(key)
        if plaintext_value and os.environ.get('ENVIRONMENT') != 'production':
            logger.warning(f"Using plaintext secret for {key} in development")
            return plaintext_value
        
        raise ValueError(f"Secret {key} not found or not properly encrypted")

# Initialize global secrets manager
secrets_manager = SecretsManager()


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/core/url_validator.py
# /sdg_root/src/core/url_validator.py
import re
import socket
import ipaddress
from urllib.parse import urlparse
from typing import Set, List
import logging

logger = logging.getLogger(__name__)

class URLValidator:
    def __init__(self):
        # Blocked IP ranges (RFC 1918 private networks, localhost, etc.)
        self.blocked_networks = [
            ipaddress.ip_network('127.0.0.0/8'),      # Localhost
            ipaddress.ip_network('10.0.0.0/8'),       # Private A
            ipaddress.ip_network('172.16.0.0/12'),    # Private B
            ipaddress.ip_network('192.168.0.0/16'),   # Private C
            ipaddress.ip_network('169.254.0.0/16'),   # Link-local
            ipaddress.ip_network('224.0.0.0/4'),      # Multicast
            ipaddress.ip_network('::1/128'),          # IPv6 localhost
            ipaddress.ip_network('fc00::/7'),         # IPv6 private
        ]
        
        # Allowed schemes
        self.allowed_schemes = {'http', 'https'}
        
        # Blocked ports
        self.blocked_ports = {
            22,    # SSH
            23,    # Telnet
            25,    # SMTP
            53,    # DNS
            135,   # RPC
            139,   # NetBIOS
            445,   # SMB
            1433,  # SQL Server
            3306,  # MySQL
            5432,  # PostgreSQL
            6379,  # Redis
            8080,  # Weaviate (our internal service)
        }
        
        # Allowed domains (whitelist for high security)
        self.allowed_domains = {
            'un.org', 'who.int', 'worldbank.org', 'oecd.org',
            'europa.eu', 'unicef.org', 'undp.org', 'unesco.org',
            'doi.org', 'crossref.org', 'googleapis.com'
        }
    
    def validate_url(self, url: str) -> tuple[bool, str]:
        """
        Validate URL for SSRF protection
        Returns: (is_valid, error_message)
        """
        try:
            # Basic URL parsing
            parsed = urlparse(url.lower())
            
            # Check scheme
            if parsed.scheme not in self.allowed_schemes:
                return False, f"Scheme '{parsed.scheme}' not allowed"
            
            # Check hostname exists
            if not parsed.hostname:
                return False, "No hostname provided"
            
            # Check domain whitelist
            if not self._is_domain_allowed(parsed.hostname):
                return False, f"Domain '{parsed.hostname}' not in whitelist"
            
            # Resolve hostname to IP
            try:
                ip_str = socket.gethostbyname(parsed.hostname)
                ip = ipaddress.ip_address(ip_str)
            except (socket.gaierror, ValueError) as e:
                return False, f"Cannot resolve hostname: {e}"
            
            # Check if IP is in blocked ranges
            if self._is_ip_blocked(ip):
                return False, f"IP address {ip} is in blocked range"
            
            # Check port
            port = parsed.port or (443 if parsed.scheme == 'https' else 80)
            if port in self.blocked_ports:
                return False, f"Port {port} is blocked"
            
            return True, ""
            
        except Exception as e:
            logger.error(f"URL validation error: {e}")
            return False, f"URL validation failed: {e}"
    
    def _is_domain_allowed(self, hostname: str) -> bool:
        """Check if domain is in whitelist"""
        hostname = hostname.lower()
        
        # Check exact match
        if hostname in self.allowed_domains:
            return True
        
        # Check subdomain match
        for allowed_domain in self.allowed_domains:
            if hostname.endswith(f'.{allowed_domain}'):
                return True
        
        return False
    
    def _is_ip_blocked(self, ip: ipaddress.ip_address) -> bool:
        """Check if IP is in blocked ranges"""
        for network in self.blocked_networks:
            if ip in network:
                return True
        return False

# Global validator instance
url_validator = URLValidator()


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/data_processing/Processing_Service.txt
Processing_Service.txt

Processing Service
Dieser Service verarbeitet die Rohdaten, um sie für die Vektorisierung vorzubereiten.

Aufgaben:

Dateityp-Erkennung: Identifiziert den Dateityp der heruntergeladenen Inhalte (PDF, Video, Audio).

Audiotranskription: Nutzt die Open-Source-Bibliothek Whisper, um Audio- und Videoinhalte zu transkribieren.

Textextraktion: Extrahiert reinen Text aus PDFs oder anderen Dokumenten.

Spracherkennung & Übersetzung: Ermittelt die Sprache des Textes. Falls der Text nicht auf Englisch vorliegt, wird er ins Englische übersetzt, um eine einheitliche Datenbasis für die KI-Verarbeitung zu schaffen.

Textaufbereitung: Bereitet den Text für die Vektorisierung vor, indem er ihn in semantische Chunks aufteilt.

Tagging: Fügt den Text-Chunks Metadaten (z. B. Quelle, Autor, Datum) hinzu.

Queue-Befüllung: Leitet die vorbereiteten Text-Chunks an den Vectorization Service weiter, indem er eine entsprechende Aufgabe in der Task Queue erstellt.


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/data_processing/core/api_client.py
import requests
import json
import re
from typing import Dict, Any

class ApiClient:
    def __init__(self):
        self.headers = {'User-Agent': 'SDG-KI-Project/1.0 (info@example.com)'}

    def get_metadata_from_doi(self, doi: str) -> Dict[str, Any]:
        print(f"Abfrage der CrossRef-API für DOI: {doi}")
        api_url = f"https://api.crossref.org/works/{doi}"
        try:
            response = requests.get(api_url, headers=self.headers, timeout=10)
            response.raise_for_status()
            data = response.json()
            message = data.get('message', {})
            metadata = {
                'title': message.get('title', [])[0] if message.get('title') else None,
                'authors': ', '.join([author.get('given', '') + ' ' + author.get('family', '') for author in message.get('author', [])]),
                'publication_year': message.get('issued', {}).get('date-parts', [[None]]),
                'publisher': message.get('publisher'),
                'doi': message.get('DOI'),
                'keywords': ", ".join(message.get('subject', [])) if 'subject' in message else None,
                'abstract_original': message.get('abstract'),
                # ggf. weitere Felder parsen und zuweisen
            }
            return metadata
        except (requests.exceptions.RequestException, IndexError, KeyError) as e:
            print(f"Fehler bei der CrossRef-Abfrage für {doi}: {e}")
            return {}

    def get_metadata_from_isbn(self, isbn: str) -> Dict[str, Any]:
        print(f"Abfrage der Google Books API für ISBN: {isbn}")
        api_url = f"https://www.googleapis.com/books/v1/volumes?q=isbn:{isbn}"
        try:
            response = requests.get(api_url, headers=self.headers, timeout=10)
            response.raise_for_status()
            data = response.json()
            if data.get('totalItems') == 0:
                return {}
            item = data['items'][0]['volumeInfo']
            metadata = {
                'title': item.get('title'),
                'authors': ', '.join(item.get('authors', [])),
                'publisher': item.get('publisher'),
                'publication_year': item.get('publishedDate', 'Unknown').split('-'),
                'isbn': isbn,
                'abstract_original': item.get('description'),
                # ggf. weitere Felder parsen und zuweisen
            }
            return metadata
        except (requests.exceptions.RequestException, IndexError, KeyError) as e:
            print(f"Fehler bei der Google Books API-Abfrage für {isbn}: {e}")
            return {}

    def get_metadata_from_un_digital_library(self, query: str) -> Dict[str, Any]:
        print(f"Abfrage der UN Digital Library für Suchbegriff: {query}")
        api_url = f"https://digitallibrary.un.org/record?format=json&searchTerm={query}"
        try:
            response = requests.get(api_url, headers=self.headers, timeout=10)
            response.raise_for_status()
            data = response.json()
            if data and 'results' in data:
                first_result = data['results'][0]['value']
                metadata = {
                    'title': first_result.get('title'),
                    'authors': first_result.get('authors_names'),
                    'publication_year': first_result.get('publication_date'),
                    'source_url': first_result.get('url'),
                    # ggf. weitere Felder
                }
                return metadata
            return {}
        except (requests.exceptions.RequestException, IndexError, KeyError) as e:
            print(f"Fehler bei der UN Digital Library Abfrage: {e}")
            return {}

    def get_metadata_from_oecd(self, dataset_id: str) -> Dict[str, Any]:
        print(f"Abfrage der OECD API für Dataset: {dataset_id}")
        api_url = f"https://sdmx.oecd.org/public/rest/data/OECD.SDD.NAD,{dataset_id}@DF_NAAG_I?format=jsondata"
        try:
            response = requests.get(api_url, headers=self.headers, timeout=10)
            response.raise_for_status()
            data = response.json()
            metadata = {
                'title': f"OECD Dataset: {dataset_id}",
                'publisher': "OECD",
                'source_url': api_url
            }
            return metadata
        except (requests.exceptions.RequestException, IndexError, KeyError) as e:
            print(f"Fehler bei der OECD API-Abfrage: {e}")
            return {}

    def get_metadata_from_world_bank(self, query: str) -> Dict[str, Any]:
        print(f"Abfrage der Weltbank API für Suchbegriff: {query}")
        api_url = f"https://search.worldbank.org/api/v3/wds?format=json&qterm={query}"
        try:
            response = requests.get(api_url, headers=self.headers, timeout=10)
            response.raise_for_status()
            data = response.json()
            if data and 'documents' in data:
                first_result = data['documents'][0]
                metadata = {
                    'title': first_result.get('title'),
                    'authors': first_result.get('authors_names'),
                    'publication_year': first_result.get('pub_date'),
                    'source_url': first_result.get('url')
                }
                return metadata
            return {}
        except (requests.exceptions.RequestException, IndexError, KeyError) as e:
            print(f"Fehler bei der Weltbank API-Abfrage: {e}")
            return {}


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/data_processing/core/db_utils.py
import os
import time
import json
import weaviate
from sqlalchemy import create_engine, text, MetaData, Table
from sqlalchemy.exc import OperationalError, IntegrityError
from sqlalchemy.orm import sessionmaker
from typing import Dict, Any, List, Optional, Union
import logging
from datetime import datetime
import hashlib

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

DATABASE_URL = os.environ.get("DATABASE_URL")
WEAVIATE_URL = os.environ.get("WEAVIATE_URL", "http://weaviate_service:8080")

def get_database_engine():
    """Create and return SQLAlchemy engine with connection pooling"""
    engine_kwargs = {
        "pool_pre_ping": True,
        "pool_recycle": 300,
        "pool_size": 10,
        "max_overflow": 20,
        "echo": False  # Set to True for SQL debugging
    }
    
    return create_engine(DATABASE_URL, **engine_kwargs)

def get_weaviate_client():
    """Create and return Weaviate client instance"""
    try:
        client = weaviate.Client(url=WEAVIATE_URL)
        
        try:
            client.schema.get("ArticleVector")
        except weaviate.exceptions.UnexpectedStatusCodeException:
            _create_weaviate_schema(client)
        
        return client
    except Exception as e:
        logger.error(f"Failed to connect to Weaviate: {e}")
        raise ConnectionError(f"Weaviate connection failed: {e}")

def _create_weaviate_schema(client):
    class_obj = {
        "class": "ArticleVector",
        "description": "SDG article embeddings for semantic search",
        "vectorizer": "none",  # We provide our own vectors
        "properties": [
            {
                "name": "text",
                "dataType": ["text"],
                "description": "Article text content"
            },
            {
                "name": "articleId", 
                "dataType": ["int"],
                "description": "Reference to article ID in PostgreSQL"
            },
            {
                "name": "chunkId",
                "dataType": ["int"], 
                "description": "Chunk ID for long documents"
            },
            {
                "name": "sdgGoals",
                "dataType": ["int[]"],
                "description": "Related SDG goals"
            },
            {
                "name": "language",
                "dataType": ["text"],
                "description": "Content language"
            },
            {
                "name": "region",
                "dataType": ["text"],
                "description": "Geographic region"
            }
        ]
    }
    
    client.schema.create_class(class_obj)
    logger.info("Created ArticleVector schema in Weaviate")

def save_to_database(
    metadata: Dict[str, Any],
    text_content: str,
    embeddings: List[float],
    chunks_data: Optional[List[Dict]] = None
):
    
    engine = get_database_engine()
    article_id = None
    
    try:
        with engine.begin() as connection:
            
            article_id = _insert_article(connection, metadata, text_content)
            
            if chunks_data:
                _insert_article_chunks(connection, article_id, chunks_data)
            
            _insert_tag_relationships(connection, article_id, metadata)
            
            _insert_ai_topic_relationships(connection, article_id, metadata)
            
            _insert_sdg_target_relationships(connection, article_id, metadata)
            
            logger.info(f"✅ Article {article_id} saved to PostgreSQL successfully")
            
        _save_to_weaviate(article_id, text_content, embeddings, chunks_data, metadata)
        
        return article_id
        
    except Exception as e:
        logger.error(f"❌ Error saving to database: {e}")
        if article_id:
            logger.error(f"Article ID {article_id} may be partially saved")
        raise

def _insert_article(connection, metadata: Dict[str, Any], text_content: str) -> int:
    
    publication_year = _extract_publication_year(metadata)
    
    insert_query = text("""
        INSERT INTO articles (
            title, content_original, content_english, summary, keywords, sdg_id, 
            authors, publication_year, publication_date, publisher, doi, isbn, 
            region, country_code, language, context, study_type, research_methods, 
            data_sources, funding, funding_info, bias_indicators, abstract_original, 
            abstract_english, relevance_questions, source_url, availability, 
            citation_count, impact_metrics, impact_factor, policy_impact,
            word_count, content_quality_score, created_at
        ) VALUES (
            :title, :content_original, :content_english, :summary, :keywords, :sdg_id,
            :authors, :publication_year, :publication_date, :publisher, :doi, :isbn,
            :region, :country_code, :language, :context, :study_type, :research_methods,
            :data_sources, :funding, :funding_info, :bias_indicators, :abstract_original,
            :abstract_english, :relevance_questions, :source_url, :availability,
            :citation_count, :impact_metrics, :impact_factor, :policy_impact,
            :word_count, :content_quality_score, NOW()
        ) RETURNING id
    """)
    
    content_quality_score = _calculate_content_quality_score(metadata, text_content)
    
    params = {
        "title": metadata.get('title', 'Untitled')[:500],  
        "content_original": text_content,
        "content_english": metadata.get('content_english') or text_content,
        "summary": metadata.get('summary'),
        "keywords": metadata.get('keywords'),
        "sdg_id": metadata.get('sdg_id'),
        "authors": metadata.get('authors'),
        "publication_year": publication_year,
        "publication_date": _parse_publication_date(metadata.get('publication_date')),
        "publisher": metadata.get('publisher'),
        "doi": metadata.get('doi'),
        "isbn": metadata.get('isbn'),
        "region": metadata.get('region'),
        "country_code": metadata.get('country_code'),
        "language": metadata.get('language', 'en'),
        "context": metadata.get('context'),
        "study_type": metadata.get('study_type'),
        "research_methods": metadata.get('research_methods'),
        "data_sources": metadata.get('data_sources'),
        "funding": metadata.get('funding'),
        "funding_info": metadata.get('funding_info'),
        "bias_indicators": metadata.get('bias_indicators'),
        "abstract_original": metadata.get('abstract_original'),
        "abstract_english": metadata.get('abstract_english'),
        "relevance_questions": metadata.get('relevance_questions'),
        "source_url": metadata.get('source_url'),
        "availability": metadata.get('availability'),
        "citation_count": metadata.get('citation_count', 0),
        "impact_metrics": json.dumps(metadata.get('impact_metrics')) if metadata.get('impact_metrics') else None,
        "impact_factor": metadata.get('impact_factor'),
        "policy_impact": metadata.get('policy_impact'),
        "word_count": len(text_content.split()) if text_content else 0,
        "content_quality_score": content_quality_score
    }
    
    result = connection.execute(insert_query, params)
    return result.scalar_one()

def _insert_article_chunks(connection, article_id: int, chunks_data: List[Dict]):

    chunk_query = text("""
        INSERT INTO article_chunks (
            article_id, chunk_id, chunk_order, text, chunk_length, 
            sdg_section, sub_section_id, sdg_relevance_scores, 
            confidence_score, created_at
        ) VALUES (
            :article_id, :chunk_id, :chunk_order, :text, :chunk_length,
            :sdg_section, :sub_section_id, :sdg_relevance_scores,
            :confidence_score, NOW()
        )
    """)
    
    for i, chunk_data in enumerate(chunks_data):
        chunk_params = {
            "article_id": article_id,
            "chunk_id": chunk_data.get("chunk_id", i),
            "chunk_order": i,
            "text": chunk_data["text"],
            "chunk_length": len(chunk_data["text"]),
            "sdg_section": chunk_data.get("sdg_section", "general"),
            "sub_section_id": chunk_data.get("sub_section_id"),
            "sdg_relevance_scores": json.dumps(chunk_data.get("sdg_relevance_scores")) if chunk_data.get("sdg_relevance_scores") else None,
            "confidence_score": chunk_data.get("confidence_score", 0.0)
        }
        
        connection.execute(chunk_query, chunk_params)
    
    logger.info(f"Inserted {len(chunks_data)} chunks for article {article_id}")

def _insert_tag_relationships(connection, article_id: int, metadata: Dict[str, Any]):
    
    tags = metadata.get('tags', [])
    if not tags:
        return
    
    for tag_name in tags:
        if not tag_name or not tag_name.strip():
            continue
            
        tag_name = tag_name.strip()[:100]  
        
        tag_id = connection.execute(
            text("SELECT id FROM tags WHERE name = :name"),
            {"name": tag_name}
        ).scalar_one_or_none()
        
        if not tag_id:
            tag_id = connection.execute(
                text("""
                    INSERT INTO tags (name, category, usage_count) 
                    VALUES (:name, :category, 1) 
                    RETURNING id
                """),
                {"name": tag_name, "category": "general"}
            ).scalar_one()
        else:
            connection.execute(
                text("UPDATE tags SET usage_count = usage_count + 1 WHERE id = :tag_id"),
                {"tag_id": tag_id}
            )
        
        try:
            connection.execute(
                text("""
                    INSERT INTO articles_tags (article_id, tag_id) 
                    VALUES (:article_id, :tag_id)
                    ON CONFLICT (article_id, tag_id) DO NOTHING
                """),
                {"article_id": article_id, "tag_id": tag_id}
            )
        except IntegrityError:
            pass 

def _insert_ai_topic_relationships(connection, article_id: int, metadata: Dict[str, Any]):
    """Insert article-AI topic relationships"""
    ai_topics = metadata.get('ai_topics', [])
    if not ai_topics:
        return
    
    for topic_name in ai_topics:
        if not topic_name or not topic_name.strip():
            continue
            
        topic_name = topic_name.strip()[:100]
        
        topic_id = connection.execute(
            text("SELECT id FROM ai_topics WHERE name = :name"),
            {"name": topic_name}
        ).scalar_one_or_none()
        
        if not topic_id:
            topic_id = connection.execute(
                text("""
                    INSERT INTO ai_topics (name, category, created_at) 
                    VALUES (:name, :category, NOW()) 
                    RETURNING id
                """),
                {"name": topic_name, "category": "general"}
            ).scalar_one()
        
        try:
            connection.execute(
                text("""
                    INSERT INTO articles_ai_topics (article_id, ai_topic_id) 
                    VALUES (:article_id, :topic_id)
                    ON CONFLICT (article_id, ai_topic_id) DO NOTHING
                """),
                {"article_id": article_id, "topic_id": topic_id}
            )
        except IntegrityError:
            pass

def _insert_sdg_target_relationships(connection, article_id: int, metadata: Dict[str, Any]):
    """Insert article-SDG target relationships"""
    sdg_goals = metadata.get('sdg_goals', [])
    if not sdg_goals:
        return
    
    for goal_id in sdg_goals:
        sdg_ids = []
    
    # Primary SDG from sdg_id field
    if metadata.get('sdg_id'):
        sdg_ids.append(metadata['sdg_id'])
    
    # Additional SDGs from sdg_goals list
    if metadata.get('sdg_goals'):
        if isinstance(metadata['sdg_goals'], list):
            sdg_ids.extend(metadata['sdg_goals'])
        elif isinstance(metadata['sdg_goals'], int):
            sdg_ids.append(metadata['sdg_goals'])
    
    # Remove duplicates and ensure valid range
    sdg_ids = list(set([sdg for sdg in sdg_ids if isinstance(sdg, int) and 1 <= sdg <= 17]))
    
    for sdg_id in sdg_ids:
        confidence_score = metadata.get('sdg_confidence', 0.8)  # Default confidence
        
        connection.execute(
            text("""
                INSERT INTO articles_sdg_targets (article_id, sdg_id, confidence_score) 
                VALUES (:article_id, :sdg_id, :confidence_score) 
                ON CONFLICT (article_id, sdg_id) DO UPDATE SET confidence_score = :confidence_score
            """),
            {
                "article_id": article_id,
                "sdg_id": sdg_id,
                "confidence_score": confidence_score
            }
        )

def _save_to_weaviate(article_id: int, text_content: str, embeddings: List[float], 
                     chunks_data: Optional[List[Dict]] = None, metadata: Dict[str, Any] = None):
    """Save embeddings to Weaviate vector database"""
    try:
        client = get_weaviate_client()
        
        # Ensure ArticleVector schema exists
        try:
            client.schema.get("ArticleVector")
        except weaviate.exceptions.UnexpectedStatusCodeException:
            _create_weaviate_schema(client)
        
        if chunks_data and len(chunks_data) > 0:
            # Save each chunk as separate vector
            for i, chunk_data in enumerate(chunks_data):
                chunk_vector = chunk_data.get("embedding") or embeddings
                
                vector_data = {
                    "text": chunk_data["text"],
                    "articleId": article_id,
                    "chunkId": i,
                    "sdgGoals": metadata.get('sdg_goals', []) if metadata else [],
                    "language": metadata.get('language', 'en') if metadata else 'en',
                    "region": metadata.get('region', '') if metadata else ''
                }
                
                client.data_object.create(
                    data_object=vector_data,
                    class_name="ArticleVector",
                    vector=chunk_vector
                )
        else:
            # Save full document as single vector
            vector_data = {
                "text": text_content,
                "articleId": article_id,
                "chunkId": 0,
                "sdgGoals": metadata.get('sdg_goals', []) if metadata else [],
                "language": metadata.get('language', 'en') if metadata else 'en',
                "region": metadata.get('region', '') if metadata else ''
            }
            
            client.data_object.create(
                data_object=vector_data,
                class_name="ArticleVector",
                vector=embeddings
            )
        
        logger.info(f"✅ Vector data for article {article_id} saved to Weaviate")
        
    except Exception as e:
        logger.error(f"❌ Error saving to Weaviate: {e}")
        raise

def get_article_by_id(article_id: int) -> Optional[Dict[str, Any]]:
    """Retrieve article by ID with related data"""
    engine = get_database_engine()
    
    try:
        with engine.connect() as connection:
            
            article_query = text("""
                SELECT a.*, s.name as sdg_name
                FROM articles a
                LEFT JOIN sdgs s ON a.sdg_id = s.id
                WHERE a.id = :article_id
            """)
            
            result = connection.execute(article_query, {"article_id": article_id}).fetchone()
            if not result:
                return None
            
            article = dict(result._mapping)
            
            chunks_query = text("""
                SELECT * FROM article_chunks 
                WHERE article_id = :article_id 
                ORDER BY chunk_order
            """)
            chunks = connection.execute(chunks_query, {"article_id": article_id}).fetchall()
            article["chunks"] = [dict(chunk._mapping) for chunk in chunks]
            
            tags_query = text("""
                SELECT t.name FROM tags t
                JOIN articles_tags at ON t.id = at.tag_id
                WHERE at.article_id = :article_id
            """)
            tags = connection.execute(tags_query, {"article_id": article_id}).fetchall()
            article["tags"] = [tag.name for tag in tags]
            
            return article
            
    except Exception as e:
        logger.error(f"Error retrieving article {article_id}: {e}")
        return None

def search_similar_content(
    query_embedding: List[float], 
    limit: int = 10,
    sdg_filter: Optional[List[int]] = None
) -> List[Dict[str, Any]]:
    """Search for similar content using Weaviate"""
    try:
        client = get_weaviate_client()
        
        query_builder = (
            client.query
            .get("ArticleVector", ["text", "articleId", "sdgGoals", "language", "region"])
            .with_near_vector({
                "vector": query_embedding,
                "certainty": 0.7
            })
            .with_limit(limit)
            .with_additional(["certainty", "distance"])
        )
        
        if sdg_filter:
            where_filter = {
                "operator": "ContainsAny",
                "path": ["sdgGoals"],
                "valueIntArray": sdg_filter
            }
            query_builder = query_builder.with_where(where_filter)
        
        result = query_builder.do()
        
        return result.get("data", {}).get("Get", {}).get("ArticleVector", [])
        
    except Exception as e:
        logger.error(f"Error in similarity search: {e}")
        return []

def _extract_publication_year(metadata: Dict[str, Any]) -> Optional[int]:
    """Extract publication year from various metadata fields"""
    year_fields = ['publication_year', 'year', 'published_year']
    
    for field in year_fields:
        year_value = metadata.get(field)
        if year_value:
            try:
                if isinstance(year_value, list) and len(year_value) > 0:
                    year_value = year_value[0]
                if isinstance(year_value, str):
                    # Extract year from date string like "2023-01-01"
                    year_match = re.match(r'(\d{4})', year_value)
                    if year_match:
                        return int(year_match.group(1))
                elif isinstance(year_value, int):
                    if 1900 <= year_value <= 2030:  # Reasonable year range
                        return year_value
            except (ValueError, TypeError):
                continue
    
    return None

def _parse_publication_date(date_string: str) -> Optional[datetime]:
    """Parse publication date from string"""
    if not date_string:
        return None
        
    try:
        # Try different date formats
        date_formats = [
            '%Y-%m-%d',
            '%Y-%m-%d %H:%M:%S',
            '%Y-%m-%dT%H:%M:%S',
            '%Y-%m-%dT%H:%M:%SZ',
            '%Y/%m/%d',
            '%d/%m/%Y',
            '%Y'
        ]
        
        for fmt in date_formats:
            try:
                return datetime.strptime(str(date_string), fmt)
            except ValueError:
                continue
                
        # If none of the formats work, try to extract year
        year_match = re.search(r'(\d{4})', str(date_string))
        if year_match:
            year = int(year_match.group(1))
            if 1900 <= year <= 2030:
                return datetime(year, 1, 1)
                
    except Exception as e:
        logger.warning(f"Could not parse date '{date_string}': {e}")
    
    return None

def _calculate_content_quality_score(metadata: Dict[str, Any], text_content: str) -> float:

    score = 0.0
    
    # Title quality (0-0.15)
    title = metadata.get('title', '')
    if title and len(title.strip()) > 10:
        score += 0.15
    elif title and len(title.strip()) > 5:
        score += 0.08
    
    # Content length (0-0.25)
    content_length = len(text_content.strip())
    if content_length > 5000:
        score += 0.25
    elif content_length > 2000:
        score += 0.20
    elif content_length > 500:
        score += 0.10
    elif content_length > 100:
        score += 0.05
    
    # Has authors (0-0.10)
    if metadata.get('authors'):
        score += 0.10
    
    # Has publication info (0-0.10)
    if metadata.get('publication_year') or metadata.get('publisher'):
        score += 0.10
    
    # Has DOI or ISBN (0-0.10)
    if metadata.get('doi') or metadata.get('isbn'):
        score += 0.10
    
    # Has abstract (0-0.10)
    if metadata.get('abstract_original') or metadata.get('abstract_english'):
        score += 0.10
    
    # Has source URL (0-0.05)
    if metadata.get('source_url'):
        score += 0.05
    
    # Has keywords or tags (0-0.05)
    if metadata.get('keywords') or metadata.get('tags'):
        score += 0.05
    
    # Has SDG classification (0-0.10)
    if metadata.get('sdg_id') or metadata.get('sdg_goals'):
        score += 0.10
    
    return min(score, 1.0)


def batch_save_to_database(items: List[Dict[str, Any]], batch_size: int = 50):
    """
    Batch save multiple items to database for better performance
    """
    engine = get_database_engine()
    successful_saves = 0
    failed_saves = 0
    
    for i in range(0, len(items), batch_size):
        batch = items[i:i + batch_size]
        
        try:
            with engine.begin() as connection:
                for item in batch:
                    metadata = item.get('metadata', {})
                    text_content = item.get('text_content', '')
                    embeddings = item.get('embeddings', [])
                    chunks_data = item.get('chunks_data')
                    
                    try:
                        article_id = _insert_article(connection, metadata, text_content)
                        
                        if chunks_data:
                            _insert_article_chunks(connection, article_id, chunks_data)
                        
                        _insert_tag_relationships(connection, article_id, metadata)
                        _insert_ai_topic_relationships(connection, article_id, metadata)
                        _insert_sdg_target_relationships(connection, article_id, metadata)
                        
                        # Save to Weaviate in separate transaction
                        _save_to_weaviate(article_id, text_content, embeddings, chunks_data, metadata)
                        
                        successful_saves += 1
                        
                    except Exception as e:
                        logger.error(f"Error saving individual item: {e}")
                        failed_saves += 1
                        continue
                        
        except Exception as e:
            logger.error(f"Error in batch save: {e}")
            failed_saves += len(batch)
    
    logger.info(f"Batch save completed: {successful_saves} successful, {failed_saves} failed")
    return {"successful": successful_saves, "failed": failed_saves}

def cleanup_old_data(retention_days: int = 30):
    """
    Cleanup old data from database based on retention policy
    """
    engine = get_database_engine()
    
    try:
        with engine.begin() as connection:
            cutoff_date = datetime.now() - timedelta(days=retention_days)
            
            # Delete old articles (this will cascade to related tables)
            result = connection.execute(
                text("DELETE FROM articles WHERE created_at < :cutoff_date"),
                {"cutoff_date": cutoff_date}
            )
            
            deleted_count = result.rowcount
            logger.info(f"Cleaned up {deleted_count} old articles older than {retention_days} days")
            
            return deleted_count
            
    except Exception as e:
        logger.error(f"Error during cleanup: {e}")
        raise

def get_database_statistics() -> Dict[str, Any]:
    """Get database statistics"""
    engine = get_database_engine()
    
    try:
        with engine.connect() as connection:
            stats = {}
            
            article_stats = connection.execute(text("""
                SELECT 
                    COUNT(*) as total_articles,
                    COUNT(*) FILTER (WHERE has_embeddings = TRUE) as articles_with_embeddings,
                    AVG(content_quality_score) as avg_quality_score,
                    COUNT(DISTINCT language) as languages,
                    COUNT(DISTINCT region) as regions
                FROM articles
            """)).fetchone()
            
            stats.update(dict(article_stats._mapping))
            
            sdg_stats = connection.execute(text("""
                SELECT 
                    s.name,
                    COUNT(a.id) as article_count
                FROM sdgs s
                LEFT JOIN articles a ON s.id = a.sdg_id
                GROUP BY s.id, s.name
                ORDER BY article_count DESC
            """)).fetchall()
            
            stats["sdg_distribution"] = [dict(row._mapping) for row in sdg_stats]
            
            return stats
            
    except Exception as e:
        logger.error(f"Error getting statistics: {e}")
        return {}

def get_database_health() -> Dict[str, Any]:
    """
    Check database health and return status information
    """
    try:
        engine = get_database_engine()
        
        with engine.connect() as connection:
            # Test basic connectivity
            connection.execute(text("SELECT 1"))
            
            # Get table counts
            tables = ['articles', 'sdgs', 'actors', 'tags', 'ai_topics', 'article_chunks']
            table_counts = {}
            
            for table in tables:
                try:
                    result = connection.execute(text(f"SELECT COUNT(*) FROM {table}"))
                    table_counts[table] = result.scalar()
                except Exception as e:
                    table_counts[table] = f"Error: {e}"
            
            # Check Weaviate health
            try:
                weaviate_client = get_weaviate_client()
                weaviate_ready = weaviate_client.is_ready()
            except Exception as e:
                weaviate_ready = False
                logger.error(f"Weaviate health check failed: {e}")
            
            return {
                "database_status": "healthy",
                "weaviate_status": "healthy" if weaviate_ready else "unhealthy",
                "table_counts": table_counts,
                "timestamp": datetime.now().isoformat()
            }
            
    except Exception as e:
        return {
            "database_status": "unhealthy",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/data_processing/core/file_handler.py
import os
import csv
from docx import Document
import PyPDF2
from PIL import Image
import pytesseract
import fitz  # PyMuPDF
import datetime

class FileHandler:
    def __init__(self, images_dir):
        self.images_dir = images_dir
        os.makedirs(self.images_dir, exist_ok=True)

    def _extract_images_from_pdf(self, file_path: str, doc_name: str) -> list:
        """Extrahiert Bilder aus einer PDF-Datei und speichert sie."""
        image_paths = []
        try:
            pdf_document = fitz.open(file_path)
            for page_index in range(len(pdf_document)):
                page = pdf_document[page_index]
                image_list = page.get_images(full=True)
                for image_index, img in enumerate(image_list, start=1):
                    xref = img[0]
                    base_image = pdf_document.extract_image(xref)
                    image_bytes = base_image["image"]
                    file_extension = base_image["ext"]
                    image_path = os.path.join(self.images_dir, f"{doc_name}_page{page_index+1}_img{image_index}.{file_extension}")
                    with open(image_path, "wb") as f:
                        f.write(image_bytes)
                    image_paths.append({
                        "original_path": image_path,
                        "page": page_index + 1,
                        "caption": None,  # Optional bei OCR/Meta extrahieren!
                        "sdg_tags": {},
                        "ai_tags": None,
                        "image_type": file_extension
                    })
        except Exception as e:
            print(f"Fehler beim Extrahieren von Bildern aus PDF: {e}")
        return image_paths

    def _extract_images_from_docx(self, file_path: str, doc_name: str) -> list:
        """Extrahiert Bilder aus einer DOCX-Datei und speichert sie."""
        image_paths = []
        try:
            document = Document(file_path)
            for rel in document.part.rels:
                if "image" in document.part.rels[rel].target_ref:
                    image_part = document.part.rels[rel].target_part
                    image_path = os.path.join(self.images_dir, f"{doc_name}_{os.path.basename(image_part.partname)}")
                    with open(image_path, "wb") as f:
                        f.write(image_part.blob)
                    image_paths.append({
                        "original_path": image_path,
                        "page": None,
                        "caption": None,
                        "sdg_tags": {},
                        "ai_tags": None,
                        "image_type": os.path.splitext(image_path)[-1].replace('.', '')
                    })
        except Exception as e:
            print(f"Fehler beim Extrahieren von Bildern aus DOCX: {e}")
        return image_paths

    def get_text_from_file(self, file_path: str) -> str:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()

    def extract_text_from_pdf(self, file_path: str) -> tuple:
        text = ""
        metadata = {}
        try:
            with open(file_path, 'rb') as f:
                pdf_reader = PyPDF2.PdfReader(f)
                doc_info = pdf_reader.metadata
                if doc_info:
                    metadata['title'] = doc_info.get('/Title', None)
                    metadata['authors'] = doc_info.get('/Author', None)
                    metadata['creation_date'] = doc_info.get('/CreationDate', None)
                for page in pdf_reader.pages:
                    text += page.extract_text()
        except Exception as e:
            print(f"Fehler beim Extrahieren von Text oder Metadaten aus PDF: {e}")
        return text, metadata

    def extract_text_from_docx(self, file_path: str) -> str:
        text = ""
        try:
            doc = Document(file_path)
            for para in doc.paragraphs:
                text += para.text + "\n"
        except Exception as e:
            print(f"Fehler beim Extrahieren von Text aus DOCX: {e}")
        return text

    def extract_text_from_csv(self, file_path: str) -> str:
        text = ""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                csv_reader = csv.reader(f)
                for row in csv_reader:
                    text += " ".join(row) + "\n"
        except Exception as e:
            print(f"Fehler beim Extrahieren von Text aus CSV: {e}")
        return text

    def extract_text(self, file_path: str) -> str:
        if file_path.endswith('.mp3'):
            return ""
        elif file_path.endswith('.pdf'):
            text, _ = self.extract_text_from_pdf(file_path)
            return text
        elif file_path.endswith('.docx'):
            return self.extract_text_from_docx(file_path)
        elif file_path.endswith('.csv'):
            return self.extract_text_from_csv(file_path)
        else:
            return self.get_text_from_file(file_path)

    def get_metadata_from_json(self, file_path: str) -> dict:
        import json
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (FileNotFoundError, json.JSONDecodeError):
            return {}

    def cleanup_processed_data(self, directory: str, retention_days: int):
        """Löscht Dateien in einem Verzeichnis, die älter als retention_days sind."""
        print(f"Starte Bereinigung von {directory}...")
        now = datetime.datetime.now()
        for filename in os.listdir(directory):
            filepath = os.path.join(directory, filename)
            if os.path.isfile(filepath):
                file_creation_time = datetime.datetime.fromtimestamp(os.path.getctime(filepath))
                if (now - file_creation_time).days > retention_days:
                    print(f"Lösche alte Datei: {filepath}")
                    os.remove(filepath)
        print("Bereinigung abgeschlossen.")


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/data_processing/core/keywords.py
sdg_keywords_dict = {
    "SDG 1 No Poverty": [
        "no poverty", "end poverty", "armut beenden", "keine Armut",
        "aucune pauvreté", "erradicar la pobreza", "消除贫困"
    ],
    "SDG 2 Zero Hunger": [
        "zero hunger", "keinen hunger", "hunger beenden", "faim zéro",
        "hambre cero", "消除饥饿"
    ],
    "SDG 3 Good Health and Well-Being": [
        "good health", "gesundheit und wohlbefinden", "bonne santé", 
        "buena salud", "健康和福祉", "well-being", "bien-être", "bienestar"
    ],
    "SDG 4 Quality Education": [
        "quality education", "bildung von qualität", "éducation de qualité",
        "educación de calidad", "优质教育"
    ],
    "SDG 5 Gender Equality": [
        "gender equality", "gleichstellung der geschlechter", "égalité des sexes",
        "igualdad de género", "性别平等"
    ],
    "SDG 6 Clean Water and Sanitation": [
        "clean water", "sauberes wasser und sanitärversorgung", "eau propre",
        "agua limpia", "清洁饮水和卫生设施", "sanitation", "sanitäre einrichtungen", "assainissement"
    ],
    "SDG 7 Affordable and Clean Energy": [
        "clean energy", "bezahlbare und saubere energie", "énergie propre",
        "energía asequible y no contaminante", "可负担的清洁能源"
    ],
    "SDG 8 Decent Work and Economic Growth": [
        "decent work", "menschenwürdige arbeit", "travail décent",
        "trabajo decente", "体面工作", "economic growth", "wirtschaftswachstum", "croissance économique"
    ],
    "SDG 9 Industry, Innovation and Infrastructure": [
        "industry", "innovation", "industrie", "innovation", "infrastructure",
        "industrie, innovation et infrastructure", "industria, innovación e infraestructura", "产业创新和基础设施"
    ],
    "SDG 10 Reduced Inequalities": [
        "reduced inequalities", "ungleichheiten verringern", "réduction des inégalités",
        "reducción de las desigualdades", "减少不平等"
    ],
    "SDG 11 Sustainable Cities and Communities": [
        "sustainable cities", "nachhaltige städte", "villes durables",
        "ciudades sostenibles", "可持续城市", "communities", "gemeinschaften", "communautés"
    ],
    "SDG 12 Responsible Consumption and Production": [
        "responsible consumption", "verantwortungsvoller konsum", "consommation responsable",
        "consumo responsable", "负责任的消费和生产", "production", "produktion", "production"
    ],
    "SDG 13 Climate Action": [
        "climate action", "klimaschutz", "action climatique",
        "acción por el clima", "气候行动"
    ],
    "SDG 14 Life Below Water": [
        "life below water", "leben unter wasser", "vie aquatique",
        "vida submarina", "水下生物"
    ],
    "SDG 15 Life on Land": [
        "life on land", "leben an land", "vie terrestre",
        "vida de ecosistemas terrestres", "陆地生物"
    ],
    "SDG 16 Peace, Justice and Strong Institutions": [
        "peace", "justice", "friedliche gesellschaften", "paix", "justice", 
        "paz", "justicia", "和平、正义与强大机构", "strong institutions", "starke institutionen", "institutions fortes"
    ],
    "SDG 17 Partnerships for the Goals": [
        "partnerships", "partnerschaften zum erreichen der ziele", "partenariats",
        "alianzas", "可持续发展伙伴关系"
    ]
}


ai_keywords_dict = {
    "AI Governance & Policy": [
        "ai governance", "algorithmic regulation", "policy framework", "ki governance"
    ],
    "Ethics & Trustworthy AI": [
        "ai ethics", "responsible ai", "trustworthy ai", "explainable ai", "fairness", "bias mitigation"
    ],
    "Machine Learning": [
        "machine learning", "deep learning", "supervised learning", "unsupervised learning"
    ],
    "Natural Language Processing": [
        "nlp", "natural language processing", "sprachverarbeitung"
    ],
    "Computer Vision": [
        "computer vision", "object detection", "image recognition"
    ],
    "Robotics & Autonomous Systems": [
        "robotics", "autonomous vehicles", "drones"
    ],
    "Data & Big Data Analytics": [
        "big data", "data analytics", "data governance", "open data"
    ],
    "AI for Social Good": [
        "ai for good", "ai for sdgs", "humanitarian ai"
    ]
}


generic_keywords_dict = {
    "philosophie": ["philosophie", "philosophisch", "denken"],
    "gesellschaft": ["gesellschaft", "sozial", "politik", "soziologie"]
}

### /source /mnt/gigabyte1tb/SDG/sdg_root/src/data_processing/core/processing_logic.py
# /sdg_root/src/data_processing/core/processing_logic.py

import re
from typing import Dict, Any, List
from .text_chunker import SDGTextChunker
from .keywords import sdg_keywords_dict, ai_keywords_dict
from deep_translator import GoogleTranslator
import pytesseract
from PIL import Image
import logging

logger = logging.getLogger(__name__)

class ProcessingLogic:
    def __init__(self, whisper_model, sentence_model):
        self.whisper_model = whisper_model
        self.sentence_model = sentence_model
        self.text_chunker = SDGTextChunker(chunk_size=512, overlap=50)
        self.translator = GoogleTranslator()
    
    def process_text_for_ai(self, text_content: str) -> Dict[str, Any]:
        """Process text for AI analysis - single chunk version."""
        embeddings = self.sentence_model.encode(text_content).tolist()
        
        tags = self._extract_tags(text_content)
        
        extracted_info = self.extract_abstract_and_keywords(text_content)
        
        return {
            "embeddings": embeddings,
            "tags": tags,
            "keywords": extracted_info.get('keywords', []),
            "abstract": extracted_info.get('abstract', ''),
            "language": self._detect_language(text_content)
        }
    
    def process_text_for_ai_with_chunking(self, text_content: str) -> Dict[str, Any]:
        """Process text with chunking for large documents."""
        if len(text_content) <= 512:
            return self.process_text_for_ai(text_content)
        
        chunks = self.text_chunker.chunk_by_sdg_sections(text_content)
        chunks = self.text_chunker.generate_embeddings_for_chunks(chunks)
        
        processed_chunks = []
        all_tags = set()
        all_embeddings = []
        
        for chunk in chunks:
            chunk_data = self.process_text_for_ai(chunk["text"])
            chunk.update({
                "sdg_tags": chunk_data["tags"],
                "keywords": chunk_data.get("keywords", []),
                "abstract": chunk_data.get("abstract", "")
            })
            processed_chunks.append(chunk)
            all_tags.update(chunk_data["tags"])
            all_embeddings.extend(chunk_data["embeddings"])
        
        if all_embeddings:
            import numpy as np
            combined_embeddings = np.mean(
                np.array(all_embeddings).reshape(len(processed_chunks), -1), 
                axis=0
            ).tolist()
        else:
            combined_embeddings = []
        
        return {
            "chunks": processed_chunks,
            "combined_tags": list(all_tags),
            "combined_embeddings": combined_embeddings,
            "total_chunks": len(processed_chunks),
            "total_length": len(text_content)
        }
    
    def transcribe_audio(self, audio_path: str) -> str:
        """Transcribe audio file using Whisper."""
        try:
            segments, info = self.whisper_model.transcribe(audio_path)
            transcription = ""
            for segment in segments:
                transcription += segment.text + " "
            return transcription.strip()
        except Exception as e:
            logger.error(f"Error transcribing audio {audio_path}: {e}")
            return ""
    
    def _extract_tags(self, text: str) -> List[str]:
        """Extract SDG and AI tags from text."""
        text_lower = text.lower()
        tags = []
        
        # Extract SDG tags
        for sdg_name, keywords in sdg_keywords_dict.items():
            for keyword in keywords:
                if keyword.lower() in text_lower:
                    tags.append(sdg_name)
                    break
        
        # Extract AI tags
        for ai_name, keywords in ai_keywords_dict.items():
            for keyword in keywords:
                if keyword.lower() in text_lower:
                    tags.append(ai_name)
                    break
        
        return list(set(tags))
    
    def extract_abstract_and_keywords(self, text: str) -> Dict[str, Any]:
        """Extract abstract and keywords from text."""
        paragraphs = text.split('\n\n')
        abstract = paragraphs[0][:300] + "..." if len(paragraphs[0]) > 300 else paragraphs[0]

        words = re.findall(r'\b\w{4,}\b', text.lower())
        word_freq = {}
        for word in words:
            if word not in ['that', 'with', 'have', 'this', 'will', 'from', 'they', 'been', 'their']:
                word_freq[word] = word_freq.get(word, 0) + 1
        
        keywords = [word for word, freq in sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]]
        
        return {
            "abstract": abstract,
            "keywords": keywords
        }
    
    def _detect_language(self, text: str) -> str:
        """Enhanced language detection including Chinese and Hindi."""
        sample = text[:500].lower()  # Increased sample size for better detection
        
        # English indicators
        if any(word in sample for word in ['the', 'and', 'that', 'with', 'have', 'this', 'will', 'from']):
            return 'en'
        
        # German indicators
        elif any(word in sample for word in ['der', 'die', 'das', 'und', 'mit', 'eine', 'einer', 'auch']):
            return 'de'
        
        # French indicators
        elif any(word in sample for word in ['le', 'la', 'et', 'des', 'les', 'une', 'dans', 'pour']):
            return 'fr'
        
        # Spanish indicators
        elif any(word in sample for word in ['el', 'la', 'los', 'las', 'que', 'con', 'una', 'para']):
            return 'es'
        
        # Chinese indicators - check for Chinese characters
        elif any('\u4e00' <= char <= '\u9fff' for char in text[:200]):  # CJK Unified Ideographs
            return 'zh'
        
        # Hindi indicators - check for Devanagari script
        elif any('\u0900' <= char <= '\u097f' for char in text[:200]):  # Devanagari Unicode block
            return 'hi'
        
        # Additional Chinese detection (Traditional Chinese)
        elif any('\u3400' <= char <= '\u4dbf' for char in text[:200]):  # CJK Extension A
            return 'zh'
        
        # Hindi common words in Latin script (transliterated)
        elif any(word in sample for word in ['hai', 'hain', 'mein', 'aur', 'kya', 'koi', 'yeh', 'woh']):
            return 'hi'
        
        # Chinese common words in Pinyin (romanized)
        elif any(word in sample for word in ['shi', 'zai', 'you', 'wei', 'dui', 'gen', 'cong']):
            return 'zh'
        
        # Default to English if no clear detection
        else:
            return 'en'

    def detect_language_advanced(self, text: str) -> Dict[str, Any]:
        """
        Advanced language detection with confidence scores
        """
        sample = text[:1000]  
        
        language_patterns = {
            'en': {
                'common_words': ['the', 'and', 'that', 'with', 'have', 'this', 'will', 'from', 'they', 'been'],
                'weight': 0
            },
            'de': {
                'common_words': ['der', 'die', 'das', 'und', 'mit', 'eine', 'einer', 'auch', 'sich', 'aber'],
                'weight': 0
            },
            'fr': {
                'common_words': ['le', 'la', 'et', 'des', 'les', 'une', 'dans', 'pour', 'qui', 'avec'],
                'weight': 0
            },
            'es': {
                'common_words': ['el', 'la', 'los', 'las', 'que', 'con', 'una', 'para', 'por', 'como'],
                'weight': 0
            },
            'zh': {
                'common_words': [],  # Will use character detection
                'weight': 0
            },
            'hi': {
                'common_words': ['hai', 'hain', 'mein', 'aur', 'kya', 'koi', 'yeh', 'woh', 'iska', 'jab'],
                'weight': 0
            }
        }
        
        sample_lower = sample.lower()
        
        # Count word matches for Latin-script languages
        for lang_code, lang_data in language_patterns.items():
            if lang_code in ['zh', 'hi']:  # Skip for now, handle separately
                continue
            
            word_matches = sum(1 for word in lang_data['common_words'] if word in sample_lower)
            lang_data['weight'] = word_matches
        
        # Chinese character detection
        chinese_chars = sum(1 for char in sample if '\u4e00' <= char <= '\u9fff' or '\u3400' <= char <= '\u4dbf')
        if chinese_chars > 10:  # Threshold for Chinese detection
            language_patterns['zh']['weight'] = chinese_chars * 2  # Give higher weight
        
        # Hindi Devanagari script detection
        hindi_chars = sum(1 for char in sample if '\u0900' <= char <= '\u097f')
        if hindi_chars > 5:  # Threshold for Hindi detection
            language_patterns['hi']['weight'] = hindi_chars * 3  # Give higher weight
        else:
            # Check romanized Hindi words
            hindi_word_matches = sum(1 for word in language_patterns['hi']['common_words'] if word in sample_lower)
            language_patterns['hi']['weight'] = hindi_word_matches
        
        # Find language with highest weight
        detected_lang = max(language_patterns.keys(), key=lambda x: language_patterns[x]['weight'])
        confidence = language_patterns[detected_lang]['weight']
        
        # Normalize confidence score
        max_possible_score = len(sample.split()) * 0.1  # Rough estimate
        normalized_confidence = min(confidence / max(max_possible_score, 1), 1.0)
        
        return {
            'language': detected_lang,
            'confidence': normalized_confidence,
            'scores': {lang: data['weight'] for lang, data in language_patterns.items()},
            'method': 'advanced_pattern_matching'
        }
    
    def extract_abstract_and_keywords(self, text: str) -> Dict[str, Any]:
        """Extract abstract and keywords from text content"""
        
        # Detect language first
        language_info = self.detect_language_advanced(text)
        
        # Extract potential abstract (first few sentences)
        sentences = re.split(r'[.!?]', text)
        abstract_candidates = []
        
        for i, sentence in enumerate(sentences[:5]):  
            sentence = sentence.strip()
            if len(sentence) > 50 and len(sentence) < 500:  
                abstract_candidates.append(sentence)
        
        abstract = '. '.join(abstract_candidates[:3]) if abstract_candidates else text[:300]
        
        words = re.findall(r'\b[a-zA-Z]{4,}\b', text.lower())
        word_freq = {}
        
        stop_words = {
            'en': ['this', 'that', 'with', 'have', 'will', 'from', 'they', 'been', 'each', 'which'],
            'de': ['dass', 'sich', 'aber', 'auch', 'noch', 'nach', 'beim', 'dann', 'kann', 'wird'],
            'fr': ['dans', 'pour', 'avec', 'sont', 'plus', 'tout', 'cette', 'peut', 'comme', 'fait'],
            'es': ['para', 'como', 'este', 'esta', 'pero', 'todo', 'hace', 'muy', 'ahora', 'cada'],
            'zh': [],  # Chinese doesn't use space-separated words in the same way
            'hi': ['hain', 'kiya', 'jata', 'karne', 'hota', 'raha', 'gaya', 'kuch', 'baat', 'saat']
        }
        
        current_stop_words = stop_words.get(language_info['language'], stop_words['en'])
        
        for word in words:
            if word not in current_stop_words and len(word) > 3:
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # Get top keywords
        keywords = [word for word, freq in sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]]
        
        return {
            'abstract': abstract,
            'keywords': keywords,
            'language_info': language_info,
            'word_count': len(text.split()),
            'character_count': len(text)
        }

 


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/data_processing/core/processing_worker.py
import os
import time
from .file_handler import FileHandler
from .processing_logic import ProcessingLogic
from .db_utils import save_to_database

class ProcessingWorker:
    def __init__(self, raw_data_dir: str, processed_data_dir: str, database_url: str,
                 whisper_model=None, sentence_model=None):
        self.raw_data_dir = raw_data_dir
        self.processed_data_dir = processed_data_dir
        self.database_url = database_url
        self.file_handler = FileHandler(images_dir=os.path.join(processed_data_dir, "images"))
        self.processing_logic = ProcessingLogic(whisper_model, sentence_model)
        os.makedirs(self.processed_data_dir, exist_ok=True)

    def run_worker(self):
        """Laufender Worker für alle JSON-Metadatendateien in raw_data_dir"""
        print("Starte Data Processing Service...")
        while True:
            json_files = [f for f in os.listdir(self.raw_data_dir) if f.endswith('.json')]
            if not json_files:
                print("Keine neuen Metadaten-Dateien gefunden. Warte...")
                time.sleep(30)
                continue

            for json_file_name in json_files:
                try:
                    metadata_path = os.path.join(self.raw_data_dir, json_file_name)
                    metadata = self.file_handler.get_metadata_from_json(metadata_path)
                    base_name = os.path.splitext(json_file_name)[0]

                    # Erweiterung: Bilder aus Medien extrahieren (optional)
                    # image_data = []
                    # if media_path.endswith('.pdf'):
                    #    image_data = self.file_handler._extract_images_from_pdf(media_path, base_name)
                    # elif media_path.endswith('.docx'):
                    #    image_data = self.file_handler._extract_images_from_docx(media_path, base_name)
                    # ... an API/db_utils übergeben ...

                    # Suche nach zugehöriger Mediendatei
                    media_path = None
                    for ext in ['.mp3', '.txt', '.pdf', '.docx', '.csv']:
                        potential_path = os.path.join(self.raw_data_dir, f"{base_name}{ext}")
                        if os.path.exists(potential_path):
                            media_path = potential_path
                            break

                    if not media_path:
                        print(f"Keine Mediendatei für {json_file_name} gefunden. Überspringe...")
                        os.remove(metadata_path)
                        continue

                    # Inhalt extrahieren
                    if media_path.endswith('.mp3'):
                        text_content = self.processing_logic.transcribe_audio(media_path)
                    else:
                        text_content = self.file_handler.extract_text(media_path)

                    processed_data = self.processing_logic.process_text_for_ai(text_content)
                    # Setze alle neuen Felder auf das Metadatenobjekt
                    for k, v in processed_data.items():
                        metadata[k] = v

                    # Embeddings als Output für Weaviate
                    embeddings = processed_data['embeddings']

                    # Datenbank speichern (DB + Vektor)
                    save_to_database(metadata, text_content, embeddings)

                    # Optional: Backup/Output
                    backup_path = os.path.join(self.processed_data_dir, f"{base_name}_processed.json")
                    with open(backup_path, 'w', encoding='utf-8') as f:
                        import json
                        json.dump({
                            "metadata": metadata,
                            "text": text_content,
                            "embeddings": embeddings
                        }, f, indent=4)
                    os.remove(metadata_path)
                    os.remove(media_path)
                    print(f"Verarbeitung von {json_file_name} erfolgreich. Dateien gelöscht.")

                except Exception as e:
                    print(f"Fehler beim Verarbeiten von {json_file_name}: {e}")
                    time.sleep(5)

if __name__ == "__main__":
    RAW_DATA_DIR = "/app/raw_data"
    PROCESSED_DATA_DIR = "/app/processed_data"
    DATABASE_URL = os.environ.get("DATABASE_URL")
    from faster_whisper import WhisperModel
    from sentence_transformers import SentenceTransformer
    whisper_model = WhisperModel("small", device="cpu")
    sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
    worker = ProcessingWorker(
        raw_data_dir=RAW_DATA_DIR,
        processed_data_dir=PROCESSED_DATA_DIR,
        database_url=DATABASE_URL,
        whisper_model=whisper_model,
        sentence_model=sentence_model
    )
    worker.run_worker()


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/data_processing/core/sdg_interlinks.py
SDG_INTERLINKS = {
    "SDG 1 No Poverty": [
        "SDG 2 Zero Hunger",
        "SDG 3 Good Health and Well-being",
        "SDG 4 Quality Education",
        "SDG 5 Gender Equality",
        "SDG 8 Decent Work and Economic Growth",
        "SDG 10 Reduced Inequalities",
        "SDG 13 Climate Action"
    ],
    "SDG 2 Zero Hunger": [
        "SDG 1 No Poverty",
        "SDG 3 Good Health and Well-being",
        "SDG 6 Clean Water and Sanitation",
        "SDG 12 Responsible Consumption and Production",
        "SDG 13 Climate Action",
        "SDG 15 Life on Land"
    ],
    "SDG 3 Good Health and Well-being": [
        "SDG 1 No Poverty",
        "SDG 2 Zero Hunger",
        "SDG 4 Quality Education",
        "SDG 5 Gender Equality",
        "SDG 6 Clean Water and Sanitation",
        "SDG 13 Climate Action",
        "SDG 16 Peace, Justice and Strong Institutions"
    ],
    "SDG 4 Quality Education": [
        "SDG 1 No Poverty",
        "SDG 3 Good Health and Well-being",
        "SDG 5 Gender Equality",
        "SDG 8 Decent Work and Economic Growth",
        "SDG 10 Reduced Inequalities",
        "SDG 13 Climate Action"
    ],
    "SDG 5 Gender Equality": [
        "SDG 1 No Poverty",
        "SDG 3 Good Health and Well-being",
        "SDG 4 Quality Education",
        "SDG 8 Decent Work and Economic Growth",
        "SDG 10 Reduced Inequalities",
        "SDG 16 Peace, Justice and Strong Institutions"
    ],
    "SDG 6 Clean Water and Sanitation": [
        "SDG 2 Zero Hunger",
        "SDG 3 Good Health and Well-being",
        "SDG 7 Affordable and Clean Energy",
        "SDG 11 Sustainable Cities and Communities",
        "SDG 13 Climate Action",
        "SDG 14 Life Below Water",
        "SDG 15 Life on Land"
    ],
    "SDG 7 Affordable and Clean Energy": [
        "SDG 6 Clean Water and Sanitation",
        "SDG 8 Decent Work and Economic Growth",
        "SDG 9 Industry, Innovation and Infrastructure",
        "SDG 11 Sustainable Cities and Communities",
        "SDG 12 Responsible Consumption and Production",
        "SDG 13 Climate Action"
    ],
    "SDG 8 Decent Work and Economic Growth": [
        "SDG 1 No Poverty",
        "SDG 4 Quality Education",
        "SDG 5 Gender Equality",
        "SDG 7 Affordable and Clean Energy",
        "SDG 9 Industry, Innovation and Infrastructure",
        "SDG 10 Reduced Inequalities",
        "SDG 12 Responsible Consumption and Production"
    ],
    "SDG 9 Industry, Innovation and Infrastructure": [
        "SDG 7 Affordable and Clean Energy",
        "SDG 8 Decent Work and Economic Growth",
        "SDG 11 Sustainable Cities and Communities",
        "SDG 12 Responsible Consumption and Production",
        "SDG 13 Climate Action",
        "SDG 17 Partnerships for the Goals"
    ],
    "SDG 10 Reduced Inequalities": [
        "SDG 1 No Poverty",
        "SDG 4 Quality Education",
        "SDG 5 Gender Equality",
        "SDG 8 Decent Work and Economic Growth",
        "SDG 16 Peace, Justice and Strong Institutions",
        "SDG 17 Partnerships for the Goals"
    ],
    "SDG 11 Sustainable Cities and Communities": [
        "SDG 6 Clean Water and Sanitation",
        "SDG 7 Affordable and Clean Energy",
        "SDG 9 Industry, Innovation and Infrastructure",
        "SDG 12 Responsible Consumption and Production",
        "SDG 13 Climate Action",
        "SDG 15 Life on Land"
    ],
    "SDG 12 Responsible Consumption and Production": [
        "SDG 2 Zero Hunger",
        "SDG 7 Affordable and Clean Energy",
        "SDG 8 Decent Work and Economic Growth",
        "SDG 9 Industry, Innovation and Infrastructure",
        "SDG 11 Sustainable Cities and Communities",
        "SDG 13 Climate Action",
        "SDG 14 Life Below Water",
        "SDG 15 Life on Land"
    ],
    "SDG 13 Climate Action": [
        "SDG 1 No Poverty",
        "SDG 2 Zero Hunger",
        "SDG 3 Good Health and Well-being",
        "SDG 4 Quality Education",
        "SDG 6 Clean Water and Sanitation",
        "SDG 7 Affordable and Clean Energy",
        "SDG 9 Industry, Innovation and Infrastructure",
        "SDG 11 Sustainable Cities and Communities",
        "SDG 12 Responsible Consumption and Production",
        "SDG 14 Life Below Water",
        "SDG 15 Life on Land"
    ],
    "SDG 14 Life Below Water": [
        "SDG 6 Clean Water and Sanitation",
        "SDG 12 Responsible Consumption and Production",
        "SDG 13 Climate Action",
        "SDG 15 Life on Land"
    ],
    "SDG 15 Life on Land": [
        "SDG 2 Zero Hunger",
        "SDG 6 Clean Water and Sanitation",
        "SDG 11 Sustainable Cities and Communities",
        "SDG 12 Responsible Consumption and Production",
        "SDG 13 Climate Action",
        "SDG 14 Life Below Water"
    ],
    "SDG 16 Peace, Justice and Strong Institutions": [
        "SDG 3 Good Health and Well-being",
        "SDG 5 Gender Equality",
        "SDG 10 Reduced Inequalities",
        "SDG 17 Partnerships for the Goals"
    ],
    "SDG 17 Partnerships for the Goals": [
        "SDG 9 Industry, Innovation and Infrastructure",
        "SDG 10 Reduced Inequalities",
        "SDG 16 Peace, Justice and Strong Institutions"
    ]
}



### /source /mnt/gigabyte1tb/SDG/sdg_root/src/data_processing/core/secrets_manager.py
# /sdg_root/src/core/secrets_manager.py
import os
import base64
from cryptography.fernet import Fernet
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC
import keyring
import logging

logger = logging.getLogger(__name__)

class SecretsManager:
    def __init__(self):
        self.key = self._get_or_create_key()
        self.cipher = Fernet(self.key)
    
    def _get_or_create_key(self):
        """Get encryption key from secure storage or create new one"""
        try:
            # Try to get key from system keyring
            key = keyring.get_password("sdg_pipeline", "encryption_key")
            if key:
                return key.encode()
            
            # Generate new key if none exists
            password = os.environ.get('MASTER_PASSWORD', 'default_dev_password').encode()
            salt = os.environ.get('ENCRYPTION_SALT', 'default_salt').encode()
            
            kdf = PBKDF2HMAC(
                algorithm=hashes.SHA256(),
                length=32,
                salt=salt,
                iterations=100000,
            )
            key = base64.urlsafe_b64encode(kdf.derive(password))
            
            # Store in system keyring for production
            if os.environ.get('ENVIRONMENT') == 'production':
                keyring.set_password("sdg_pipeline", "encryption_key", key.decode())
            
            return key
        except Exception as e:
            logger.error(f"Error managing encryption key: {e}")
            raise
    
    def encrypt_secret(self, plaintext: str) -> str:
        """Encrypt a secret value"""
        return self.cipher.encrypt(plaintext.encode()).decode()
    
    def decrypt_secret(self, encrypted: str) -> str:
        """Decrypt a secret value"""
        return self.cipher.decrypt(encrypted.encode()).decode()
    
    def get_secret(self, key: str) -> str:
        """Get decrypted secret from environment"""
        encrypted_value = os.environ.get(f"{key}_ENCRYPTED")
        if encrypted_value:
            return self.decrypt_secret(encrypted_value)
        
        # Fallback to plaintext for development (with warning)
        plaintext_value = os.environ.get(key)
        if plaintext_value and os.environ.get('ENVIRONMENT') != 'production':
            logger.warning(f"Using plaintext secret for {key} in development")
            return plaintext_value
        
        raise ValueError(f"Secret {key} not found or not properly encrypted")

# Initialize global secrets manager
secrets_manager = SecretsManager()


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/data_processing/core/text_chunker.py
# src/data_processing/core/text_chunker.py
import re
from typing import List, Dict, Any
from sentence_transformers import SentenceTransformer

class SDGTextChunker:
    def __init__(self, chunk_size: int = 512, overlap: int = 50, model_name: str = "all-MiniLM-L6-v2"):
        self.chunk_size = chunk_size
        self.overlap = overlap
        self.model = SentenceTransformer(model_name)
    
    def smart_chunk_by_sentences(self, text: str) -> List[Dict[str, Any]]:
        """
        Intelligently chunks text by sentences while respecting size limits.
        Maintains context and semantic coherence.
        """
        sentences = self._split_into_sentences(text)
        chunks = []
        current_chunk = ""
        current_length = 0
        chunk_id = 0
        
        for sentence in sentences:
            sentence_length = len(sentence)
            
            # If adding this sentence exceeds chunk_size, save current chunk
            if current_length + sentence_length > self.chunk_size and current_chunk:
                chunks.append({
                    "chunk_id": chunk_id,
                    "text": current_chunk.strip(),
                    "length": current_length,
                    "embedding": None  # To be filled later
                })
                
                # Start new chunk with overlap
                overlap_text = self._get_overlap_text(current_chunk)
                current_chunk = overlap_text + " " + sentence
                current_length = len(current_chunk)
                chunk_id += 1
            else:
                current_chunk += " " + sentence
                current_length += sentence_length
        
        # Add final chunk
        if current_chunk.strip():
            chunks.append({
                "chunk_id": chunk_id,
                "text": current_chunk.strip(),
                "length": current_length,
                "embedding": None
            })
            
        return chunks
    
    # def chunk_by_sdg_sections(self, text: str) -> List[Dict[str, Any]]:
    #     """
    #     Chunks text by SDG-specific sections and topics.
    #     Uses your existing SDG keywords for intelligent sectioning.
    #     """
    #     from .keywords import sdg_keywords_dict
        
    #     chunks = []
    #     sections = self._identify_sdg_sections(text, sdg_keywords_dict)
        
    #     for section_name, section_text in sections.items():
    #         if len(section_text) > self.chunk_size:
    #             # Further chunk large sections
    #             sub_chunks = self.smart_chunk_by_sentences(section_text)
    #             for i, sub_chunk in enumerate(sub_chunks):
    #                 sub_chunk.update({
    #                     "sdg_section": section_name,
    #                     "sub_section_id": i
    #                 })
    #                 chunks.append(sub_chunk)
    #         else:
    #             chunks.append({
    #                 "chunk_id": len(chunks),
    #                 "text": section_text,
    #                 "length": len(section_text),
    #                 "sdg_section": section_name,
    #                 "embedding": None
    #             })
        
    #     return chunks

    def chunk_by_sdg_sections(self, text: str) -> List[Dict[str, Any]]:
        """Split text into chunks based on SDG-related sections"""
        chunks = []
        words = text.split()
        
        for i in range(0, len(words), self.chunk_size - self.overlap):
            chunk_text = ' '.join(words[i:i + self.chunk_size])
            chunks.append({
                "text": chunk_text,
                "chunk_id": i // (self.chunk_size - self.overlap),
                "start_word": i,
                "end_word": min(i + self.chunk_size, len(words))
            })
            
        return chunks
    
    def generate_embeddings_for_chunks(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Generate embeddings for each chunk using your existing sentence transformer."""
        for chunk in chunks:
            chunk["embedding"] = self.model.encode(chunk["text"]).tolist()
        return chunks
    
    def _split_into_sentences(self, text: str) -> List[str]:
        """Split text into sentences using regex."""
        sentences = re.split(r'(?<=[.!?])\s+', text)
        return [s.strip() for s in sentences if s.strip()]
    
    def _get_overlap_text(self, text: str) -> str:
        """Get last N characters for overlap."""
        return text[-self.overlap:] if len(text) > self.overlap else text
    
    def _identify_sdg_sections(self, text: str, sdg_keywords: Dict) -> Dict[str, str]:
        """Identify sections of text related to specific SDGs."""
        sections = {"general": ""}
        text_lower = text.lower()
        
        for sdg_name, keywords in sdg_keywords.items():
            section_text = ""
            for keyword in keywords:
                sentences = self._split_into_sentences(text)
                for sentence in sentences:
                    if keyword.lower() in sentence.lower():
                        section_text += sentence + " "
            
            if section_text.strip():
                sections[sdg_name] = section_text.strip()
            else:
                sections["general"] += text 
                
        return sections


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/data_processing/core/vektorizer/text_vektorizer.py
from sentence_transformers import SentenceTransformer

class TextVectorizer:
    def __init__(self, model_name="all-MiniLM-L6-v2", device="cpu"):
        self.model = SentenceTransformer(model_name, device=device)

    def embed(self, text: str) -> list:
        return self.model.encode(text).tolist()

### /source /mnt/gigabyte1tb/SDG/sdg_root/src/data_processing/main.py
import os
import time
import json
import psycopg2
from sqlalchemy import create_engine, text
from sentence_transformers import SentenceTransformer
from faster_whisper import WhisperModel
from sqlalchemy.exc import OperationalError
import PyPDF2
from docx import Document
import csv
import re
import datetime
import logging

from core.db_utils import save_to_database
from core.file_extraction import FileHandler
from core.processing_logic import ProcessingLogic
from core.api_client import ApiClient

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

RAW_DATA_DIR = "/app/raw_data"
PROCESSED_DATA_DIR = "/app/processed_data"
DATABASE_URL = os.environ.get("DATABASE_URL")
IMAGES_DIR = "/app/images"

CLEANUP_INTERVAL_DAYS = 7 
last_cleanup_timestamp = 0

os.makedirs(RAW_DATA_DIR, exist_ok=True)
os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)
os.makedirs(IMAGES_DIR, exist_ok=True)

try:
    whisper_model = WhisperModel("small", device="cpu")
    sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
    logger.info("AI models loaded successfully")
except Exception as e:
    logger.error(f"Error loading AI models: {e}")
    exit(1)

file_handler = FileHandler(IMAGES_DIR)
processing_logic = ProcessingLogic(whisper_model, sentence_model)
api_client = ApiClient()

def run_processing_worker():
    """Monitor RAW_DATA_DIR for new files and process them."""
    logger.info("Starting Data Processing Service...")
    global last_cleanup_timestamp
    
    while True:
        try:
            current_time = time.time()
            if (current_time - last_cleanup_timestamp) > (CLEANUP_INTERVAL_DAYS * 24 * 3600):
                file_handler.cleanup_processed_data(PROCESSED_DATA_DIR, CLEANUP_INTERVAL_DAYS)
                last_cleanup_timestamp = current_time
                
            json_files = [f for f in os.listdir(RAW_DATA_DIR) if f.endswith('.json')]
            if not json_files:
                logger.info("No new metadata files found. Waiting...")
                time.sleep(30)
                continue

            for json_file_name in json_files:
                try:
                    metadata_path = os.path.join(RAW_DATA_DIR, json_file_name)
                    
                    metadata = file_handler.get_metadata_from_json(metadata_path)
                    source_url = metadata.get('source_url', '')
                    logger.info(f"Processing: {source_url}")

                    api_metadata = extract_api_metadata(source_url, api_client)
                    metadata.update(api_metadata)
                    
                    base_name = os.path.splitext(json_file_name)[0]
                    media_path = find_media_file(base_name, RAW_DATA_DIR)

                    if not media_path:
                        logger.warning(f"No media file found for {json_file_name}. Skipping...")
                        os.remove(metadata_path)
                        continue

                    text_content = extract_content(media_path, file_handler, processing_logic)
                    
                    if not text_content:
                        logger.warning(f"No content extracted from {media_path}")
                        continue
                    
                    if len(text_content) > 1000:
                        processed_data = processing_logic.process_text_for_ai_with_chunking(text_content)
                        save_to_database(metadata, text_content, processed_data['combined_embeddings'], processed_data['chunks'])
                    else:
                        processed_data = processing_logic.process_text_for_ai(text_content)
                        save_to_database(metadata, text_content, processed_data['embeddings'])

                    save_backup(metadata, text_content, processed_data, base_name, PROCESSED_DATA_DIR)

                    os.remove(metadata_path)
                    os.remove(media_path)
                    logger.info(f"Processing of {json_file_name} completed successfully")

                except Exception as e:
                    logger.error(f"Error processing {json_file_name}: {e}")
                    time.sleep(5)
                    
        except KeyboardInterrupt:
            logger.info("Processing worker stopped by user")
            break
        except Exception as e:
            logger.error(f"Unexpected error in processing worker: {e}")
            time.sleep(60)

def process_text_with_chunking(self, text_content: str) -> Dict[str, Any]:
    """Process text with intelligent chunking for large documents"""
    
    if len(text_content) <= 512:
        return self._process_single_chunk(text_content)
    
    chunks = self.text_chunker.chunk_by_sdg_sections(text_content)
    chunks = self.text_chunker.generate_embeddings_for_chunks(chunks)
    
    processed_chunks = []
    all_tags = set()
    
    for chunk in chunks:
        chunk_data = self._process_single_chunk(chunk["text"])
        chunk.update({
            "sdg_tags": chunk_data["tags"],
            "keywords": chunk_data.get("keywords", []),
            "confidence_score": chunk_data.get("confidence_score", 0.0)
        })
        processed_chunks.append(chunk)
        all_tags.update(chunk_data["tags"])
    
    return {
        "chunks": processed_chunks,
        "combined_tags": list(all_tags),
        "total_chunks": len(processed_chunks),
        "embeddings": [chunk["embedding"] for chunk in processed_chunks]
    }


def extract_api_metadata(source_url: str, api_client: ApiClient) -> dict:
    """Extract metadata from various APIs based on URL patterns."""
    doi_match = re.search(r'(10\.\d{4,9}/[-._;()/:a-zA-Z0-9]+)', source_url)
    isbn_match = re.search(r'ISBN(-1[03])?:?\s+((978|979)[- ]?\d{1,5}[- ]?\d{1,7}[- ]?\d{1,6}[- ]?\d{1,1})', source_url)
    
    if doi_match:
        metadata = api_client.get_metadata_from_doi(doi_match.group(1))
        metadata['doi'] = doi_match.group(1)
        return metadata
    elif isbn_match:
        metadata = api_client.get_metadata_from_isbn(isbn_match.group(2))
        metadata['isbn'] = isbn_match.group(2)
        return metadata
    elif re.search(r'(undocs\.org|un\.org)', source_url):
        return api_client.get_metadata_from_un_digital_library("E/2023/1")
    elif re.search(r'(oecd-ilibrary\.org|stats\.oecd\.org)', source_url):
        return api_client.get_metadata_from_oecd("EDU_ENRL")
    elif re.search(r'(worldbank\.org|data\.worldbank\.org)', source_url):
        return api_client.get_metadata_from_world_bank("SP.POP.TOTL")
    
    return {}

def find_media_file(base_name: str, data_dir: str) -> str:
    """Find media file with given base name."""
    for ext in ['.mp3', '.txt', '.pdf', '.docx', '.csv']:
        potential_path = os.path.join(data_dir, f"{base_name}{ext}")
        if os.path.exists(potential_path):
            return potential_path
    return None

def extract_content(media_path: str, file_handler: FileHandler, processing_logic: ProcessingLogic) -> str:
    """Extract content from media file."""
    if media_path.endswith('.mp3'):
        return processing_logic.transcribe_audio(media_path)
    else:
        return file_handler.extract_text(media_path)

def save_backup(metadata: dict, text_content: str, processed_data: dict, base_name: str, processed_dir: str):
    """Save processed data as JSON backup."""
    backup_path = os.path.join(processed_dir, f"{base_name}_processed.json")
    backup_content = {
        "metadata": metadata,
        "text": text_content,
        "processed_data": processed_data
    }
    with open(backup_path, 'w', encoding='utf-8') as f:
        json.dump(backup_content, f, indent=4, ensure_ascii=False)
    logger.info(f"Backup saved: {backup_path}")

if __name__ == "__main__":
    run_processing_worker()

### /source /mnt/gigabyte1tb/SDG/sdg_root/src/data_processing/requirements.txt
faster-whisper
sentence-transformers
sqlalchemy
psycopg2-binary
pypdf2
python-docx
requests
Pillow
pytesseract
deep_translator
weaviate-client>=4.0.0


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/data_retrieval/main.py
from retrieval_worker import RetrievalWorker
import time

if __name__ == "__main__":
    # Konfiguration: Container-Pfade verwenden
    sources_file = "/app/quelle.txt"
    data_dir = "/app/raw_data"
    processed_file = "/app/processed_data/processed_data.json"

    worker = RetrievalWorker(
        sources_file=sources_file,
        data_dir=data_dir,
        processed_file=processed_file
    )

    print("Starting RetrievalWorker...")

    while True:
        try:
            worker.run()
            print("Warte 60 Minuten bis zum nächsten Zyklus...")
            time.sleep(3600)  # 60 Minuten
        except Exception as e:
            print(f"Fehler: {e}")
            time.sleep(60)   # 1 Minute bei Fehler



### /source /mnt/gigabyte1tb/SDG/sdg_root/src/data_retrieval/quelle.txt


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/data_retrieval/requirements.txt
weaviate-client>=4.0.0
requests
yt-dlp
sentence-transformers
faster-whisper
PyPDF2
python-docx
sqlalchemy
psycopg2-binary
pytest



### /source /mnt/gigabyte1tb/SDG/sdg_root/src/data_retrieval/retrieval_worker.py
# src/data_retrieval/retrieval_worker.py

import os
import json
import requests
import csv
import fcntl
from datetime import datetime
from yt_dlp import YoutubeDL
from yt_dlp.utils import DownloadError, ExtractorError
from urllib.parse import quote
from ..core.url_validator import url_validator
import logging
logging.basicConfig(level=logging.INFO)

class RetrievalWorker:
    def __init__(self, sources_file, data_dir, processed_file):
        self.sources_file = sources_file
        self.data_dir = data_dir
        self.processed_file = processed_file
        self.downloaded_urls_file = os.path.join(data_dir, "downloaded_urls.csv")

        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'SDG-Pipeline-Bot/1.0 (+https://sdg-pipeline.org/bot)'
        })
        
        # Security settings
        self.session.max_redirects = 3
        self.timeout = 15
        self.max_file_size = 50 * 1024 * 1024  # 50MB limit

        print("---retrievalWorker instanz gestartet")
    
    def load_downloaded_urls(self):
        """Lade bereits heruntergeladene URLs aus CSV."""
        downloaded_urls = set()
        if os.path.exists(self.downloaded_urls_file):
            try:
                with open(self.downloaded_urls_file, 'r', encoding='utf-8') as f:
                    reader = csv.DictReader(f)
                    for row in reader:
                        downloaded_urls.add(row['url'])
                logging.info(f"✅ {len(downloaded_urls)} bereits heruntergeladene URLs geladen")
            except Exception as e:
                logging.error(f"❌ Fehler beim Laden der URL-Historie: {e}")
        return downloaded_urls

    def save_downloaded_url(self, url, filename, status="success"):
        """Speichere erfolgreich heruntergeladene URL in CSV."""
        try:
            file_exists = os.path.exists(self.downloaded_urls_file)
            with open(self.downloaded_urls_file, 'a', encoding='utf-8', newline='') as f:
                fieldnames = ['url', 'filename', 'timestamp', 'status']
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                
                if not file_exists:
                    writer.writeheader()
                
                writer.writerow({
                    'url': url,
                    'filename': filename,
                    'timestamp': datetime.now().isoformat(),
                    'status': status
                })
            logging.info(f"✅ URL gespeichert in Historie: {url}")
        except Exception as e:
            logging.error(f"❌ Fehler beim Speichern der URL: {e}")

    def run(self):
        all_urls = self.load_sources()
        downloaded_urls = self.load_downloaded_urls()
        processed_data = []
        new_urls = all_urls - downloaded_urls

        if not new_urls:
            logging.info("🔄 Alle URLs bereits heruntergeladen. Keine neuen URLs zu verarbeiten.")
            return
        
        logging.info(f"📥 {len(new_urls)} neue URLs zu verarbeiten von {len(all_urls)} gesamt")

        for url in new_urls:
            try:
                if self.is_youtube(url):
                    item = self.download_youtube_content(url)
                else:
                    item = self.download_generic_content(url)
                if item:
                    processed_data.append(item)
                    self.save_downloaded_url(url, item['title'])
                    meta_filename = os.path.splitext(item['title'])[0].replace(" ", "_").replace("%20", "_") + ".json"
                    meta_path = os.path.join(self.data_dir, meta_filename)
                    with open(meta_path, "w", encoding="utf-8") as meta_f:
                        json.dump(item, meta_f, indent=2, ensure_ascii=False)
                    logging.info(f"✅ Metadaten gespeichert: {meta_path}")
                else:
                    self.save_downloaded_url(url, "failed", "failed")
            except Exception as e:
                self.handle_errors(url, e)
                self.save_downloaded_url(url, "error", "error")
        self.save_to_file(processed_data)
        self.signal_processing(processed_data)

    def load_sources(self):
        all_urls = set()
        if os.path.exists(self.sources_file):
            with open(self.sources_file, "r", encoding="utf-8") as f:
                for line in f:
                    url = line.strip()
                    if url:
                        all_urls.add(url)
        logging.info(f"📋 {len(all_urls)} URLs aus Quellenliste geladen")
        return all_urls

    def is_youtube(self, url):
        return "youtube.com" in url or "youtu.be" in url

    def download_youtube_content(self, url):
        # Analog zu deinem main.py mit yt-dlp
        ydl_opts = {"skip_download": True, "quiet": True}
        try:
            with YoutubeDL(ydl_opts) as ydl:
                info = ydl.extract_info(url, download=False)
            return {
                "url": url,
                "title": info.get("title"),
                "description": info.get("description"),
                "metadata": info,
                "source_url": url
            }
        except (DownloadError, ExtractorError) as e:
            print(f"Error: {e}")
            return None

    def download_generic_content(self, url: str):
        """Download content with comprehensive security validation"""
        logging.info(f"Validating URL: {url}")
        
        # Validate URL first
        is_valid, error_msg = url_validator.validate_url(url)
        if not is_valid:
            logging.error(f"URL validation failed for {url}: {error_msg}")
            return None
        
        try:
            os.makedirs(self.data_dir, exist_ok=True)
            
            # Make request with security headers
            response = self.session.get(
                url, 
                timeout=self.timeout,
                allow_redirects=True,
                stream=True,  # Stream for size checking
                headers={
                    'Accept': 'application/pdf,text/html,application/xml,text/plain',
                    'Accept-Encoding': 'gzip, deflate',
                    'Connection': 'close'
                }
            )
            response.raise_for_status()
            
            # Check content type
            content_type = response.headers.get('content-type', '').lower()
            allowed_types = ['application/pdf', 'text/html', 'text/plain', 'application/xml']
            if not any(allowed_type in content_type for allowed_type in allowed_types):
                logging.error(f"Disallowed content type: {content_type}")
                return None
            
            # Check file size
            content_length = response.headers.get('content-length')
            if content_length and int(content_length) > self.max_file_size:
                logging.error(f"File too large: {content_length} bytes")
                return None
            
            # Download with size limit
            filename = self._generate_safe_filename(url)
            file_path = os.path.join(self.data_dir, filename)
            
            downloaded_size = 0
            with open(file_path, "wb") as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        downloaded_size += len(chunk)
                        if downloaded_size > self.max_file_size:
                            f.close()
                            os.remove(file_path)
                            logging.error(f"File size exceeded limit during download")
                            return None
                        f.write(chunk)
            
            file_size = os.path.getsize(file_path)
            logging.info(f"✅ Download successful: {filename} ({file_size} bytes)")
            
            return {
                "url": url,
                "title": filename,
                "file_path": file_path,
                "source_url": url,
                "content_type": content_type,
                "file_size": file_size
            }
            
        except requests.exceptions.Timeout:
            logging.error(f"Timeout downloading {url}")
            return None
        except requests.exceptions.RequestException as e:
            logging.error(f"Request error downloading {url}: {e}")
            return None
        except Exception as e:
            logging.error(f"Unexpected error downloading {url}: {e}")
            return None
    
    def _generate_safe_filename(self, url: str) -> str:
        """Generate safe filename from URL"""
        import hashlib
        from urllib.parse import urlparse
        
        parsed = urlparse(url)
        path_part = parsed.path.split('/')[-1] if parsed.path else 'download'
        
        # Sanitize filename
        safe_chars = '-_.abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'
        safe_filename = ''.join(c for c in path_part if c in safe_chars)
        
        if not safe_filename or len(safe_filename) < 3:
            # Generate filename from URL hash
            url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
            safe_filename = f"download_{url_hash}"
        
        # Add extension if missing
        if '.' not in safe_filename:
            safe_filename += '.pdf'  # Default extension
        

    def save_to_file(self, data):
        with open(self.processed_file, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def signal_processing(self, processed_data):
        # Hier reicht in der Baseline das Ablegen der JSON (File-Drop)
        print(f"Signal für Processing-Service: {self.processed_file}")

    def handle_errors(self, url, error):
        print(f"Fehler bei {url}: {error}")


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/data_retrieval/test_retrieval_worker.py
# test_retrieval_worker.py

import os
import pytest
from pathlib import Path
from retrieval_worker import RetrievalWorker

tmp_path = Path("/app/raw_data")

def test_load_sources(tmp_path):
    testfile = tmp_path / "quelle.txt"
    test_link = "https://wwwcdn.imo.org/localresources/en/MediaCentre/HotTopics/Documents/IMO%20SDG%20Brochure.pdf"
    testfile.write_text(test_link)
    worker = RetrievalWorker(str(testfile), str(tmp_path), str(tmp_path / "out.json"))
    urls = worker.load_sources()
    assert test_link in urls

def test_download_generic_content(tmp_path):
    worker = RetrievalWorker("", str(tmp_path), "")
    # Die URL muss auf eine kleine, existierende Datei im Netz zeigen!
    result = worker.download_generic_content("https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf")
    assert result is not None
    assert result["file_path"].endswith(".pdf")
    assert os.path.exists(result["file_path"])


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/vectorization/__init__.py
"""
SDG Vectorization Service
Handles embedding generation, vector storage, and semantic search
"""
__version__ = "1.0.0"
__author__ = "SDG Pipeline Team"

from .embedding_models import EmbeddingManager
from .vector_db_client import VectorDBClient
from .similarity_search import SimilaritySearch

__all__ = ["EmbeddingManager", "VectorDBClient", "SimilaritySearch"]


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/vectorization/embedding_models.py
"""
Advanced embedding models for multilingual SDG content
Extracted and enhanced from your text_vektorizer.py and processing_logic.py
"""
import logging
from typing import List, Dict, Any, Optional, Union
import numpy as np
from sentence_transformers import SentenceTransformer
import openai
from transformers import AutoTokenizer, AutoModel
import torch
import torch.nn.functional as F
from abc import ABC, abstractmethod

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class BaseEmbeddingModel(ABC):
    """Abstract base class for embedding models"""
    
    @abstractmethod
    def encode(self, texts: Union[str, List[str]], **kwargs) -> np.ndarray:
        """Generate embeddings for input texts"""
        pass
    
    @abstractmethod
    def get_dimension(self) -> int:
        """Return embedding dimension"""
        pass

class SentenceTransformerModel(BaseEmbeddingModel):
    """Multilingual Sentence Transformer for SDG content"""
    
    def __init__(self, model_name: str = "paraphrase-multilingual-MiniLM-L12-v2"):
        self.model_name = model_name
        self.model = SentenceTransformer(model_name)
        self.dimension = self.model.get_sentence_embedding_dimension()
        logger.info(f"Loaded SentenceTransformer model: {model_name}")
    
    def encode(self, texts: Union[str, List[str]], 
               normalize_embeddings: bool = True,
               batch_size: int = 32,
               **kwargs) -> np.ndarray:
        """Generate embeddings with batch processing"""
        if isinstance(texts, str):
            texts = [texts]
        
        embeddings = self.model.encode(
            texts,
            normalize_embeddings=normalize_embeddings,
            batch_size=batch_size,
            show_progress_bar=len(texts) > 100,
            **kwargs
        )
        return embeddings
    
    def get_dimension(self) -> int:
        return self.dimension

class OpenAIEmbeddingModel(BaseEmbeddingModel):
    """OpenAI embeddings for high-quality SDG analysis"""
    
    def __init__(self, model_name: str = "text-embedding-ada-002", api_key: str = None):
        self.model_name = model_name
        self.dimension = 1536 if model_name == "text-embedding-ada-002" else 1536
        if api_key:
            openai.api_key = api_key
        logger.info(f"Initialized OpenAI embedding model: {model_name}")
    
    def encode(self, texts: Union[str, List[str]], **kwargs) -> np.ndarray:
        """Generate OpenAI embeddings"""
        if isinstance(texts, str):
            texts = [texts]
        
        try:
            response = openai.Embedding.create(
                input=texts,
                model=self.model_name
            )
            embeddings = np.array([item['embedding'] for item in response['data']])
            return embeddings
        except Exception as e:
            logger.error(f"OpenAI embedding error: {e}")
            raise
    
    def get_dimension(self) -> int:
        return self.dimension

class SDGSpecificModel(BaseEmbeddingModel):
    """Custom model fine-tuned for SDG content"""
    
    def __init__(self, model_path: str = "bert-base-multilingual-cased"):
        self.model_path = model_path
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModel.from_pretrained(model_path)
        self.dimension = self.model.config.hidden_size
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)
        logger.info(f"Loaded custom SDG model: {model_path}")
    
    def mean_pooling(self, model_output, attention_mask):
        """Mean pooling for sentence embeddings"""
        token_embeddings = model_output[0]
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)
    
    def encode(self, texts: Union[str, List[str]], 
               batch_size: int = 16,
               max_length: int = 512,
               **kwargs) -> np.ndarray:
        """Generate custom SDG embeddings"""
        if isinstance(texts, str):
            texts = [texts]
        
        all_embeddings = []
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            
            # Tokenize
            encoded_input = self.tokenizer(
                batch_texts,
                padding=True,
                truncation=True,
                max_length=max_length,
                return_tensors='pt'
            ).to(self.device)
            
            # Generate embeddings
            with torch.no_grad():
                model_output = self.model(**encoded_input)
                embeddings = self.mean_pooling(model_output, encoded_input['attention_mask'])
                embeddings = F.normalize(embeddings, p=2, dim=1)
                
            all_embeddings.extend(embeddings.cpu().numpy())
        
        return np.array(all_embeddings)
    
    def get_dimension(self) -> int:
        return self.dimension

class EmbeddingManager:
    """
    Manager for multiple embedding models with SDG-specific optimizations
    Enhanced version of your text_vektorizer.py functionality
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.models: Dict[str, BaseEmbeddingModel] = {}
        self.default_model = "sentence_transformer"
        
        # Initialize default models
        self._initialize_models()
    
    def _initialize_models(self):
        """Initialize available embedding models"""
        try:
            # Sentence Transformer (multilingual)
            self.models["sentence_transformer"] = SentenceTransformerModel(
                self.config.get("sentence_transformer_model", "paraphrase-multilingual-MiniLM-L12-v2")
            )
            
            # OpenAI (if API key provided)
            if self.config.get("openai_api_key"):
                self.models["openai"] = OpenAIEmbeddingModel(
                    api_key=self.config.get("openai_api_key")
                )
            
            # Custom SDG model
            self.models["sdg_custom"] = SDGSpecificModel(
                self.config.get("custom_model_path", "bert-base-multilingual-cased")
            )
            
            logger.info(f"Initialized {len(self.models)} embedding models")
            
        except Exception as e:
            logger.error(f"Error initializing models: {e}")
            raise
    
    def encode(self, 
               texts: Union[str, List[str]], 
               model_name: str = None,
               **kwargs) -> np.ndarray:
        """Generate embeddings using specified model"""
        model_name = model_name or self.default_model
        
        if model_name not in self.models:
            raise ValueError(f"Model {model_name} not available. Available: {list(self.models.keys())}")
        
        return self.models[model_name].encode(texts, **kwargs)
    
    def encode_sdg_content(self, 
                          content: str,
                          sdg_goals: List[int] = None,
                          language: str = "en",
                          **kwargs) -> Dict[str, Any]:
        """
        Enhanced SDG-specific encoding with metadata
        Integrates your SDG classification from keywords.py
        """
        # Generate base embeddings
        embeddings = self.encode(content, **kwargs)
        
        # Add SDG-specific metadata
        metadata = {
            "embedding": embeddings[0] if len(embeddings) == 1 else embeddings,
            "content_length": len(content),
            "language": language,
            "timestamp": np.datetime64('now'),
            "model_used": kwargs.get("model_name", self.default_model),
            "dimension": len(embeddings) if len(embeddings) > 0 else 0
        }
        
        # Add SDG classification if provided
        if sdg_goals:
            metadata["sdg_goals"] = sdg_goals
            metadata["primary_sdg"] = sdg_goals if sdg_goals else None
        
        return metadata
    
    def get_model_info(self, model_name: str = None) -> Dict[str, Any]:
        """Get information about embedding model"""
        model_name = model_name or self.default_model
        model = self.models.get(model_name)
        
        if not model:
            return {}
        
        return {
            "model_name": model_name,
            "dimension": model.get_dimension(),
            "type": type(model).__name__,
            "available": True
        }
    
    def batch_encode_with_progress(self, 
                                 texts: List[str], 
                                 batch_size: int = 100,
                                 model_name: str = None) -> List[np.ndarray]:
        """Batch encoding with progress tracking"""
        model_name = model_name or self.default_model
        results = []
        
        total_batches = (len(texts) + batch_size - 1) // batch_size
        logger.info(f"Processing {len(texts)} texts in {total_batches} batches")
        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            batch_embeddings = self.encode(batch, model_name=model_name)
            results.extend(batch_embeddings)
            
            batch_num = (i // batch_size) + 1
            logger.info(f"Completed batch {batch_num}/{total_batches}")
        
        return results

# Language-specific embedding configurations
LANGUAGE_MODEL_MAPPING = {
    "en": "paraphrase-multilingual-MiniLM-L12-v2",
    "de": "paraphrase-multilingual-MiniLM-L12-v2", 
    "fr": "paraphrase-multilingual-MiniLM-L12-v2",
    "es": "paraphrase-multilingual-MiniLM-L12-v2",
    "zh": "paraphrase-multilingual-MiniLM-L12-v2",
    "hi": "paraphrase-multilingual-MiniLM-L12-v2"
}

def get_optimal_model_for_language(language: str) -> str:
    """Get optimal embedding model for specific language"""
    return LANGUAGE_MODEL_MAPPING.get(language, "paraphrase-multilingual-MiniLM-L12-v2")


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/vectorization/main.py
# src/vectorization/main.py
"""
SDG Vectorization Service - FastAPI Application
Microservice for embedding generation, vector storage, and semantic search
"""
import logging
from typing import List, Dict, Any, Optional
import asyncio
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException, BackgroundTasks, Depends, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
import numpy as np
import uvicorn

from .embedding_models import EmbeddingManager
from .vector_db_client import VectorDBClient, get_vector_client  
from .similarity_search import SimilaritySearch, SDGRecommendationEngine

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Pydantic models for API
class EmbeddingRequest(BaseModel):
    texts: List[str] = Field(..., description="List of texts to embed")
    model_name: Optional[str] = Field("sentence_transformer", description="Embedding model to use")
    normalize: bool = Field(True, description="Normalize embeddings")

class DocumentRequest(BaseModel):
    title: str = Field(..., description="Document title")
    content: str = Field(..., description="Document content")  
    summary: Optional[str] = Field(None, description="Document summary")
    sdg_goals: List[int] = Field(..., description="Related SDG goals")
    region: str = Field(..., description="Geographic region")
    language: str = Field("en", description="Content language")
    source_url: Optional[str] = Field(None, description="Source URL")
    metadata: Optional[Dict[str, Any]] = Field(None, description="Additional metadata")

class SearchRequest(BaseModel):
    query: str = Field(..., description="Search query")
    search_type: str = Field("general", description="Search type: general, high_quality, recent, comprehensive")
    language: Optional[str] = Field("en", description="Content language")
    region: Optional[str] = Field(None, description="Geographic region filter")
    sdg_goals: Optional[List[int]] = Field(None, description="SDG goals filter")
    limit: int = Field(10, description="Maximum results", ge=1, le=100)

class RecommendationRequest(BaseModel):
    user_interests: List[int] = Field(..., description="User's SDG interests (1-17)")
    region: Optional[str] = Field(None, description="User's region")
    language: str = Field("en", description="Preferred language")
    limit: int = Field(10, description="Number of recommendations", ge=1, le=50)

# Global service instances
embedding_manager: Optional[EmbeddingManager] = None
vector_client: Optional[VectorDBClient] = None
similarity_search: Optional[SimilaritySearch] = None
recommendation_engine: Optional[SDGRecommendationEngine] = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan management"""
    # Startup
    global embedding_manager, vector_client, similarity_search, recommendation_engine
    
    try:
        # Load configuration (in production, use proper config management)
        config = {
            "weaviate": {
                "url": "http://localhost:8080", 
                "embedded": False,
                "max_connections": 10
            },
            "embeddings": {
                "sentence_transformer_model": "paraphrase-multilingual-MiniLM-L12-v2",
                "openai_api_key": None  # Set from environment
            }
        }
        
        # Initialize services
        logger.info("Initializing Vectorization Service...")
        
        embedding_manager = EmbeddingManager(config.get("embeddings", {}))
        vector_client = VectorDBClient(config.get("weaviate", {}))
        similarity_search = SimilaritySearch(vector_client, embedding_manager)
        recommendation_engine = SDGRecommendationEngine(similarity_search)
        
        logger.info("Vectorization Service initialized successfully")
        yield
        
    except Exception as e:
        logger.error(f"Error during startup: {e}")
        raise
    finally:
        # Shutdown
        if vector_client:
            vector_client.close()
        logger.info("Vectorization Service shutdown complete")

# Initialize FastAPI app
app = FastAPI(
    title="SDG Vectorization Service",
    description="Microservice for SDG content embedding generation, vector storage, and semantic search",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc",
    lifespan=lifespan
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Configure appropriately for production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Dependency injection
async def get_embedding_manager() -> EmbeddingManager:
    if embedding_manager is None:
        raise HTTPException(status_code=503, detail="Embedding manager not initialized")
    return embedding_manager

async def get_vector_client_dep() -> VectorDBClient:
    if vector_client is None:
        raise HTTPException(status_code=503, detail="Vector client not initialized") 
    return vector_client

async def get_similarity_search() -> SimilaritySearch:
    if similarity_search is None:
        raise HTTPException(status_code=503, detail="Similarity search not initialized")
    return similarity_search

async def get_recommendation_engine() -> SDGRecommendationEngine:
    if recommendation_engine is None:
        raise HTTPException(status_code=503, detail="Recommendation engine not initialized")
    return recommendation_engine

# API Endpoints

@app.get("/health", tags=["Health"])
async def health_check():
    """Service health check endpoint"""
    try:
        # Check all service components
        embedding_health = embedding_manager is not None
        vector_health = vector_client.health_check() if vector_client else {"status": "unavailable"}
        search_health = similarity_search.health_check() if similarity_search else {"status": "unavailable"}
        
        overall_status = "healthy" if (
            embedding_health and 
            vector_health.get("status") == "healthy" and
            search_health.get("status") == "healthy"
        ) else "unhealthy"
        
        return {
            "status": overall_status,
            "service": "SDG Vectorization Service",
            "version": "1.0.0",
            "components": {
                "embedding_manager": "healthy" if embedding_health else "unhealthy",
                "vector_client": vector_health,
                "similarity_search": search_health
            }
        }
    except Exception as e:
        logger.error(f"Health check error: {e}")
        return JSONResponse(
            status_code=503,
            content={"status": "error", "error": str(e)}
        )

@app.post("/embeddings", tags=["Embeddings"])
async def generate_embeddings(
    request: EmbeddingRequest,
    embedding_mgr: EmbeddingManager = Depends(get_embedding_manager)
):
    """Generate embeddings for input texts"""
    try:
        embeddings = embedding_mgr.encode(
            texts=request.texts,
            model_name=request.model_name
        )
        
        # Convert numpy arrays to lists for JSON serialization
        embeddings_list = embeddings.tolist()
        
        return {
            "embeddings": embeddings_list,
            "dimension": len(embeddings_list[0]) if embeddings_list else 0,
            "model_used": request.model_name,
            "text_count": len(request.texts)
        }
        
    except Exception as e:
        logger.error(f"Error generating embeddings: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/documents", tags=["Documents"])
async def store_document(
    request: DocumentRequest,
    background_tasks: BackgroundTasks,
    embedding_mgr: EmbeddingManager = Depends(get_embedding_manager),
    vector_db: VectorDBClient = Depends(get_vector_client_dep)
):
    """Store document with embeddings in vector database"""
    try:
        # Generate embeddings for title and content
        content_for_embedding = f"{request.title}\n\n{request.summary or request.content[:1000]}"
        embeddings = embedding_mgr.encode_sdg_content(
            content=content_for_embedding,
            sdg_goals=request.sdg_goals,
            language=request.language
        )
        
        # Prepare document for storage
        document = {
            "title": request.title,
            "content": request.content,
            "summary": request.summary,
            "sdg_goals": request.sdg_goals,
            "region": request.region,
            "language": request.language,
            "source_url": request.source_url,
            "confidence_score": 0.8,  # Default confidence
            "publication_date": "2024-01-01T00:00:00Z",  # Should come from metadata
            "vector": embeddings["embedding"]
        }
        
        # Add any additional metadata
        if request.metadata:
            document.update(request.metadata)
        
        # Store document (async in background)
        background_tasks.add_task(
            vector_db.insert_embeddings,
            documents=[document],
            class_name="SDGArticle"
        )
        
        return {
            "status": "accepted",
            "message": "Document queued for storage",
            "document_id": f"pending_{hash(request.title)}",  # Generate proper ID in production
            "embedding_dimension": embeddings["dimension"]
        }
        
    except Exception as e:
        logger.error(f"Error storing document: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/search", tags=["Search"])
async def semantic_search(
    request: SearchRequest,
    search_service: SimilaritySearch = Depends(get_similarity_search)
):
    """Perform semantic search across SDG content"""
    try:
        results = await search_service.semantic_search(
            query=request.query,
            search_type=request.search_type,
            language=request.language,
            region=request.region,
            sdg_goals=request.sdg_goals,
            limit=request.limit
        )
        
        return results
        
    except Exception as e:
        logger.error(f"Error in semantic search: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/search/sdg/{sdg_goal}", tags=["Search"])
async def search_by_sdg_goal(
    sdg_goal: int,
    limit: int = 20,
    region: Optional[str] = None,
    vector_db: VectorDBClient = Depends(get_vector_client_dep)
):
    """Search for content related to specific SDG goal"""
    try:
        if not (1 <= sdg_goal <= 17):
            raise HTTPException(status_code=400, detail="SDG goal must be between 1 and 17")
        
        results = vector_db.search_by_sdg_goals(
            sdg_goals=[sdg_goal],
            limit=limit
        )
        
        # Filter by region if specified
        if region:
            results = [r for r in results if r.get("region") == region]
        
        return {
            "sdg_goal": sdg_goal,
            "region": region,
            "total_results": len(results),
            "results": results
        }
        
    except Exception as e:
        logger.error(f"Error searching by SDG goal: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/search/region/{region}", tags=["Search"])
async def search_by_region(
    region: str,
    limit: int = 30,
    vector_db: VectorDBClient = Depends(get_vector_client_dep)
):
    """Search for region-specific SDG content"""
    try:
        results = vector_db.search_by_region(
            region=region,
            limit=limit
        )
        
        return {
            "region": region,
            "total_results": len(results),
            "results": results
        }
        
    except Exception as e:
        logger.error(f"Error searching by region: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/recommendations", tags=["Recommendations"])
async def get_recommendations(
    request: RecommendationRequest,
    rec_engine: SDGRecommendationEngine = Depends(get_recommendation_engine)
):
    """Get personalized SDG content recommendations"""
    try:
        # Validate SDG goals
        invalid_goals = [g for g in request.user_interests if not (1 <= g <= 17)]
        if invalid_goals:
            raise HTTPException(
                status_code=400, 
                detail=f"Invalid SDG goals: {invalid_goals}. Must be between 1 and 17."
            )
        
        recommendations = await rec_engine.recommend_content(
            user_interests=request.user_interests,
            region=request.region,
            language=request.language,
            limit=request.limit
        )
        
        return recommendations
        
    except Exception as e:
        logger.error(f"Error generating recommendations: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/statistics", tags=["Analytics"])
async def get_statistics(
    vector_db: VectorDBClient = Depends(get_vector_client_dep)
):
    """Get vector database statistics"""
    try:
        stats = vector_db.get_statistics()
        return {
            "database_statistics": stats,
            "total_documents": sum(stats.values()),
            "available_classes": list(stats.keys())
        }
    except Exception as e:
        logger.error(f"Error getting statistics: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/models", tags=["Configuration"])
async def get_available_models(
    embedding_mgr: EmbeddingManager = Depends(get_embedding_manager)
):
    """Get available embedding models"""
    try:
        models_info = {}
        for model_name in embedding_mgr.models.keys():
            models_info[model_name] = embedding_mgr.get_model_info(model_name)
        
        return {
            "available_models": models_info,
            "default_model": embedding_mgr.default_model
        }
    except Exception as e:
        logger.error(f"Error getting model information: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# Error handlers
@app.exception_handler(ValueError)
async def value_error_handler(request, exc):
    return JSONResponse(
        status_code=400,
        content={"detail": str(exc), "type": "ValueError"}
    )

@app.exception_handler(ConnectionError)
async def connection_error_handler(request, exc):
    return JSONResponse(
        status_code=503,
        content={"detail": "Service temporarily unavailable", "type": "ConnectionError"}
    )

if __name__ == "__main__":
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8003,
        reload=True,
        log_level="info"
    )


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/vectorization/requirements.txt
fastapi==0.104.1
uvicorn[standard]==0.24.0
sentence-transformers==2.2.2
transformers==4.36.0
torch>=1.9.0
numpy==1.24.3
scikit-learn==1.3.0
weaviate-client==3.25.3
openai==0.28.1
networkx==3.2
asyncio-mqtt==0.13.0
python-multipart==0.0.6
pydantic==2.5.0
python-jose[cryptography]==3.3.0


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/vectorization/similarity_search.py
"""
Advanced Similarity Search for SDG Content
Semantic search, SDG interlinkage analysis, and content recommendations
"""
import logging
from typing import List, Dict, Any, Optional, Tuple, Union
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
import networkx as nx
from collections import defaultdict
import asyncio

from .vector_db_client import VectorDBClient
from .embedding_models import EmbeddingManager

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SimilaritySearch:
    """
    Advanced similarity search with SDG-specific intelligence
    Integrates with your sdg_interlinks.py for cross-SDG analysis
    """
    
    def __init__(self, 
                 vector_client: VectorDBClient,
                 embedding_manager: EmbeddingManager,
                 config: Dict[str, Any] = None):
        self.vector_client = vector_client
        self.embedding_manager = embedding_manager
        self.config = config or {}
        
        # SDG interlinkage matrix (from your sdg_interlinks.py)
        self.sdg_interlinkages = self._load_sdg_interlinkages()
        
        # Search configuration
        self.default_similarity_threshold = config.get("similarity_threshold", 0.7)
        self.max_results = config.get("max_results", 100)
        
    def _load_sdg_interlinkages(self) -> Dict[int, List[int]]:
        """
        Load SDG interlinkage data
        This should integrate with your existing sdg_interlinks.py
        """
        # Simplified interlinkage mapping - replace with your full data
        interlinkages = {
            1: [2, 3, 4, 6, 8, 10],  # No Poverty links
            2: [1, 3, 4, 5, 6, 8, 12], # Zero Hunger links
            3: [1, 2, 4, 5, 6, 8, 10, 11], # Good Health links
            4: [1, 2, 3, 5, 8, 10, 16], # Quality Education links
            5: [1, 2, 3, 4, 8, 10, 16], # Gender Equality links
            6: [1, 2, 3, 7, 11, 12, 13, 14, 15], # Clean Water links
            7: [1, 8, 9, 11, 12, 13], # Affordable Energy links
            8: [1, 2, 3, 4, 5, 7, 9, 10, 12, 16], # Decent Work links
            9: [7, 8, 11, 12, 17], # Industry Innovation links
            10: [1, 3, 4, 5, 8, 11, 16], # Reduced Inequalities links
            11: [1, 3, 6, 7, 9, 10, 12, 13, 15], # Sustainable Cities links
            12: [2, 6, 7, 8, 9, 11, 13, 14, 15], # Responsible Consumption links
            13: [6, 7, 11, 12, 14, 15], # Climate Action links
            14: [6, 12, 13, 15], # Life Below Water links
            15: [6, 11, 12, 13, 14], # Life on Land links
            16: [1, 4, 5, 8, 10, 17], # Peace Justice links
            17: [9, 16] # Partnerships links
        }
        return interlinkages
    
    async def semantic_search(self, 
                            query: str,
                            search_type: str = "general",
                            language: str = "en",
                            region: str = None,
                            sdg_goals: List[int] = None,
                            limit: int = 10) -> Dict[str, Any]:
        """
        Advanced semantic search with multiple search strategies
        """
        try:
            # Generate query embedding
            query_embedding = self.embedding_manager.encode(query)
            if len(query_embedding.shape) > 1:
                query_embedding = query_embedding[0]
            
            # Base search parameters
            search_params = {
                "query_vector": query_embedding,
                "limit": limit * 2,  # Get more results for filtering
                "additional_fields": [
                    "title", "summary", "content", "sdg_goals", 
                    "region", "language", "confidence_score", "publication_date"
                ]
            }
            
            # Apply filters based on search type
            where_filter = self._build_search_filter(
                search_type=search_type,
                language=language,
                region=region,
                sdg_goals=sdg_goals
            )
            
            if where_filter:
                search_params["where_filter"] = where_filter
            
            # Execute vector search
            raw_results = self.vector_client.search_similar(**search_params)
            
            # Post-process and rank results
            processed_results = await self._process_search_results(
                raw_results, query, search_type, limit
            )
            
            # Add SDG interlinkage suggestions
            interlinkage_suggestions = self._get_interlinkage_suggestions(processed_results)
            
            return {
                "query": query,
                "search_type": search_type,
                "total_results": len(processed_results),
                "results": processed_results[:limit],
                "interlinkage_suggestions": interlinkage_suggestions,
                "search_metadata": {
                    "language": language,
                    "region": region,
                    "sdg_goals": sdg_goals,
                    "similarity_threshold": self.default_similarity_threshold
                }
            }
            
        except Exception as e:
            logger.error(f"Error in semantic search: {e}")
            raise
    
    def _build_search_filter(self,
                           search_type: str,
                           language: str = None,
                           region: str = None,
                           sdg_goals: List[int] = None) -> Optional[Dict[str, Any]]:
        """Build Weaviate where filter based on search parameters"""
        filters = []
        
        # Language filter
        if language and language != "all":
            filters.append({
                "operator": "Equal",
                "path": ["language"],
                "valueText": language
            })
        
        # Region filter
        if region and region != "all":
            filters.append({
                "operator": "Equal", 
                "path": ["region"],
                "valueText": region
            })
        
        # SDG goals filter
        if sdg_goals:
            filters.append({
                "operator": "ContainsAny",
                "path": ["sdg_goals"],
                "valueIntArray": sdg_goals
            })
        
        # Search type specific filters
        if search_type == "high_quality":
            filters.append({
                "operator": "GreaterThan",
                "path": ["confidence_score"],
                "valueNumber": 0.8
            })
        elif search_type == "recent":
            # Filter for recent publications (last 2 years)
            filters.append({
                "operator": "GreaterThan",
                "path": ["publication_date"],
                "valueDate": "2022-01-01T00:00:00Z"
            })
        
        # Combine filters
        if not filters:
            return None
        elif len(filters) == 1:
            return filters[0]
        else:
            return {
                "operator": "And",
                "operands": filters
            }
    
    async def _process_search_results(self,
                                    raw_results: List[Dict[str, Any]],
                                    query: str,
                                    search_type: str,
                                    limit: int) -> List[Dict[str, Any]]:
        """Post-process and enhance search results"""
        processed_results = []
        
        for result in raw_results:
            # Extract additional metadata
            additional = result.get("_additional", {})
            certainty = additional.get("certainty", 0.0)
            distance = additional.get("distance", 1.0)
            
            # Skip results below similarity threshold
            if certainty < self.default_similarity_threshold:
                continue
            
            # Enhance result with computed fields
            enhanced_result = {
                **result,
                "similarity_score": certainty,
                "distance": distance,
                "relevance_score": self._compute_relevance_score(result, query, search_type),
                "sdg_coverage": self._analyze_sdg_coverage(result.get("sdg_goals", [])),
                "content_quality": self._assess_content_quality(result)
            }
            
            processed_results.append(enhanced_result)
        
        # Sort by relevance score
        processed_results.sort(key=lambda x: x["relevance_score"], reverse=True)
        
        return processed_results[:limit]
    
    def _compute_relevance_score(self, 
                               result: Dict[str, Any],
                               query: str,
                               search_type: str) -> float:
        """Compute comprehensive relevance score"""
        base_score = result.get("_additional", {}).get("certainty", 0.0)
        
        # Factor in confidence score
        confidence_score = result.get("confidence_score", 0.5)
        confidence_weight = 0.2
        
        # Factor in SDG goal relevance
        sdg_goals = result.get("sdg_goals", [])
        sdg_relevance = len(sdg_goals) / 17.0 if sdg_goals else 0.0  # Normalize by max SDGs
        sdg_weight = 0.2
        
        # Factor in content length (prefer substantial content)
        content_length = len(result.get("content", ""))
        length_score = min(content_length / 10000.0, 1.0)  # Normalize to 10k chars
        length_weight = 0.1
        
        # Search type specific adjustments
        type_bonus = 0.0
        if search_type == "high_quality" and confidence_score > 0.8:
            type_bonus = 0.1
        elif search_type == "comprehensive" and len(sdg_goals) > 2:
            type_bonus = 0.1
        
        relevance_score = (
            base_score * 0.5 +
            confidence_score * confidence_weight +
            sdg_relevance * sdg_weight +
            length_score * length_weight +
            type_bonus
        )
        
        return min(relevance_score, 1.0)
    
    def _analyze_sdg_coverage(self, sdg_goals: List[int]) -> Dict[str, Any]:
        """Analyze SDG goal coverage and interlinkages"""
        if not sdg_goals:
            return {"coverage": 0.0, "interlinkages": [], "primary_goal": None}
        
        primary_goal = sdg_goals[0] if sdg_goals else None
        coverage = len(sdg_goals) / 17.0  # Percentage of SDGs covered
        
        # Find interlinkages
        interlinkages = set()
        for goal in sdg_goals:
            if goal in self.sdg_interlinkages:
                interlinkages.update(self.sdg_interlinkages[goal])
        
        # Remove already covered goals
        interlinkages = list(interlinkages - set(sdg_goals))
        
        return {
            "coverage": coverage,
            "goals_covered": len(sdg_goals),
            "primary_goal": primary_goal,
            "interlinkages": interlinkages[:5],  # Top 5 related goals
            "interconnectivity": len(interlinkages) / 17.0
        }
    
    def _assess_content_quality(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """Assess content quality metrics"""
        content = result.get("content", "")
        title = result.get("title", "")
        summary = result.get("summary", "")
        
        # Basic quality metrics
        content_length = len(content)
        has_title = len(title.strip()) > 0
        has_summary = len(summary.strip()) > 0
        has_url = bool(result.get("source_url"))
        
        # Text quality heuristics
        sentence_count = content.count('.') + content.count('!') + content.count('?')
        avg_sentence_length = content_length / max(sentence_count, 1)
        
        quality_score = 0.0
        if has_title: quality_score += 0.2
        if has_summary: quality_score += 0.2  
        if has_url: quality_score += 0.1
        if content_length > 500: quality_score += 0.2
        if 50 <= avg_sentence_length <= 200: quality_score += 0.3  # Optimal sentence length
        
        return {
            "quality_score": quality_score,
            "content_length": content_length,
            "has_metadata": has_title and has_summary,
            "avg_sentence_length": avg_sentence_length,
            "estimated_reading_time": max(1, content_length // 250)  # Words per minute
        }
    
    def _get_interlinkage_suggestions(self, 
                                    results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Generate SDG interlinkage suggestions based on search results"""
        sdg_frequency = defaultdict(int)
        
        # Count SDG goal frequencies in results
        for result in results:
            for goal in result.get("sdg_goals", []):
                sdg_frequency[goal] += 1
        
        # Generate suggestions for top SDGs
        suggestions = []
        for goal, frequency in sorted(sdg_frequency.items(), key=lambda x: x[1], reverse=True)[:3]:
            interlinkages = self.sdg_interlinkages.get(goal, [])
            if interlinkages:
                suggestions.append({
                    "primary_sdg": goal,
                    "frequency": frequency,
                    "related_sdgs": interlinkages[:5],
                    "suggestion": f"Explore connections between SDG {goal} and related goals: {', '.join(map(str, interlinkages[:3]))}"
                })
        
        return suggestions
    
    async def find_similar_documents(self,
                                   document_id: str,
                                   similarity_threshold: float = 0.7,
                                   limit: int = 10) -> List[Dict[str, Any]]:
        """Find documents similar to a given document"""
        try:
            # Get source document embedding
            # This would require storing document embeddings with IDs
            # For now, implement a placeholder
            
            # In production, you'd:
            # 1. Retrieve document by ID
            # 2. Get its embedding
            # 3. Perform similarity search
            # 4. Return similar documents
            
            logger.warning("find_similar_documents not fully implemented - requires document ID storage")
            return []
            
        except Exception as e:
            logger.error(f"Error finding similar documents: {e}")
            raise
    
    async def cluster_search_results(self,
                                   results: List[Dict[str, Any]],
                                   num_clusters: int = 5) -> Dict[str, Any]:
        """Cluster search results for better organization"""
        try:
            if len(results) < num_clusters:
                return {"clusters": [{"documents": results, "theme": "All Results"}]}
            
            # Extract embeddings (would need to be stored with results)
            # For now, use SDG goals as clustering feature
            
            features = []
            for result in results:
                # Create feature vector based on SDG goals
                feature = [0] * 17
                for goal in result.get("sdg_goals", []):
                    if 1 <= goal <= 17:
                        feature[goal-1] = 1
                features.append(feature)
            
            # Perform clustering
            kmeans = KMeans(n_clusters=min(num_clusters, len(results)), random_state=42)
            cluster_labels = kmeans.fit_predict(features)
            
            # Organize results by cluster
            clusters = defaultdict(list)
            for i, label in enumerate(cluster_labels):
                clusters[label].append(results[i])
            
            # Generate cluster themes
            cluster_results = []
            for cluster_id, docs in clusters.items():
                # Determine theme based on most common SDGs
                sdg_counts = defaultdict(int)
                for doc in docs:
                    for goal in doc.get("sdg_goals", []):
                        sdg_counts[goal] += 1
                
                top_sdgs = sorted(sdg_counts.items(), key=lambda x: x[1], reverse=True)[:2]
                theme = f"SDG {top_sdgs}" if top_sdgs else "Mixed Content"
                if len(top_sdgs) > 1:
                    theme += f" & {top_sdgs}"
                
                cluster_results.append({
                    "cluster_id": cluster_id,
                    "theme": theme,
                    "document_count": len(docs),
                    "documents": docs,
                    "dominant_sdgs": [sdg for sdg, _ in top_sdgs]
                })
            
            return {
                "total_clusters": len(cluster_results),
                "clusters": cluster_results,
                "clustering_method": "SDG-based K-means"
            }
            
        except Exception as e:
            logger.error(f"Error clustering search results: {e}")
            return {"clusters": [{"documents": results, "theme": "Unclustered"}]}
    
    def health_check(self) -> Dict[str, Any]:
        """Health check for similarity search service"""
        try:
            # Test vector client connection
            vector_health = self.vector_client.health_check()
            
            # Test embedding manager
            test_embedding = self.embedding_manager.encode("test query")
            embedding_healthy = len(test_embedding) > 0
            
            return {
                "status": "healthy" if vector_health.get("status") == "healthy" and embedding_healthy else "unhealthy",
                "vector_db": vector_health,
                "embedding_manager": {
                    "status": "healthy" if embedding_healthy else "unhealthy",
                    "available_models": list(self.embedding_manager.models.keys())
                },
                "sdg_interlinkages_loaded": len(self.sdg_interlinkages) == 17
            }
            
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }

class SDGRecommendationEngine:
    """
    SDG-specific content recommendation engine
    """
    
    def __init__(self, similarity_search: SimilaritySearch):
        self.similarity_search = similarity_search
        self.sdg_weights = self._initialize_sdg_weights()
    
    def _initialize_sdg_weights(self) -> Dict[int, float]:
        """Initialize SDG importance weights based on global priorities"""
        # Weights based on UN priority areas and interconnectedness
        return {
            1: 1.0,   # No Poverty - foundational
            2: 0.95,  # Zero Hunger - critical
            3: 0.9,   # Good Health - essential
            4: 0.85,  # Quality Education - long-term impact
            5: 0.8,   # Gender Equality - cross-cutting
            6: 0.9,   # Clean Water - fundamental
            7: 0.85,  # Affordable Energy - enabling
            8: 0.8,   # Decent Work - economic
            9: 0.75,  # Industry Innovation - development
            10: 0.8,  # Reduced Inequalities - social
            11: 0.7,  # Sustainable Cities - urban
            12: 0.75, # Responsible Consumption - environmental
            13: 1.0,  # Climate Action - urgent priority
            14: 0.8,  # Life Below Water - environmental
            15: 0.8,  # Life on Land - environmental
            16: 0.85, # Peace Justice - governance
            17: 0.9   # Partnerships - enabling
        }
    
    async def recommend_content(self,
                              user_interests: List[int],
                              region: str = None,
                              language: str = "en",
                              limit: int = 10) -> Dict[str, Any]:
        """Generate personalized SDG content recommendations"""
        try:
            recommendations = []
            
            # Get content for user's primary interests
            for sdg_goal in user_interests:
                results = self.similarity_search.vector_client.search_by_sdg_goals(
                    sdg_goals=[sdg_goal],
                    limit=limit // len(user_interests) + 2
                )
                
                # Apply recommendation scoring
                for result in results:
                    score = self._calculate_recommendation_score(
                        result, user_interests, region, language
                    )
                    result["recommendation_score"] = score
                    recommendations.append(result)
            
            # Add interlinkage-based recommendations
            interlinkage_recs = await self._get_interlinkage_recommendations(
                user_interests, region, language, limit // 2
            )
            recommendations.extend(interlinkage_recs)
            
            # Sort and deduplicate
            recommendations.sort(key=lambda x: x["recommendation_score"], reverse=True)
            unique_recommendations = self._deduplicate_recommendations(recommendations)
            
            return {
                "user_interests": user_interests,
                "total_recommendations": len(unique_recommendations),
                "recommendations": unique_recommendations[:limit],
                "recommendation_metadata": {
                    "region": region,
                    "language": language,
                    "interlinkage_based": len(interlinkage_recs),
                    "direct_interest": len(recommendations) - len(interlinkage_recs)
                }
            }
            
        except Exception as e:
            logger.error(f"Error generating recommendations: {e}")
            raise
    
    def _calculate_recommendation_score(self,
                                      result: Dict[str, Any],
                                      user_interests: List[int],
                                      region: str = None,
                                      language: str = None) -> float:
        """Calculate recommendation score for content"""
        base_score = result.get("confidence_score", 0.5)
        
        # Interest alignment score
        result_sdgs = result.get("sdg_goals", [])
        interest_overlap = len(set(result_sdgs) & set(user_interests))
        interest_score = interest_overlap / max(len(user_interests), 1)
        
        # SDG importance weighting
        sdg_weight = sum(self.sdg_weights.get(goal, 0.5) for goal in result_sdgs) / max(len(result_sdgs), 1)
        
        # Region/language bonus
        region_bonus = 0.1 if result.get("region") == region else 0.0
        language_bonus = 0.1 if result.get("language") == language else 0.0
        
        # Content quality bonus
        quality_bonus = 0.1 if len(result.get("content", "")) > 1000 else 0.0
        
        recommendation_score = (
            base_score * 0.4 +
            interest_score * 0.3 +
            sdg_weight * 0.2 +
            region_bonus + language_bonus + quality_bonus
        )
        
        return min(recommendation_score, 1.0)
    
    async def _get_interlinkage_recommendations(self,
                                             user_interests: List[int],
                                             region: str,
                                             language: str,
                                             limit: int) -> List[Dict[str, Any]]:
        """Get recommendations based on SDG interlinkages"""
        related_sdgs = set()
        
        # Find related SDGs through interlinkages
        for goal in user_interests:
            related_sdgs.update(self.similarity_search.sdg_interlinkages.get(goal, []))
        
        # Remove already interested SDGs
        related_sdgs = list(related_sdgs - set(user_interests))
        
        # Get content for related SDGs
        if related_sdgs:
            results = self.similarity_search.vector_client.search_by_sdg_goals(
                sdg_goals=related_sdgs[:5],  # Top 5 related
                limit=limit
            )
            
            # Mark as interlinkage-based recommendations
            for result in results:
                result["recommendation_type"] = "interlinkage"
                result["recommendation_score"] = self._calculate_recommendation_score(
                    result, user_interests, region, language
                ) * 0.8  # Slight penalty for indirect interest
            
            return results
        
        return []
    
    def _deduplicate_recommendations(self, recommendations: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Remove duplicate recommendations based on content similarity"""
        seen_titles = set()
        unique_recs = []
        
        for rec in recommendations:
            title = rec.get("title", "").strip().lower()
            if title and title not in seen_titles:
                seen_titles.add(title)
                unique_recs.append(rec)
        
        return unique_recs


### /source /mnt/gigabyte1tb/SDG/sdg_root/src/vectorization/vector_db_client.py
"""
Vector Database Client for SDG Pipeline
Enhanced Weaviate integration with connection pooling and SDG schema
"""
import logging
import time
from typing import List, Dict, Any, Optional, Union
import weaviate
import numpy as np
from weaviate.embedded import EmbeddedOptions
import json
from datetime import datetime
import asyncio
from contextlib import asynccontextmanager
import threading

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class VectorDBClient:
    """
    Enhanced Weaviate client for SDG vector operations
    Includes connection pooling, retry logic, and SDG-specific schema
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.client = None
        self._connection_pool = []
        self._pool_lock = threading.Lock()
        self.max_connections = config.get("max_connections", 10)
        self.retry_attempts = config.get("retry_attempts", 3)
        self.retry_delay = config.get("retry_delay", 1.0)
        
        # SDG-specific configuration
        self.sdg_classes = [
            "SDGArticle", "SDGProgress", "SDGTarget", 
            "SDGIndicator", "RegionalData", "AITopic"
        ]
        
        self._initialize_client()
        self._setup_sdg_schema()
    
    def _initialize_client(self):
        """Initialize Weaviate client with configuration"""
        try:
            if self.config.get("embedded", False):
                # Embedded Weaviate for development
                self.client = weaviate.Client(
                    embedded_options=EmbeddedOptions(
                        hostname=self.config.get("hostname", "localhost"),
                        port=self.config.get("port", 8080),
                        grpc_port=self.config.get("grpc_port", 50051)
                    )
                )
            else:
                # Remote Weaviate instance
                auth_config = None
                if self.config.get("api_key"):
                    auth_config = weaviate.AuthApiKey(api_key=self.config["api_key"])
                
                headers = {}
                if self.config.get("openai_api_key"):
                    headers["X-OpenAI-Api-Key"] = self.config["openai_api_key"]
                
                self.client = weaviate.Client(
                    url=self.config.get("url", "http://localhost:8080"),
                    auth_client_secret=auth_config,
                    additional_headers=headers
                )
            
            # Test connection
            if self.client.is_ready():
                logger.info("Weaviate client initialized successfully")
            else:
                raise ConnectionError("Weaviate client not ready")
                
        except Exception as e:
            logger.error(f"Error initializing Weaviate client: {e}")
            raise
    
    def _setup_sdg_schema(self):
        """Setup SDG-specific Weaviate schema"""
        try:
            # Define SDG Article class schema
            sdg_article_schema = {
                "class": "SDGArticle",
                "description": "SDG-related articles and documents",
                "vectorizer": "none",  # We'll provide our own vectors
                "properties": [
                    {
                        "name": "title",
                        "dataType": ["text"],
                        "description": "Article title"
                    },
                    {
                        "name": "content",
                        "dataType": ["text"],
                        "description": "Full article content"
                    },
                    {
                        "name": "summary",
                        "dataType": ["text"],
                        "description": "Article summary"
                    },
                    {
                        "name": "sdg_goals",
                        "dataType": ["int[]"],
                        "description": "Related SDG goals (1-17)"
                    },
                    {
                        "name": "sdg_targets",
                        "dataType": ["text[]"],
                        "description": "Related SDG targets"
                    },
                    {
                        "name": "region",
                        "dataType": ["text"],
                        "description": "Geographic region"
                    },
                    {
                        "name": "language",
                        "dataType": ["text"],
                        "description": "Content language"
                    },
                    {
                        "name": "publication_date",
                        "dataType": ["date"],
                        "description": "Publication date"
                    },
                    {
                        "name": "source_url",
                        "dataType": ["text"],
                        "description": "Source URL"
                    },
                    {
                        "name": "confidence_score",
                        "dataType": ["number"],
                        "description": "SDG classification confidence"
                    }
                ]
            }
            
            # Create schema if it doesn't exist
            if not self.client.schema.exists("SDGArticle"):
                self.client.schema.create_class(sdg_article_schema)
                logger.info("Created SDGArticle schema")
            
            # Additional schemas for other SDG entities
            self._create_additional_schemas()
            
        except Exception as e:
            logger.error(f"Error setting up SDG schema: {e}")
            raise
    
    def _create_additional_schemas(self):
        """Create additional SDG-related schemas"""
        schemas = [
            {
                "class": "SDGProgress",
                "description": "SDG progress tracking data",
                "vectorizer": "none",
                "properties": [
                    {"name": "country", "dataType": ["text"]},
                    {"name": "sdg_goal", "dataType": ["int"]},
                    {"name": "indicator_value", "dataType": ["number"]},
                    {"name": "year", "dataType": ["int"]},
                    {"name": "data_source", "dataType": ["text"]}
                ]
            },
            {
                "class": "RegionalData", 
                "description": "Region-specific SDG data",
                "vectorizer": "none",
                "properties": [
                    {"name": "region", "dataType": ["text"]},
                    {"name": "country", "dataType": ["text"]},
                    {"name": "sdg_scores", "dataType": ["number[]"]},
                    {"name": "metadata", "dataType": ["text"]}
                ]
            }
        ]
        
        for schema in schemas:
            if not self.client.schema.exists(schema["class"]):
                self.client.schema.create_class(schema)
                logger.info(f"Created {schema['class']} schema")
    
    async def insert_embeddings(self, 
                              documents: List[Dict[str, Any]], 
                              class_name: str = "SDGArticle",
                              batch_size: int = 100) -> List[str]:
        """
        Insert documents with embeddings into Weaviate
        """
        uuids = []
        total_batches = (len(documents) + batch_size - 1) // batch_size
        
        logger.info(f"Inserting {len(documents)} documents in {total_batches} batches")
        
        for i in range(0, len(documents), batch_size):
            batch = documents[i:i + batch_size]
            batch_uuids = await self._insert_batch(batch, class_name)
            uuids.extend(batch_uuids)
            
            batch_num = (i // batch_size) + 1
            logger.info(f"Inserted batch {batch_num}/{total_batches}")
        
        return uuids
    
    async def _insert_batch(self, documents: List[Dict[str, Any]], class_name: str) -> List[str]:
        """Insert a batch of documents with retry logic"""
        for attempt in range(self.retry_attempts):
            try:
                with self.client.batch as batch:
                    batch.batch_size = len(documents)
                    uuids = []
                    
                    for doc in documents:
                        # Extract vector and properties
                        vector = doc.pop("vector", None)
                        uuid = self.client.batch.add_data_object(
                            data_object=doc,
                            class_name=class_name,
                            vector=vector
                        )
                        uuids.append(uuid)
                    
                    return uuids
                    
            except Exception as e:
                logger.warning(f"Batch insert attempt {attempt + 1} failed: {e}")
                if attempt < self.retry_attempts - 1:
                    await asyncio.sleep(self.retry_delay * (2 ** attempt))
                else:
                    raise
    
    def search_similar(self, 
                      query_vector: np.ndarray,
                      class_name: str = "SDGArticle", 
                      limit: int = 10,
                      where_filter: Dict[str, Any] = None,
                      additional_fields: List[str] = None) -> List[Dict[str, Any]]:
        """
        Semantic similarity search with SDG-specific filtering
        """
        try:
            # Build the query
            query_builder = (
                self.client.query
                .get(class_name, additional_fields or ["title", "summary", "sdg_goals", "region"])
                .with_near_vector({
                    "vector": query_vector.tolist(),
                    "certainty": 0.7
                })
                .with_limit(limit)
                .with_additional(["certainty", "distance"])
            )
            
            # Add where filter if provided
            if where_filter:
                query_builder = query_builder.with_where(where_filter)
            
            result = query_builder.do()
            
            # Extract results
            class_results = result.get("data", {}).get("Get", {}).get(class_name, [])
            return class_results
            
        except Exception as e:
            logger.error(f"Error in similarity search: {e}")
            raise
    
    def search_by_sdg_goals(self, 
                           sdg_goals: List[int],
                           query_vector: np.ndarray = None,
                           limit: int = 50) -> List[Dict[str, Any]]:
        """Search for content related to specific SDG goals"""
        where_filter = {
            "operator": "ContainsAny",
            "path": ["sdg_goals"], 
            "valueIntArray": sdg_goals
        }
        
        if query_vector is not None:
            return self.search_similar(
                query_vector=query_vector,
                where_filter=where_filter,
                limit=limit
            )
        else:
            # Pure filter search without vector similarity
            try:
                result = (
                    self.client.query
                    .get("SDGArticle", ["title", "summary", "sdg_goals", "region", "confidence_score"])
                    .with_where(where_filter)
                    .with_limit(limit)
                    .do()
                )
                return result.get("data", {}).get("Get", {}).get("SDGArticle", [])
            except Exception as e:
                logger.error(f"Error in SDG goal search: {e}")
                raise
    
    def search_by_region(self, 
                        region: str,
                        query_vector: np.ndarray = None,
                        limit: int = 30) -> List[Dict[str, Any]]:
        """Search for region-specific SDG content"""
        where_filter = {
            "operator": "Equal",
            "path": ["region"],
            "valueText": region
        }
        
        if query_vector is not None:
            return self.search_similar(
                query_vector=query_vector,
                where_filter=where_filter,
                limit=limit
            )
        else:
            try:
                result = (
                    self.client.query
                    .get("SDGArticle", ["title", "summary", "region", "sdg_goals"])
                    .with_where(where_filter)
                    .with_limit(limit)
                    .do()
                )
                return result.get("data", {}).get("Get", {}).get("SDGArticle", [])
            except Exception as e:
                logger.error(f"Error in region search: {e}")
                raise
    
    def get_statistics(self) -> Dict[str, Any]:
        """Get database statistics"""
        try:
            stats = {}
            for class_name in self.sdg_classes:
                if self.client.schema.exists(class_name):
                    result = self.client.query.aggregate(class_name).with_meta_count().do()
                    count = result.get("data", {}).get("Aggregate", {}).get(class_name, [{}])[0].get("meta", {}).get("count", 0)
                    stats[class_name] = count
            return stats
        except Exception as e:
            logger.error(f"Error getting statistics: {e}")
            return {}
    
    def health_check(self) -> Dict[str, Any]:
        """Perform health check on vector database"""
        try:
            is_ready = self.client.is_ready()
            is_live = self.client.is_live()
            
            return {
                "status": "healthy" if is_ready and is_live else "unhealthy",
                "ready": is_ready,
                "live": is_live,
                "timestamp": datetime.utcnow().isoformat(),
                "statistics": self.get_statistics()
            }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "timestamp": datetime.utcnow().isoformat()
            }
    
    def close(self):
        """Close the vector database connection"""
        if self.client:
            # Weaviate client doesn't have explicit close method
            # but we can clear the connection pool
            with self._pool_lock:
                self._connection_pool.clear()
            logger.info("Vector database client closed")

# Connection manager for async operations
@asynccontextmanager
async def get_vector_client(config: Dict[str, Any]):
    """Context manager for vector database operations"""
    client = VectorDBClient(config)
    try:
        yield client
    finally:
        client.close()


