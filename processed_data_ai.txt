/sdg_root/src/data_processing

-- Dockerfile

# sdg_root/src/data_processing/Dockerfile
FROM python:3.11-slim

# Installiere ffmpeg und andere notwendige Tools
RUN apt-get update && apt-get install -y ffmpeg python3 python3-pip

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["python3", "main.py"]

-- requirement.txt

faster-whisper
sentence-transformers
sqlalchemy
psycopg2-binary
pypdf2
python-docx
requests
Pillow
pytesseract
deep_translator
weaviate-client>=4.0.0

-- main.py

import os
import time
import json
import psycopg2
from sqlalchemy import create_engine, text
from sentence_transformers import SentenceTransformer
from faster_whisper import WhisperModel
from sqlalchemy.exc import OperationalError
import PyPDF2
from docx import Document
import csv
import re
import datetime

from .core.db_utils import save_to_database
from .core.file_extraction import FileHandler
from .core.processing_logic import ProcessingLogic
from .core.api_client import ApiClient


# Konfiguration
RAW_DATA_DIR = "/app/raw_data"
PROCESSED_DATA_DIR = "/app/processed_data"
DATABASE_URL = os.environ.get("DATABASE_URL")
IMAGES_DIR = "/app/images"

CLEANUP_INTERVAL_DAYS = 7 # Bereinigung alle 7 Tage
last_cleanup_timestamp = 0


# Sicherstellen, dass die Verzeichnisse existieren
os.makedirs(RAW_DATA_DIR, exist_ok=True)
os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)

# Lade KI-Modelle einmalig
try:
    whisper_model = WhisperModel("small", device="cpu")
    sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
except Exception as e:
    print(f"Fehler beim Laden der KI-Modelle: {e}")
    exit(1)

file_handler = FileHandler()
processing_logic = ProcessingLogic(whisper_model, sentence_model)
api_client = ApiClient()


def run_processing_worker():
    """Überwacht das RAW_DATA_DIR auf neue Dateien und verarbeitet sie."""
    print("Starte Data Processing Service...")
    global last_cleanup_timestamp
    while True:
        # Periodische Bereinigung durchführen
        if (datetime.datetime.now() - datetime.datetime.fromtimestamp(last_cleanup_timestamp)).days >= CLEANUP_INTERVAL_DAYS:
            file_handler.cleanup_processed_data(PROCESSED_DATA_DIR, CLEANUP_INTERVAL_DAYS)
            last_cleanup_timestamp = time.time()
            
        json_files = [f for f in os.listdir(RAW_DATA_DIR) if f.endswith('.json')]
        if not json_files:
            print("Keine neuen Metadaten-Dateien gefunden. Warte...")
            time.sleep(30)
            continue

        for json_file_name in json_files:
            try:
                metadata_path = os.path.join(RAW_DATA_DIR, json_file_name)
                
                # Metadaten aus JSON laden
                metadata = file_handler.get_metadata_from_json(metadata_path)
                source_url = metadata.get('source_url', '')
                print(source_url)

                # API-basierte Metadatenextraktion (wie zuvor)
                doi_match = re.search(r'(10\.\d{4,9}/[-._;()/:a-zA-Z0-9]+)', source_url)
                isbn_match = re.search(r'ISBN(-1[03])?:?\s+((978|979)[- ]?\d{1,5}[- ]?\d{1,7}[- ]?\d{1,6}[- ]?\d{1,1})', source_url)
                un_match = re.search(r'(undocs\.org|un\.org)', source_url)
                oecd_match = re.search(r'(oecd-ilibrary\.org|stats\.oecd\.org)', source_url)
                wb_match = re.search(r'(worldbank\.org|data\.worldbank\.org)', source_url)
                
                api_metadata = {}
                if doi_match:
                    api_metadata = api_client.get_metadata_from_doi(doi_match.group(1))
                    api_metadata['doi'] = doi_match.group(1)
                elif isbn_match:
                    api_metadata = api_client.get_metadata_from_isbn(isbn_match.group(2))
                    api_metadata['isbn'] = isbn_match.group(2)
                elif un_match:
                    symbol = "E/2023/1"
                    api_metadata = api_client.get_metadata_from_un_digital_library(symbol)
                elif oecd_match:
                    dataset_id = "EDU_ENRL"
                    api_metadata = api_client.get_metadata_from_oecd(dataset_id)
                elif wb_match:
                    indicator_id = "SP.POP.TOTL"
                    api_metadata = api_client.get_metadata_from_world_bank(indicator_id)
                
                metadata.update(api_metadata)
                
                # Bestimme den Pfad zur Mediendatei
                base_name = os.path.splitext(json_file_name)[0]
                media_path = None
                for ext in ['.mp3', '.txt', '.pdf', '.docx', '.csv']:
                    potential_path = os.path.join(RAW_DATA_DIR, f"{base_name}{ext}")
                    if os.path.exists(potential_path):
                        media_path = potential_path
                        break

                if not media_path:
                    print(f"Keine Mediendatei für {json_file_name} gefunden. Überspringe...")
                    os.remove(metadata_path)
                    continue

                # Text extrahieren / Audio transkribieren
                text_content = ""
                if media_path.endswith('.mp3'):
                    text_content = processing_logic.transcribe_audio(media_path)
                else:
                    text_content = file_handler.extract_text(media_path)
                
                if len(text_content) > 1000:  # Use chunking for large documents
                    chunks_data = processing_logic.process_text_with_chunking(text_content)
                    save_to_database_with_chunks(metadata, text_content, chunks_data['chunks'], chunks_data['embeddings'])
                else:
                    processed_data = processing_logic.process_text_for_ai(text_content)
                    save_to_database(metadata, text_content, processed_data['embeddings'])

                # Speichern in DB
                save_to_database(metadata, text_content, embeddings)

                # **Speichern als JSON-Backup in PROCESSED_DATA_DIR**
                backup_path = os.path.join(PROCESSED_DATA_DIR, f"{base_name}_processed.json")
                backup_content = {
                    "metadata": metadata,
                    "text": text_content,
                    "embeddings": embeddings
                }
                with open(backup_path, 'w', encoding='utf-8') as f:
                    json.dump(backup_content, f, indent=4)
                print(f"Backup gespeichert: {backup_path}")

                # Rohdateien löschen
                os.remove(metadata_path)
                os.remove(media_path)
                print(f"Verarbeitung von {json_file_name} erfolgreich. Rohdateien gelöscht.")

            except Exception as e:
                print(f"Fehler beim Verarbeiten von {json_file_name}: {e}")
                time.sleep(10)
      
if __name__ == "__main__":
    run_processing_worker()
    

/sdg_root/src/data_processing/core

-- api_client.py

import requests
import json
import re
from typing import Dict, Any

class ApiClient:
    def __init__(self):
        self.headers = {'User-Agent': 'SDG-KI-Project/1.0 (info@example.com)'}

    def get_metadata_from_doi(self, doi: str) -> Dict[str, Any]:
        print(f"Abfrage der CrossRef-API für DOI: {doi}")
        api_url = f"https://api.crossref.org/works/{doi}"
        try:
            response = requests.get(api_url, headers=self.headers, timeout=10)
            response.raise_for_status()
            data = response.json()
            message = data.get('message', {})
            metadata = {
                'title': message.get('title', [])[0] if message.get('title') else None,
                'authors': ', '.join([author.get('given', '') + ' ' + author.get('family', '') for author in message.get('author', [])]),
                'publication_year': message.get('issued', {}).get('date-parts', [[None]]),
                'publisher': message.get('publisher'),
                'doi': message.get('DOI'),
                'keywords': ", ".join(message.get('subject', [])) if 'subject' in message else None,
                'abstract_original': message.get('abstract'),
                # ggf. weitere Felder parsen und zuweisen
            }
            return metadata
        except (requests.exceptions.RequestException, IndexError, KeyError) as e:
            print(f"Fehler bei der CrossRef-Abfrage für {doi}: {e}")
            return {}

    def get_metadata_from_isbn(self, isbn: str) -> Dict[str, Any]:
        print(f"Abfrage der Google Books API für ISBN: {isbn}")
        api_url = f"https://www.googleapis.com/books/v1/volumes?q=isbn:{isbn}"
        try:
            response = requests.get(api_url, headers=self.headers, timeout=10)
            response.raise_for_status()
            data = response.json()
            if data.get('totalItems') == 0:
                return {}
            item = data['items'][0]['volumeInfo']
            metadata = {
                'title': item.get('title'),
                'authors': ', '.join(item.get('authors', [])),
                'publisher': item.get('publisher'),
                'publication_year': item.get('publishedDate', 'Unknown').split('-'),
                'isbn': isbn,
                'abstract_original': item.get('description'),
                # ggf. weitere Felder parsen und zuweisen
            }
            return metadata
        except (requests.exceptions.RequestException, IndexError, KeyError) as e:
            print(f"Fehler bei der Google Books API-Abfrage für {isbn}: {e}")
            return {}

    def get_metadata_from_un_digital_library(self, query: str) -> Dict[str, Any]:
        print(f"Abfrage der UN Digital Library für Suchbegriff: {query}")
        api_url = f"https://digitallibrary.un.org/record?format=json&searchTerm={query}"
        try:
            response = requests.get(api_url, headers=self.headers, timeout=10)
            response.raise_for_status()
            data = response.json()
            if data and 'results' in data:
                first_result = data['results'][0]['value']
                metadata = {
                    'title': first_result.get('title'),
                    'authors': first_result.get('authors_names'),
                    'publication_year': first_result.get('publication_date'),
                    'source_url': first_result.get('url'),
                    # ggf. weitere Felder
                }
                return metadata
            return {}
        except (requests.exceptions.RequestException, IndexError, KeyError) as e:
            print(f"Fehler bei der UN Digital Library Abfrage: {e}")
            return {}

    def get_metadata_from_oecd(self, dataset_id: str) -> Dict[str, Any]:
        print(f"Abfrage der OECD API für Dataset: {dataset_id}")
        api_url = f"https://sdmx.oecd.org/public/rest/data/OECD.SDD.NAD,{dataset_id}@DF_NAAG_I?format=jsondata"
        try:
            response = requests.get(api_url, headers=self.headers, timeout=10)
            response.raise_for_status()
            data = response.json()
            metadata = {
                'title': f"OECD Dataset: {dataset_id}",
                'publisher': "OECD",
                'source_url': api_url
            }
            return metadata
        except (requests.exceptions.RequestException, IndexError, KeyError) as e:
            print(f"Fehler bei der OECD API-Abfrage: {e}")
            return {}

    def get_metadata_from_world_bank(self, query: str) -> Dict[str, Any]:
        print(f"Abfrage der Weltbank API für Suchbegriff: {query}")
        api_url = f"https://search.worldbank.org/api/v3/wds?format=json&qterm={query}"
        try:
            response = requests.get(api_url, headers=self.headers, timeout=10)
            response.raise_for_status()
            data = response.json()
            if data and 'documents' in data:
                first_result = data['documents'][0]
                metadata = {
                    'title': first_result.get('title'),
                    'authors': first_result.get('authors_names'),
                    'publication_year': first_result.get('pub_date'),
                    'source_url': first_result.get('url')
                }
                return metadata
            return {}
        except (requests.exceptions.RequestException, IndexError, KeyError) as e:
            print(f"Fehler bei der Weltbank API-Abfrage: {e}")
            return {}


-- db_utils.py

import os
import time
import weaviate
from sqlalchemy import create_engine, text
from sqlalchemy.exc import OperationalError
from typing import Dict, Any, List

DATABASE_URL = os.environ.get("DATABASE_URL")
WEAVIATE_URL = os.environ.get("WEAVIATE_URL")

def get_weaviate_client():
    """Erstellt eine Weaviate-Client-Instanz."""
    return weaviate.Client(url=WEAVIATE_URL)

def save_to_database(
    metadata: Dict[str, Any],
    text_content: str,
    embeddings: List[float],
    chunks_data: List[Dict] = None
):
    """
    Speichert Metadaten in PostgreSQL und Vektoren in Weaviate.
    Optional: Speichert Chunks, falls chunks_data übergeben wird.
    """
    try:
        engine = create_engine(DATABASE_URL)
        with engine.connect() as connection:
            # Save article as usual
            insert_query = text("""
                INSERT INTO articles (
                    title, content_original, content_english, keywords, sdg_id, authors, publication_year,
                    publisher, doi, isbn, region, context, study_type, research_methods, data_sources,
                    funding, funding_info, bias_indicators, abstract_original, abstract_english,
                    relevance_questions, source_url, availability, citation_count, impact_metrics,
                    impact_factor, policy_impact
                ) VALUES (
                    :title, :content_original, :content_english, :keywords, :sdg_id, :authors, :publication_year,
                    :publisher, :doi, :isbn, :region, :context, :study_type, :research_methods, :data_sources,
                    :funding, :funding_info, :bias_indicators, :abstract_original, :abstract_english,
                    :relevance_questions, :source_url, :availability, :citation_count, :impact_metrics,
                    :impact_factor, :policy_impact
                ) RETURNING id
            """)

            result = connection.execute(insert_query, {
                "title": metadata.get('title'),
                "content_original": metadata.get('content_original', text_content),
                "content_english": metadata.get('content_english'),
                "keywords": metadata.get('keywords'),
                "sdg_id": metadata.get('sdg_id'),
                "authors": metadata.get('authors'),
                "publication_year": metadata.get('publication_year'),
                "publisher": metadata.get('publisher'),
                "doi": metadata.get('doi'),
                "isbn": metadata.get('isbn'),
                "region": metadata.get('region'),
                "context": metadata.get('context'),
                "study_type": metadata.get('study_type'),
                "research_methods": metadata.get('research_methods'),
                "data_sources": metadata.get('data_sources'),
                "funding": metadata.get('funding'),
                "funding_info": metadata.get('funding_info'),
                "bias_indicators": metadata.get('bias_indicators'),
                "abstract_original": metadata.get('abstract_original'),
                "abstract_english": metadata.get('abstract_english'),
                "relevance_questions": metadata.get('relevance_questions'),
                "source_url": metadata.get('source_url'),
                "availability": metadata.get('availability'),
                "citation_count": metadata.get('citation_count'),
                "impact_metrics": metadata.get('impact_metrics'),
                "impact_factor": metadata.get('impact_factor'),
                "policy_impact": metadata.get('policy_impact')
            })
            article_id = result.scalar_one()

            # --- Tag relations exactly as before ---
            for tag_name in metadata.get('tags', []):
                tag_id = connection.execute(
                    text("SELECT id FROM tags WHERE name = :name"),
                    {"name": tag_name}
                ).scalar_one_or_none()
                if not tag_id:
                    tag_id = connection.execute(
                        text("INSERT INTO tags (name) VALUES (:name) RETURNING id"),
                        {"name": tag_name}
                    ).scalar_one()
                connection.execute(
                    text("INSERT INTO articles_tags (article_id, tag_id) VALUES (:article_id, :tag_id)"),
                    {"article_id": article_id, "tag_id": tag_id}
                )

            # --- AI topics relations exactly as before ---
            for topic_name in metadata.get('ai_topics', []):
                topic_id = connection.execute(
                    text("SELECT id FROM ai_topics WHERE name = :name"),
                    {"name": topic_name}
                ).scalar_one_or_none()
                if not topic_id:
                    topic_id = connection.execute(
                        text("INSERT INTO ai_topics (name) VALUES (:name) RETURNING id"),
                        {"name": topic_name}
                    ).scalar_one()
                connection.execute(
                    text("INSERT INTO articles_ai_topics (article_id, ai_topic_id) VALUES (:article_id, :topic_id)"),
                    {"article_id": article_id, "topic_id": topic_id}
                )

            # --- NEW: Save chunks if provided ---
            if chunks_data:
                for i, chunk_data in enumerate(chunks_data):
                    chunk_query = text("""
                        INSERT INTO article_chunks (
                            article_id, chunk_order, text, chunk_length, sdg_section, confidence_score
                        ) VALUES (
                            :article_id, :chunk_order, :text, :chunk_length, :sdg_section, :confidence_score
                        )
                    """)
                    connection.execute(chunk_query, {
                        "article_id": article_id,
                        "chunk_order": i,
                        "text": chunk_data["text"],
                        "chunk_length": len(chunk_data["text"]),
                        "sdg_section": chunk_data.get("sdg_section", "general"),
                        "confidence_score": chunk_data.get("confidence_score", 0.0)
                    })

            connection.commit()
            print(f"Metadaten für Artikel {article_id} in PostgreSQL gespeichert.")

        # --- Vector: always save in Weaviate, as in original code ---
        client = get_weaviate_client()
        client.schema.get()
        try:
            client.schema.get("ArticleVector")
        except weaviate.exceptions.UnexpectedStatusCodeException:
            class_obj = {
                "class": "ArticleVector",
                "vectorizer": "text2vec-transformers",
                "properties": [
                    {"name": "text", "dataType": ["text"]},
                    {"name": "articleId", "dataType": ["int"]}
                ]
            }
            client.schema.create_class(class_obj)

        data_object = {"text": text_content, "articleId": article_id}
        client.data_object.create(data_object=data_object, class_name="ArticleVector", vector=embeddings)
        print(f"Vektor für Artikel {article_id} in Weaviate gespeichert.")

    except OperationalError as e:
        print(f"Datenbankverbindung fehlgeschlagen: {e}")
        time.sleep(10)
    except Exception as e:
        print(f"Fehler beim Speichern in der Datenbank: {e}")


-- file_handler.py

import os
import csv
from docx import Document
import PyPDF2
from PIL import Image
import pytesseract
import fitz  # PyMuPDF
import datetime

class FileHandler:
    def __init__(self, images_dir):
        self.images_dir = images_dir
        os.makedirs(self.images_dir, exist_ok=True)

    def _extract_images_from_pdf(self, file_path: str, doc_name: str) -> list:
        """Extrahiert Bilder aus einer PDF-Datei und speichert sie."""
        image_paths = []
        try:
            pdf_document = fitz.open(file_path)
            for page_index in range(len(pdf_document)):
                page = pdf_document[page_index]
                image_list = page.get_images(full=True)
                for image_index, img in enumerate(image_list, start=1):
                    xref = img[0]
                    base_image = pdf_document.extract_image(xref)
                    image_bytes = base_image["image"]
                    file_extension = base_image["ext"]
                    image_path = os.path.join(self.images_dir, f"{doc_name}_page{page_index+1}_img{image_index}.{file_extension}")
                    with open(image_path, "wb") as f:
                        f.write(image_bytes)
                    image_paths.append({
                        "original_path": image_path,
                        "page": page_index + 1,
                        "caption": None,  # Optional bei OCR/Meta extrahieren!
                        "sdg_tags": {},
                        "ai_tags": None,
                        "image_type": file_extension
                    })
        except Exception as e:
            print(f"Fehler beim Extrahieren von Bildern aus PDF: {e}")
        return image_paths

    def _extract_images_from_docx(self, file_path: str, doc_name: str) -> list:
        """Extrahiert Bilder aus einer DOCX-Datei und speichert sie."""
        image_paths = []
        try:
            document = Document(file_path)
            for rel in document.part.rels:
                if "image" in document.part.rels[rel].target_ref:
                    image_part = document.part.rels[rel].target_part
                    image_path = os.path.join(self.images_dir, f"{doc_name}_{os.path.basename(image_part.partname)}")
                    with open(image_path, "wb") as f:
                        f.write(image_part.blob)
                    image_paths.append({
                        "original_path": image_path,
                        "page": None,
                        "caption": None,
                        "sdg_tags": {},
                        "ai_tags": None,
                        "image_type": os.path.splitext(image_path)[-1].replace('.', '')
                    })
        except Exception as e:
            print(f"Fehler beim Extrahieren von Bildern aus DOCX: {e}")
        return image_paths

    def get_text_from_file(self, file_path: str) -> str:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()

    def extract_text_from_pdf(self, file_path: str) -> tuple:
        text = ""
        metadata = {}
        try:
            with open(file_path, 'rb') as f:
                pdf_reader = PyPDF2.PdfReader(f)
                doc_info = pdf_reader.metadata
                if doc_info:
                    metadata['title'] = doc_info.get('/Title', None)
                    metadata['authors'] = doc_info.get('/Author', None)
                    metadata['creation_date'] = doc_info.get('/CreationDate', None)
                for page in pdf_reader.pages:
                    text += page.extract_text()
        except Exception as e:
            print(f"Fehler beim Extrahieren von Text oder Metadaten aus PDF: {e}")
        return text, metadata

    def extract_text_from_docx(self, file_path: str) -> str:
        text = ""
        try:
            doc = Document(file_path)
            for para in doc.paragraphs:
                text += para.text + "\n"
        except Exception as e:
            print(f"Fehler beim Extrahieren von Text aus DOCX: {e}")
        return text

    def extract_text_from_csv(self, file_path: str) -> str:
        text = ""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                csv_reader = csv.reader(f)
                for row in csv_reader:
                    text += " ".join(row) + "\n"
        except Exception as e:
            print(f"Fehler beim Extrahieren von Text aus CSV: {e}")
        return text

    def extract_text(self, file_path: str) -> str:
        if file_path.endswith('.mp3'):
            return ""
        elif file_path.endswith('.pdf'):
            text, _ = self.extract_text_from_pdf(file_path)
            return text
        elif file_path.endswith('.docx'):
            return self.extract_text_from_docx(file_path)
        elif file_path.endswith('.csv'):
            return self.extract_text_from_csv(file_path)
        else:
            return self.get_text_from_file(file_path)

    def get_metadata_from_json(self, file_path: str) -> dict:
        import json
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (FileNotFoundError, json.JSONDecodeError):
            return {}

    def cleanup_processed_data(self, directory: str, retention_days: int):
        """Löscht Dateien in einem Verzeichnis, die älter als retention_days sind."""
        print(f"Starte Bereinigung von {directory}...")
        now = datetime.datetime.now()
        for filename in os.listdir(directory):
            filepath = os.path.join(directory, filename)
            if os.path.isfile(filepath):
                file_creation_time = datetime.datetime.fromtimestamp(os.path.getctime(filepath))
                if (now - file_creation_time).days > retention_days:
                    print(f"Lösche alte Datei: {filepath}")
                    os.remove(filepath)
        print("Bereinigung abgeschlossen.")
        
-- keywords.py

sdg_keywords_dict = {
    "SDG 1 No Poverty": [
        "no poverty", "end poverty", "armut beenden", "keine Armut",
        "aucune pauvreté", "erradicar la pobreza", "消除贫困"
    ],
    "SDG 2 Zero Hunger": [
        "zero hunger", "keinen hunger", "hunger beenden", "faim zéro",
        "hambre cero", "消除饥饿"
    ],
    "SDG 3 Good Health and Well-Being": [
        "good health", "gesundheit und wohlbefinden", "bonne santé", 
        "buena salud", "健康和福祉", "well-being", "bien-être", "bienestar"
    ],
    "SDG 4 Quality Education": [
        "quality education", "bildung von qualität", "éducation de qualité",
        "educación de calidad", "优质教育"
    ],
    "SDG 5 Gender Equality": [
        "gender equality", "gleichstellung der geschlechter", "égalité des sexes",
        "igualdad de género", "性别平等"
    ],
    "SDG 6 Clean Water and Sanitation": [
        "clean water", "sauberes wasser und sanitärversorgung", "eau propre",
        "agua limpia", "清洁饮水和卫生设施", "sanitation", "sanitäre einrichtungen", "assainissement"
    ],
    "SDG 7 Affordable and Clean Energy": [
        "clean energy", "bezahlbare und saubere energie", "énergie propre",
        "energía asequible y no contaminante", "可负担的清洁能源"
    ],
    "SDG 8 Decent Work and Economic Growth": [
        "decent work", "menschenwürdige arbeit", "travail décent",
        "trabajo decente", "体面工作", "economic growth", "wirtschaftswachstum", "croissance économique"
    ],
    "SDG 9 Industry, Innovation and Infrastructure": [
        "industry", "innovation", "industrie", "innovation", "infrastructure",
        "industrie, innovation et infrastructure", "industria, innovación e infraestructura", "产业创新和基础设施"
    ],
    "SDG 10 Reduced Inequalities": [
        "reduced inequalities", "ungleichheiten verringern", "réduction des inégalités",
        "reducción de las desigualdades", "减少不平等"
    ],
    "SDG 11 Sustainable Cities and Communities": [
        "sustainable cities", "nachhaltige städte", "villes durables",
        "ciudades sostenibles", "可持续城市", "communities", "gemeinschaften", "communautés"
    ],
    "SDG 12 Responsible Consumption and Production": [
        "responsible consumption", "verantwortungsvoller konsum", "consommation responsable",
        "consumo responsable", "负责任的消费和生产", "production", "produktion", "production"
    ],
    "SDG 13 Climate Action": [
        "climate action", "klimaschutz", "action climatique",
        "acción por el clima", "气候行动"
    ],
    "SDG 14 Life Below Water": [
        "life below water", "leben unter wasser", "vie aquatique",
        "vida submarina", "水下生物"
    ],
    "SDG 15 Life on Land": [
        "life on land", "leben an land", "vie terrestre",
        "vida de ecosistemas terrestres", "陆地生物"
    ],
    "SDG 16 Peace, Justice and Strong Institutions": [
        "peace", "justice", "friedliche gesellschaften", "paix", "justice", 
        "paz", "justicia", "和平、正义与强大机构", "strong institutions", "starke institutionen", "institutions fortes"
    ],
    "SDG 17 Partnerships for the Goals": [
        "partnerships", "partnerschaften zum erreichen der ziele", "partenariats",
        "alianzas", "可持续发展伙伴关系"
    ]
}


ai_keywords_dict = {
    "AI Governance & Policy": [
        "ai governance", "algorithmic regulation", "policy framework", "ki governance"
    ],
    "Ethics & Trustworthy AI": [
        "ai ethics", "responsible ai", "trustworthy ai", "explainable ai", "fairness", "bias mitigation"
    ],
    "Machine Learning": [
        "machine learning", "deep learning", "supervised learning", "unsupervised learning"
    ],
    "Natural Language Processing": [
        "nlp", "natural language processing", "sprachverarbeitung"
    ],
    "Computer Vision": [
        "computer vision", "object detection", "image recognition"
    ],
    "Robotics & Autonomous Systems": [
        "robotics", "autonomous vehicles", "drones"
    ],
    "Data & Big Data Analytics": [
        "big data", "data analytics", "data governance", "open data"
    ],
    "AI for Social Good": [
        "ai for good", "ai for sdgs", "humanitarian ai"
    ]
}


generic_keywords_dict = {
    "philosophie": ["philosophie", "philosophisch", "denken"],
    "gesellschaft": ["gesellschaft", "sozial", "politik", "soziologie"]
}

-- processing_logic.py

# Enhanced processing_logic.py
import re
from .text_chunker import SDGTextChunker
from .keywords import sdg_keywords_dict
from deep_translator import GoogleTranslator
import pytesseract
from PIL import Image

class ProcessingLogic:
    def __init__(self, whisper_model, sentence_model):
        self.whisper_model = whisper_model
        self.sentence_model = sentence_model
        self.text_chunker = SDGTextChunker(chunk_size=512, overlap=50)
    
    def process_text_for_ai_with_chunking(self, text_content: str) -> Dict[str, Any]:
        
        if len(text_content) <= 512:
            return self._process_single_chunk(text_content)
        
        chunks = self.text_chunker.chunk_by_sdg_sections(text_content)
        chunks = self.text_chunker.generate_embeddings_for_chunks(chunks)
        
        processed_chunks = []
        all_tags = set()
        
        for chunk in chunks:
            chunk_data = self._process_single_chunk(chunk["text"])
            chunk.update({
                "sdg_tags": chunk_data["tags"],
                "keywords": chunk_data.get("keywords", []),
                "abstract": chunk_data.get("abstract")
            })
            processed_chunks.append(chunk)
            all_tags.update(chunk_data["tags"])
        
        return {
            "chunks": processed_chunks,
            "combined_tags": list(all_tags),
            "total_chunks": len(processed_chunks),
            "total_length": len(text_content)
        }
    
    def _process_single_chunk(self, text: str) -> Dict[str, Any]:
        """Process a single chunk (your existing logic)."""
        embeddings = self.sentence_model.encode(text).tolist()
        tags = []
        
        text_lower = text.lower()
        for sdg_name, keywords in sdg_keywords_dict.items():
            for keyword in keywords:
                if keyword.lower() in text_lower:
                    tags.append(sdg_name)
                    break
        
        extracted_info = self.extract_abstract_and_keywords(text)
        tags.extend(extracted_info.get('keywords', []))
        
        return {
            "text": text,
            "embeddings": embeddings,
            "tags": list(set(tags)),
            "abstract": extracted_info.get('abstract'),
            "keywords": extracted_info.get('keywords', [])
        }
        
-- processing_worker.py

import os
import time
from .file_handler import FileHandler
from .processing_logic import ProcessingLogic
from .db_utils import save_to_database

class ProcessingWorker:
    def __init__(self, raw_data_dir: str, processed_data_dir: str, database_url: str,
                 whisper_model=None, sentence_model=None):
        self.raw_data_dir = raw_data_dir
        self.processed_data_dir = processed_data_dir
        self.database_url = database_url
        self.file_handler = FileHandler(images_dir=os.path.join(processed_data_dir, "images"))
        self.processing_logic = ProcessingLogic(whisper_model, sentence_model)
        os.makedirs(self.processed_data_dir, exist_ok=True)

    def run_worker(self):
        """Laufender Worker für alle JSON-Metadatendateien in raw_data_dir"""
        print("Starte Data Processing Service...")
        while True:
            json_files = [f for f in os.listdir(self.raw_data_dir) if f.endswith('.json')]
            if not json_files:
                print("Keine neuen Metadaten-Dateien gefunden. Warte...")
                time.sleep(30)
                continue

            for json_file_name in json_files:
                try:
                    metadata_path = os.path.join(self.raw_data_dir, json_file_name)
                    metadata = self.file_handler.get_metadata_from_json(metadata_path)
                    base_name = os.path.splitext(json_file_name)[0]

                    # Erweiterung: Bilder aus Medien extrahieren (optional)
                    # image_data = []
                    # if media_path.endswith('.pdf'):
                    #    image_data = self.file_handler._extract_images_from_pdf(media_path, base_name)
                    # elif media_path.endswith('.docx'):
                    #    image_data = self.file_handler._extract_images_from_docx(media_path, base_name)
                    # ... an API/db_utils übergeben ...

                    # Suche nach zugehöriger Mediendatei
                    media_path = None
                    for ext in ['.mp3', '.txt', '.pdf', '.docx', '.csv']:
                        potential_path = os.path.join(self.raw_data_dir, f"{base_name}{ext}")
                        if os.path.exists(potential_path):
                            media_path = potential_path
                            break

                    if not media_path:
                        print(f"Keine Mediendatei für {json_file_name} gefunden. Überspringe...")
                        os.remove(metadata_path)
                        continue

                    # Inhalt extrahieren
                    if media_path.endswith('.mp3'):
                        text_content = self.processing_logic.transcribe_audio(media_path)
                    else:
                        text_content = self.file_handler.extract_text(media_path)

                    processed_data = self.processing_logic.process_text_for_ai(text_content)
                    # Setze alle neuen Felder auf das Metadatenobjekt
                    for k, v in processed_data.items():
                        metadata[k] = v

                    # Embeddings als Output für Weaviate
                    embeddings = processed_data['embeddings']

                    # Datenbank speichern (DB + Vektor)
                    save_to_database(metadata, text_content, embeddings)

                    # Optional: Backup/Output
                    backup_path = os.path.join(self.processed_data_dir, f"{base_name}_processed.json")
                    with open(backup_path, 'w', encoding='utf-8') as f:
                        import json
                        json.dump({
                            "metadata": metadata,
                            "text": text_content,
                            "embeddings": embeddings
                        }, f, indent=4)
                    os.remove(metadata_path)
                    os.remove(media_path)
                    print(f"Verarbeitung von {json_file_name} erfolgreich. Dateien gelöscht.")

                except Exception as e:
                    print(f"Fehler beim Verarbeiten von {json_file_name}: {e}")
                    time.sleep(5)

if __name__ == "__main__":
    RAW_DATA_DIR = "/app/raw_data"
    PROCESSED_DATA_DIR = "/app/processed_data"
    DATABASE_URL = os.environ.get("DATABASE_URL")
    from faster_whisper import WhisperModel
    from sentence_transformers import SentenceTransformer
    whisper_model = WhisperModel("small", device="cpu")
    sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
    worker = ProcessingWorker(
        raw_data_dir=RAW_DATA_DIR,
        processed_data_dir=PROCESSED_DATA_DIR,
        database_url=DATABASE_URL,
        whisper_model=whisper_model,
        sentence_model=sentence_model
    )
    worker.run_worker()
    
-- sdg_interlinks

SDG_INTERLINKS = {
    "SDG 1 No Poverty": [
        "SDG 2 Zero Hunger",
        "SDG 3 Good Health and Well-being",
        "SDG 4 Quality Education",
        "SDG 5 Gender Equality",
        "SDG 8 Decent Work and Economic Growth",
        "SDG 10 Reduced Inequalities",
        "SDG 13 Climate Action"
    ],
    "SDG 2 Zero Hunger": [
        "SDG 1 No Poverty",
        "SDG 3 Good Health and Well-being",
        "SDG 6 Clean Water and Sanitation",
        "SDG 12 Responsible Consumption and Production",
        "SDG 13 Climate Action",
        "SDG 15 Life on Land"
    ],
    "SDG 3 Good Health and Well-being": [
        "SDG 1 No Poverty",
        "SDG 2 Zero Hunger",
        "SDG 4 Quality Education",
        "SDG 5 Gender Equality",
        "SDG 6 Clean Water and Sanitation",
        "SDG 13 Climate Action",
        "SDG 16 Peace, Justice and Strong Institutions"
    ],
    "SDG 4 Quality Education": [
        "SDG 1 No Poverty",
        "SDG 3 Good Health and Well-being",
        "SDG 5 Gender Equality",
        "SDG 8 Decent Work and Economic Growth",
        "SDG 10 Reduced Inequalities",
        "SDG 13 Climate Action"
    ],
    "SDG 5 Gender Equality": [
        "SDG 1 No Poverty",
        "SDG 3 Good Health and Well-being",
        "SDG 4 Quality Education",
        "SDG 8 Decent Work and Economic Growth",
        "SDG 10 Reduced Inequalities",
        "SDG 16 Peace, Justice and Strong Institutions"
    ],
    "SDG 6 Clean Water and Sanitation": [
        "SDG 2 Zero Hunger",
        "SDG 3 Good Health and Well-being",
        "SDG 7 Affordable and Clean Energy",
        "SDG 11 Sustainable Cities and Communities",
        "SDG 13 Climate Action",
        "SDG 14 Life Below Water",
        "SDG 15 Life on Land"
    ],
    "SDG 7 Affordable and Clean Energy": [
        "SDG 6 Clean Water and Sanitation",
        "SDG 8 Decent Work and Economic Growth",
        "SDG 9 Industry, Innovation and Infrastructure",
        "SDG 11 Sustainable Cities and Communities",
        "SDG 12 Responsible Consumption and Production",
        "SDG 13 Climate Action"
    ],
    "SDG 8 Decent Work and Economic Growth": [
        "SDG 1 No Poverty",
        "SDG 4 Quality Education",
        "SDG 5 Gender Equality",
        "SDG 7 Affordable and Clean Energy",
        "SDG 9 Industry, Innovation and Infrastructure",
        "SDG 10 Reduced Inequalities",
        "SDG 12 Responsible Consumption and Production"
    ],
    "SDG 9 Industry, Innovation and Infrastructure": [
        "SDG 7 Affordable and Clean Energy",
        "SDG 8 Decent Work and Economic Growth",
        "SDG 11 Sustainable Cities and Communities",
        "SDG 12 Responsible Consumption and Production",
        "SDG 13 Climate Action",
        "SDG 17 Partnerships for the Goals"
    ],
    "SDG 10 Reduced Inequalities": [
        "SDG 1 No Poverty",
        "SDG 4 Quality Education",
        "SDG 5 Gender Equality",
        "SDG 8 Decent Work and Economic Growth",
        "SDG 16 Peace, Justice and Strong Institutions",
        "SDG 17 Partnerships for the Goals"
    ],
    "SDG 11 Sustainable Cities and Communities": [
        "SDG 6 Clean Water and Sanitation",
        "SDG 7 Affordable and Clean Energy",
        "SDG 9 Industry, Innovation and Infrastructure",
        "SDG 12 Responsible Consumption and Production",
        "SDG 13 Climate Action",
        "SDG 15 Life on Land"
    ],
    "SDG 12 Responsible Consumption and Production": [
        "SDG 2 Zero Hunger",
        "SDG 7 Affordable and Clean Energy",
        "SDG 8 Decent Work and Economic Growth",
        "SDG 9 Industry, Innovation and Infrastructure",
        "SDG 11 Sustainable Cities and Communities",
        "SDG 13 Climate Action",
        "SDG 14 Life Below Water",
        "SDG 15 Life on Land"
    ],
    "SDG 13 Climate Action": [
        "SDG 1 No Poverty",
        "SDG 2 Zero Hunger",
        "SDG 3 Good Health and Well-being",
        "SDG 4 Quality Education",
        "SDG 6 Clean Water and Sanitation",
        "SDG 7 Affordable and Clean Energy",
        "SDG 9 Industry, Innovation and Infrastructure",
        "SDG 11 Sustainable Cities and Communities",
        "SDG 12 Responsible Consumption and Production",
        "SDG 14 Life Below Water",
        "SDG 15 Life on Land"
    ],
    "SDG 14 Life Below Water": [
        "SDG 6 Clean Water and Sanitation",
        "SDG 12 Responsible Consumption and Production",
        "SDG 13 Climate Action",
        "SDG 15 Life on Land"
    ],
    "SDG 15 Life on Land": [
        "SDG 2 Zero Hunger",
        "SDG 6 Clean Water and Sanitation",
        "SDG 11 Sustainable Cities and Communities",
        "SDG 12 Responsible Consumption and Production",
        "SDG 13 Climate Action",
        "SDG 14 Life Below Water"
    ],
    "SDG 16 Peace, Justice and Strong Institutions": [
        "SDG 3 Good Health and Well-being",
        "SDG 5 Gender Equality",
        "SDG 10 Reduced Inequalities",
        "SDG 17 Partnerships for the Goals"
    ],
    "SDG 17 Partnerships for the Goals": [
        "SDG 9 Industry, Innovation and Infrastructure",
        "SDG 10 Reduced Inequalities",
        "SDG 16 Peace, Justice and Strong Institutions"
    ]
}

-- text_chunker.py

# src/data_processing/core/text_chunker.py
import re
from typing import List, Dict, Any
from sentence_transformers import SentenceTransformer

class SDGTextChunker:
    def __init__(self, chunk_size: int = 512, overlap: int = 50, model_name: str = "all-MiniLM-L6-v2"):
        self.chunk_size = chunk_size
        self.overlap = overlap
        self.model = SentenceTransformer(model_name)
        
    def smart_chunk_by_sentences(self, text: str) -> List[Dict[str, Any]]:
        """
        Intelligently chunks text by sentences while respecting size limits.
        Maintains context and semantic coherence.
        """
        sentences = self._split_into_sentences(text)
        chunks = []
        current_chunk = ""
        current_length = 0
        chunk_id = 0
        
        for sentence in sentences:
            sentence_length = len(sentence)
            
            # If adding this sentence exceeds chunk_size, save current chunk
            if current_length + sentence_length > self.chunk_size and current_chunk:
                chunks.append({
                    "chunk_id": chunk_id,
                    "text": current_chunk.strip(),
                    "length": current_length,
                    "embedding": None  # To be filled later
                })
                
                # Start new chunk with overlap
                overlap_text = self._get_overlap_text(current_chunk)
                current_chunk = overlap_text + " " + sentence
                current_length = len(current_chunk)
                chunk_id += 1
            else:
                current_chunk += " " + sentence
                current_length += sentence_length
        
        # Add final chunk
        if current_chunk.strip():
            chunks.append({
                "chunk_id": chunk_id,
                "text": current_chunk.strip(),
                "length": current_length,
                "embedding": None
            })
            
        return chunks
    
    def chunk_by_sdg_sections(self, text: str) -> List[Dict[str, Any]]:
        """
        Chunks text by SDG-specific sections and topics.
        Uses your existing SDG keywords for intelligent sectioning.
        """
        from .keywords import sdg_keywords_dict
        
        chunks = []
        sections = self._identify_sdg_sections(text, sdg_keywords_dict)
        
        for section_name, section_text in sections.items():
            if len(section_text) > self.chunk_size:
                # Further chunk large sections
                sub_chunks = self.smart_chunk_by_sentences(section_text)
                for i, sub_chunk in enumerate(sub_chunks):
                    sub_chunk.update({
                        "sdg_section": section_name,
                        "sub_section_id": i
                    })
                    chunks.append(sub_chunk)
            else:
                chunks.append({
                    "chunk_id": len(chunks),
                    "text": section_text,
                    "length": len(section_text),
                    "sdg_section": section_name,
                    "embedding": None
                })
        
        return chunks
    
    def generate_embeddings_for_chunks(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Generate embeddings for each chunk using your existing sentence transformer."""
        for chunk in chunks:
            chunk["embedding"] = self.model.encode(chunk["text"]).tolist()
        return chunks
    
    def _split_into_sentences(self, text: str) -> List[str]:
        """Split text into sentences using regex."""
        sentences = re.split(r'(?<=[.!?])\s+', text)
        return [s.strip() for s in sentences if s.strip()]
    
    def _get_overlap_text(self, text: str) -> str:
        """Get last N characters for overlap."""
        return text[-self.overlap:] if len(text) > self.overlap else text
    
    def _identify_sdg_sections(self, text: str, sdg_keywords: Dict) -> Dict[str, str]:
        """Identify sections of text related to specific SDGs."""
        sections = {"general": ""}
        text_lower = text.lower()
        
        for sdg_name, keywords in sdg_keywords.items():
            section_text = ""
            for keyword in keywords:
                sentences = self._split_into_sentences(text)
                for sentence in sentences:
                    if keyword.lower() in sentence.lower():
                        section_text += sentence + " "
            
            if section_text.strip():
                sections[sdg_name] = section_text.strip()
            else:
                sections["general"] += text 
                
        return sections
        
/sdg_root/src/data_processing/core/vektorizer

-- text_vektorizer.py

from sentence_transformers import SentenceTransformer

class TextVectorizer:
    def __init__(self, model_name="all-MiniLM-L6-v2", device="cpu"):
        self.model = SentenceTransformer(model_name, device=device)

    def embed(self, text: str) -> list:
        return self.model.encode(text).tolist()
