/sdg_root/src/data_processing

-- Dockerfile

# sdg_root/src/data_processing/Dockerfile
FROM python:3.11-slim

# Installiere ffmpeg und andere notwendige Tools
RUN apt-get update && apt-get install -y ffmpeg python3 python3-pip

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["python3", "main.py"]

-- requirement.txt

faster-whisper
sentence-transformers
sqlalchemy
psycopg2-binary
pypdf2
python-docx
requests
Pillow
pytesseract
deep_translator
weaviate-client>=4.0.0

-- main.py

import os
import time
import json
import psycopg2
from sqlalchemy import create_engine, text
from sentence_transformers import SentenceTransformer
from faster_whisper import WhisperModel
from sqlalchemy.exc import OperationalError
import PyPDF2
from docx import Document
import csv
import re
import datetime
import logging

from .core.db_utils import save_to_database
from .core.file_extraction import FileHandler
from .core.processing_logic import ProcessingLogic
from .core.api_client import ApiClient

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

RAW_DATA_DIR = "/app/raw_data"
PROCESSED_DATA_DIR = "/app/processed_data"
DATABASE_URL = os.environ.get("DATABASE_URL")
IMAGES_DIR = "/app/images"

CLEANUP_INTERVAL_DAYS = 7 
last_cleanup_timestamp = 0

os.makedirs(RAW_DATA_DIR, exist_ok=True)
os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)
os.makedirs(IMAGES_DIR, exist_ok=True)

try:
    whisper_model = WhisperModel("small", device="cpu")
    sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
    logger.info("AI models loaded successfully")
except Exception as e:
    logger.error(f"Error loading AI models: {e}")
    exit(1)

file_handler = FileHandler(IMAGES_DIR)
processing_logic = ProcessingLogic(whisper_model, sentence_model)
api_client = ApiClient()

def run_processing_worker():
    """Monitor RAW_DATA_DIR for new files and process them."""
    logger.info("Starting Data Processing Service...")
    global last_cleanup_timestamp
    
    while True:
        try:
            current_time = time.time()
            if (current_time - last_cleanup_timestamp) > (CLEANUP_INTERVAL_DAYS * 24 * 3600):
                file_handler.cleanup_processed_data(PROCESSED_DATA_DIR, CLEANUP_INTERVAL_DAYS)
                last_cleanup_timestamp = current_time
                
            json_files = [f for f in os.listdir(RAW_DATA_DIR) if f.endswith('.json')]
            if not json_files:
                logger.info("No new metadata files found. Waiting...")
                time.sleep(30)
                continue

            for json_file_name in json_files:
                try:
                    metadata_path = os.path.join(RAW_DATA_DIR, json_file_name)
                    
                    metadata = file_handler.get_metadata_from_json(metadata_path)
                    source_url = metadata.get('source_url', '')
                    logger.info(f"Processing: {source_url}")

                    api_metadata = extract_api_metadata(source_url, api_client)
                    metadata.update(api_metadata)
                    
                    base_name = os.path.splitext(json_file_name)[0]
                    media_path = find_media_file(base_name, RAW_DATA_DIR)

                    if not media_path:
                        logger.warning(f"No media file found for {json_file_name}. Skipping...")
                        os.remove(metadata_path)
                        continue

                    text_content = extract_content(media_path, file_handler, processing_logic)
                    
                    if not text_content:
                        logger.warning(f"No content extracted from {media_path}")
                        continue
                    
                    if len(text_content) > 1000:
                        processed_data = processing_logic.process_text_for_ai_with_chunking(text_content)
                        save_to_database(metadata, text_content, processed_data['combined_embeddings'], processed_data['chunks'])
                    else:
                        processed_data = processing_logic.process_text_for_ai(text_content)
                        save_to_database(metadata, text_content, processed_data['embeddings'])

                    save_backup(metadata, text_content, processed_data, base_name, PROCESSED_DATA_DIR)

                    os.remove(metadata_path)
                    os.remove(media_path)
                    logger.info(f"Processing of {json_file_name} completed successfully")

                except Exception as e:
                    logger.error(f"Error processing {json_file_name}: {e}")
                    time.sleep(5)
                    
        except KeyboardInterrupt:
            logger.info("Processing worker stopped by user")
            break
        except Exception as e:
            logger.error(f"Unexpected error in processing worker: {e}")
            time.sleep(60)

def process_text_with_chunking(self, text_content: str) -> Dict[str, Any]:
    """Process text with intelligent chunking for large documents"""
    
    if len(text_content) <= 512:
        return self._process_single_chunk(text_content)
    
    chunks = self.text_chunker.chunk_by_sdg_sections(text_content)
    chunks = self.text_chunker.generate_embeddings_for_chunks(chunks)
    
    processed_chunks = []
    all_tags = set()
    
    for chunk in chunks:
        chunk_data = self._process_single_chunk(chunk["text"])
        chunk.update({
            "sdg_tags": chunk_data["tags"],
            "keywords": chunk_data.get("keywords", []),
            "confidence_score": chunk_data.get("confidence_score", 0.0)
        })
        processed_chunks.append(chunk)
        all_tags.update(chunk_data["tags"])
    
    return {
        "chunks": processed_chunks,
        "combined_tags": list(all_tags),
        "total_chunks": len(processed_chunks),
        "embeddings": [chunk["embedding"] for chunk in processed_chunks]
    }


def extract_api_metadata(source_url: str, api_client: ApiClient) -> dict:
    """Extract metadata from various APIs based on URL patterns."""
    doi_match = re.search(r'(10\.\d{4,9}/[-._;()/:a-zA-Z0-9]+)', source_url)
    isbn_match = re.search(r'ISBN(-1[03])?:?\s+((978|979)[- ]?\d{1,5}[- ]?\d{1,7}[- ]?\d{1,6}[- ]?\d{1,1})', source_url)
    
    if doi_match:
        metadata = api_client.get_metadata_from_doi(doi_match.group(1))
        metadata['doi'] = doi_match.group(1)
        return metadata
    elif isbn_match:
        metadata = api_client.get_metadata_from_isbn(isbn_match.group(2))
        metadata['isbn'] = isbn_match.group(2)
        return metadata
    elif re.search(r'(undocs\.org|un\.org)', source_url):
        return api_client.get_metadata_from_un_digital_library("E/2023/1")
    elif re.search(r'(oecd-ilibrary\.org|stats\.oecd\.org)', source_url):
        return api_client.get_metadata_from_oecd("EDU_ENRL")
    elif re.search(r'(worldbank\.org|data\.worldbank\.org)', source_url):
        return api_client.get_metadata_from_world_bank("SP.POP.TOTL")
    
    return {}

def find_media_file(base_name: str, data_dir: str) -> str:
    """Find media file with given base name."""
    for ext in ['.mp3', '.txt', '.pdf', '.docx', '.csv']:
        potential_path = os.path.join(data_dir, f"{base_name}{ext}")
        if os.path.exists(potential_path):
            return potential_path
    return None

def extract_content(media_path: str, file_handler: FileHandler, processing_logic: ProcessingLogic) -> str:
    """Extract content from media file."""
    if media_path.endswith('.mp3'):
        return processing_logic.transcribe_audio(media_path)
    else:
        return file_handler.extract_text(media_path)

def save_backup(metadata: dict, text_content: str, processed_data: dict, base_name: str, processed_dir: str):
    """Save processed data as JSON backup."""
    backup_path = os.path.join(processed_dir, f"{base_name}_processed.json")
    backup_content = {
        "metadata": metadata,
        "text": text_content,
        "processed_data": processed_data
    }
    with open(backup_path, 'w', encoding='utf-8') as f:
        json.dump(backup_content, f, indent=4, ensure_ascii=False)
    logger.info(f"Backup saved: {backup_path}")

if __name__ == "__main__":
    run_processing_worker()
    

/sdg_root/src/data_processing/core

-- api_client.py

import requests
import json
import re
from typing import Dict, Any

class ApiClient:
    def __init__(self):
        self.headers = {'User-Agent': 'SDG-KI-Project/1.0 (info@example.com)'}

    def get_metadata_from_doi(self, doi: str) -> Dict[str, Any]:
        print(f"Abfrage der CrossRef-API für DOI: {doi}")
        api_url = f"https://api.crossref.org/works/{doi}"
        try:
            response = requests.get(api_url, headers=self.headers, timeout=10)
            response.raise_for_status()
            data = response.json()
            message = data.get('message', {})
            metadata = {
                'title': message.get('title', [])[0] if message.get('title') else None,
                'authors': ', '.join([author.get('given', '') + ' ' + author.get('family', '') for author in message.get('author', [])]),
                'publication_year': message.get('issued', {}).get('date-parts', [[None]]),
                'publisher': message.get('publisher'),
                'doi': message.get('DOI'),
                'keywords': ", ".join(message.get('subject', [])) if 'subject' in message else None,
                'abstract_original': message.get('abstract'),
                # ggf. weitere Felder parsen und zuweisen
            }
            return metadata
        except (requests.exceptions.RequestException, IndexError, KeyError) as e:
            print(f"Fehler bei der CrossRef-Abfrage für {doi}: {e}")
            return {}

    def get_metadata_from_isbn(self, isbn: str) -> Dict[str, Any]:
        print(f"Abfrage der Google Books API für ISBN: {isbn}")
        api_url = f"https://www.googleapis.com/books/v1/volumes?q=isbn:{isbn}"
        try:
            response = requests.get(api_url, headers=self.headers, timeout=10)
            response.raise_for_status()
            data = response.json()
            if data.get('totalItems') == 0:
                return {}
            item = data['items'][0]['volumeInfo']
            metadata = {
                'title': item.get('title'),
                'authors': ', '.join(item.get('authors', [])),
                'publisher': item.get('publisher'),
                'publication_year': item.get('publishedDate', 'Unknown').split('-'),
                'isbn': isbn,
                'abstract_original': item.get('description'),
                # ggf. weitere Felder parsen und zuweisen
            }
            return metadata
        except (requests.exceptions.RequestException, IndexError, KeyError) as e:
            print(f"Fehler bei der Google Books API-Abfrage für {isbn}: {e}")
            return {}

    def get_metadata_from_un_digital_library(self, query: str) -> Dict[str, Any]:
        print(f"Abfrage der UN Digital Library für Suchbegriff: {query}")
        api_url = f"https://digitallibrary.un.org/record?format=json&searchTerm={query}"
        try:
            response = requests.get(api_url, headers=self.headers, timeout=10)
            response.raise_for_status()
            data = response.json()
            if data and 'results' in data:
                first_result = data['results'][0]['value']
                metadata = {
                    'title': first_result.get('title'),
                    'authors': first_result.get('authors_names'),
                    'publication_year': first_result.get('publication_date'),
                    'source_url': first_result.get('url'),
                    # ggf. weitere Felder
                }
                return metadata
            return {}
        except (requests.exceptions.RequestException, IndexError, KeyError) as e:
            print(f"Fehler bei der UN Digital Library Abfrage: {e}")
            return {}

    def get_metadata_from_oecd(self, dataset_id: str) -> Dict[str, Any]:
        print(f"Abfrage der OECD API für Dataset: {dataset_id}")
        api_url = f"https://sdmx.oecd.org/public/rest/data/OECD.SDD.NAD,{dataset_id}@DF_NAAG_I?format=jsondata"
        try:
            response = requests.get(api_url, headers=self.headers, timeout=10)
            response.raise_for_status()
            data = response.json()
            metadata = {
                'title': f"OECD Dataset: {dataset_id}",
                'publisher': "OECD",
                'source_url': api_url
            }
            return metadata
        except (requests.exceptions.RequestException, IndexError, KeyError) as e:
            print(f"Fehler bei der OECD API-Abfrage: {e}")
            return {}

    def get_metadata_from_world_bank(self, query: str) -> Dict[str, Any]:
        print(f"Abfrage der Weltbank API für Suchbegriff: {query}")
        api_url = f"https://search.worldbank.org/api/v3/wds?format=json&qterm={query}"
        try:
            response = requests.get(api_url, headers=self.headers, timeout=10)
            response.raise_for_status()
            data = response.json()
            if data and 'documents' in data:
                first_result = data['documents'][0]
                metadata = {
                    'title': first_result.get('title'),
                    'authors': first_result.get('authors_names'),
                    'publication_year': first_result.get('pub_date'),
                    'source_url': first_result.get('url')
                }
                return metadata
            return {}
        except (requests.exceptions.RequestException, IndexError, KeyError) as e:
            print(f"Fehler bei der Weltbank API-Abfrage: {e}")
            return {}



-- db_utils.py

import os
import time
import json
import weaviate
from sqlalchemy import create_engine, text, MetaData, Table
from sqlalchemy.exc import OperationalError, IntegrityError
from sqlalchemy.orm import sessionmaker
from typing import Dict, Any, List, Optional, Union
import logging
from datetime import datetime
import hashlib

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

DATABASE_URL = os.environ.get("DATABASE_URL")
WEAVIATE_URL = os.environ.get("WEAVIATE_URL", "http://weaviate_service:8080")

def get_database_engine():
    """Create and return SQLAlchemy engine with connection pooling"""
    engine_kwargs = {
        "pool_pre_ping": True,
        "pool_recycle": 300,
        "pool_size": 10,
        "max_overflow": 20,
        "echo": False  # Set to True for SQL debugging
    }
    
    return create_engine(DATABASE_URL, **engine_kwargs)

def get_weaviate_client():
    """Create and return Weaviate client instance"""
    try:
        client = weaviate.Client(url=WEAVIATE_URL)
        
        try:
            client.schema.get("ArticleVector")
        except weaviate.exceptions.UnexpectedStatusCodeException:
            _create_weaviate_schema(client)
        
        return client
    except Exception as e:
        logger.error(f"Failed to connect to Weaviate: {e}")
        raise ConnectionError(f"Weaviate connection failed: {e}")

def _create_weaviate_schema(client):
    class_obj = {
        "class": "ArticleVector",
        "description": "SDG article embeddings for semantic search",
        "vectorizer": "none",  # We provide our own vectors
        "properties": [
            {
                "name": "text",
                "dataType": ["text"],
                "description": "Article text content"
            },
            {
                "name": "articleId", 
                "dataType": ["int"],
                "description": "Reference to article ID in PostgreSQL"
            },
            {
                "name": "chunkId",
                "dataType": ["int"], 
                "description": "Chunk ID for long documents"
            },
            {
                "name": "sdgGoals",
                "dataType": ["int[]"],
                "description": "Related SDG goals"
            },
            {
                "name": "language",
                "dataType": ["text"],
                "description": "Content language"
            },
            {
                "name": "region",
                "dataType": ["text"],
                "description": "Geographic region"
            }
        ]
    }
    
    client.schema.create_class(class_obj)
    logger.info("Created ArticleVector schema in Weaviate")

def save_to_database(
    metadata: Dict[str, Any],
    text_content: str,
    embeddings: List[float],
    chunks_data: Optional[List[Dict]] = None
):
    
    engine = get_database_engine()
    article_id = None
    
    try:
        with engine.begin() as connection:
            
            article_id = _insert_article(connection, metadata, text_content)
            
            if chunks_data:
                _insert_article_chunks(connection, article_id, chunks_data)
            
            _insert_tag_relationships(connection, article_id, metadata)
            
            _insert_ai_topic_relationships(connection, article_id, metadata)
            
            _insert_sdg_target_relationships(connection, article_id, metadata)
            
            logger.info(f"✅ Article {article_id} saved to PostgreSQL successfully")
            
        _save_to_weaviate(article_id, text_content, embeddings, chunks_data, metadata)
        
        return article_id
        
    except Exception as e:
        logger.error(f"❌ Error saving to database: {e}")
        if article_id:
            logger.error(f"Article ID {article_id} may be partially saved")
        raise

def _insert_article(connection, metadata: Dict[str, Any], text_content: str) -> int:
    
    publication_year = _extract_publication_year(metadata)
    
    insert_query = text("""
        INSERT INTO articles (
            title, content_original, content_english, summary, keywords, sdg_id, 
            authors, publication_year, publication_date, publisher, doi, isbn, 
            region, country_code, language, context, study_type, research_methods, 
            data_sources, funding, funding_info, bias_indicators, abstract_original, 
            abstract_english, relevance_questions, source_url, availability, 
            citation_count, impact_metrics, impact_factor, policy_impact,
            word_count, content_quality_score, created_at
        ) VALUES (
            :title, :content_original, :content_english, :summary, :keywords, :sdg_id,
            :authors, :publication_year, :publication_date, :publisher, :doi, :isbn,
            :region, :country_code, :language, :context, :study_type, :research_methods,
            :data_sources, :funding, :funding_info, :bias_indicators, :abstract_original,
            :abstract_english, :relevance_questions, :source_url, :availability,
            :citation_count, :impact_metrics, :impact_factor, :policy_impact,
            :word_count, :content_quality_score, NOW()
        ) RETURNING id
    """)
    
    content_quality_score = _calculate_content_quality_score(metadata, text_content)
    
    params = {
        "title": metadata.get('title', 'Untitled')[:500],  
        "content_original": text_content,
        "content_english": metadata.get('content_english') or text_content,
        "summary": metadata.get('summary'),
        "keywords": metadata.get('keywords'),
        "sdg_id": metadata.get('sdg_id'),
        "authors": metadata.get('authors'),
        "publication_year": publication_year,
        "publication_date": _parse_publication_date(metadata.get('publication_date')),
        "publisher": metadata.get('publisher'),
        "doi": metadata.get('doi'),
        "isbn": metadata.get('isbn'),
        "region": metadata.get('region'),
        "country_code": metadata.get('country_code'),
        "language": metadata.get('language', 'en'),
        "context": metadata.get('context'),
        "study_type": metadata.get('study_type'),
        "research_methods": metadata.get('research_methods'),
        "data_sources": metadata.get('data_sources'),
        "funding": metadata.get('funding'),
        "funding_info": metadata.get('funding_info'),
        "bias_indicators": metadata.get('bias_indicators'),
        "abstract_original": metadata.get('abstract_original'),
        "abstract_english": metadata.get('abstract_english'),
        "relevance_questions": metadata.get('relevance_questions'),
        "source_url": metadata.get('source_url'),
        "availability": metadata.get('availability'),
        "citation_count": metadata.get('citation_count', 0),
        "impact_metrics": json.dumps(metadata.get('impact_metrics')) if metadata.get('impact_metrics') else None,
        "impact_factor": metadata.get('impact_factor'),
        "policy_impact": metadata.get('policy_impact'),
        "word_count": len(text_content.split()) if text_content else 0,
        "content_quality_score": content_quality_score
    }
    
    result = connection.execute(insert_query, params)
    return result.scalar_one()

def _insert_article_chunks(connection, article_id: int, chunks_data: List[Dict]):

    chunk_query = text("""
        INSERT INTO article_chunks (
            article_id, chunk_id, chunk_order, text, chunk_length, 
            sdg_section, sub_section_id, sdg_relevance_scores, 
            confidence_score, created_at
        ) VALUES (
            :article_id, :chunk_id, :chunk_order, :text, :chunk_length,
            :sdg_section, :sub_section_id, :sdg_relevance_scores,
            :confidence_score, NOW()
        )
    """)
    
    for i, chunk_data in enumerate(chunks_data):
        chunk_params = {
            "article_id": article_id,
            "chunk_id": chunk_data.get("chunk_id", i),
            "chunk_order": i,
            "text": chunk_data["text"],
            "chunk_length": len(chunk_data["text"]),
            "sdg_section": chunk_data.get("sdg_section", "general"),
            "sub_section_id": chunk_data.get("sub_section_id"),
            "sdg_relevance_scores": json.dumps(chunk_data.get("sdg_relevance_scores")) if chunk_data.get("sdg_relevance_scores") else None,
            "confidence_score": chunk_data.get("confidence_score", 0.0)
        }
        
        connection.execute(chunk_query, chunk_params)
    
    logger.info(f"Inserted {len(chunks_data)} chunks for article {article_id}")

def _insert_tag_relationships(connection, article_id: int, metadata: Dict[str, Any]):
    
    tags = metadata.get('tags', [])
    if not tags:
        return
    
    for tag_name in tags:
        if not tag_name or not tag_name.strip():
            continue
            
        tag_name = tag_name.strip()[:100]  
        
        tag_id = connection.execute(
            text("SELECT id FROM tags WHERE name = :name"),
            {"name": tag_name}
        ).scalar_one_or_none()
        
        if not tag_id:
            tag_id = connection.execute(
                text("""
                    INSERT INTO tags (name, category, usage_count) 
                    VALUES (:name, :category, 1) 
                    RETURNING id
                """),
                {"name": tag_name, "category": "general"}
            ).scalar_one()
        else:
            connection.execute(
                text("UPDATE tags SET usage_count = usage_count + 1 WHERE id = :tag_id"),
                {"tag_id": tag_id}
            )
        
        try:
            connection.execute(
                text("""
                    INSERT INTO articles_tags (article_id, tag_id) 
                    VALUES (:article_id, :tag_id)
                    ON CONFLICT (article_id, tag_id) DO NOTHING
                """),
                {"article_id": article_id, "tag_id": tag_id}
            )
        except IntegrityError:
            pass 

def _insert_ai_topic_relationships(connection, article_id: int, metadata: Dict[str, Any]):
    """Insert article-AI topic relationships"""
    ai_topics = metadata.get('ai_topics', [])
    if not ai_topics:
        return
    
    for topic_name in ai_topics:
        if not topic_name or not topic_name.strip():
            continue
            
        topic_name = topic_name.strip()[:100]
        
        topic_id = connection.execute(
            text("SELECT id FROM ai_topics WHERE name = :name"),
            {"name": topic_name}
        ).scalar_one_or_none()
        
        if not topic_id:
            topic_id = connection.execute(
                text("""
                    INSERT INTO ai_topics (name, category, created_at) 
                    VALUES (:name, :category, NOW()) 
                    RETURNING id
                """),
                {"name": topic_name, "category": "general"}
            ).scalar_one()
        
        try:
            connection.execute(
                text("""
                    INSERT INTO articles_ai_topics (article_id, ai_topic_id) 
                    VALUES (:article_id, :topic_id)
                    ON CONFLICT (article_id, ai_topic_id) DO NOTHING
                """),
                {"article_id": article_id, "topic_id": topic_id}
            )
        except IntegrityError:
            pass

def _insert_sdg_target_relationships(connection, article_id: int, metadata: Dict[str, Any]):
    """Insert article-SDG target relationships"""
    sdg_goals = metadata.get('sdg_goals', [])
    if not sdg_goals:
        return
    
    for goal_id in sdg_goals:
        sdg_ids = []
    
    # Primary SDG from sdg_id field
    if metadata.get('sdg_id'):
        sdg_ids.append(metadata['sdg_id'])
    
    # Additional SDGs from sdg_goals list
    if metadata.get('sdg_goals'):
        if isinstance(metadata['sdg_goals'], list):
            sdg_ids.extend(metadata['sdg_goals'])
        elif isinstance(metadata['sdg_goals'], int):
            sdg_ids.append(metadata['sdg_goals'])
    
    # Remove duplicates and ensure valid range
    sdg_ids = list(set([sdg for sdg in sdg_ids if isinstance(sdg, int) and 1 <= sdg <= 17]))
    
    for sdg_id in sdg_ids:
        confidence_score = metadata.get('sdg_confidence', 0.8)  # Default confidence
        
        connection.execute(
            text("""
                INSERT INTO articles_sdg_targets (article_id, sdg_id, confidence_score) 
                VALUES (:article_id, :sdg_id, :confidence_score) 
                ON CONFLICT (article_id, sdg_id) DO UPDATE SET confidence_score = :confidence_score
            """),
            {
                "article_id": article_id,
                "sdg_id": sdg_id,
                "confidence_score": confidence_score
            }
        )

def _save_to_weaviate(article_id: int, text_content: str, embeddings: List[float], 
                     chunks_data: Optional[List[Dict]] = None, metadata: Dict[str, Any] = None):
    """Save embeddings to Weaviate vector database"""
    try:
        client = get_weaviate_client()
        
        # Ensure ArticleVector schema exists
        try:
            client.schema.get("ArticleVector")
        except weaviate.exceptions.UnexpectedStatusCodeException:
            _create_weaviate_schema(client)
        
        if chunks_data and len(chunks_data) > 0:
            # Save each chunk as separate vector
            for i, chunk_data in enumerate(chunks_data):
                chunk_vector = chunk_data.get("embedding") or embeddings
                
                vector_data = {
                    "text": chunk_data["text"],
                    "articleId": article_id,
                    "chunkId": i,
                    "sdgGoals": metadata.get('sdg_goals', []) if metadata else [],
                    "language": metadata.get('language', 'en') if metadata else 'en',
                    "region": metadata.get('region', '') if metadata else ''
                }
                
                client.data_object.create(
                    data_object=vector_data,
                    class_name="ArticleVector",
                    vector=chunk_vector
                )
        else:
            # Save full document as single vector
            vector_data = {
                "text": text_content,
                "articleId": article_id,
                "chunkId": 0,
                "sdgGoals": metadata.get('sdg_goals', []) if metadata else [],
                "language": metadata.get('language', 'en') if metadata else 'en',
                "region": metadata.get('region', '') if metadata else ''
            }
            
            client.data_object.create(
                data_object=vector_data,
                class_name="ArticleVector",
                vector=embeddings
            )
        
        logger.info(f"✅ Vector data for article {article_id} saved to Weaviate")
        
    except Exception as e:
        logger.error(f"❌ Error saving to Weaviate: {e}")
        raise

def get_article_by_id(article_id: int) -> Optional[Dict[str, Any]]:
    """Retrieve article by ID with related data"""
    engine = get_database_engine()
    
    try:
        with engine.connect() as connection:
            
            article_query = text("""
                SELECT a.*, s.name as sdg_name
                FROM articles a
                LEFT JOIN sdgs s ON a.sdg_id = s.id
                WHERE a.id = :article_id
            """)
            
            result = connection.execute(article_query, {"article_id": article_id}).fetchone()
            if not result:
                return None
            
            article = dict(result._mapping)
            
            chunks_query = text("""
                SELECT * FROM article_chunks 
                WHERE article_id = :article_id 
                ORDER BY chunk_order
            """)
            chunks = connection.execute(chunks_query, {"article_id": article_id}).fetchall()
            article["chunks"] = [dict(chunk._mapping) for chunk in chunks]
            
            tags_query = text("""
                SELECT t.name FROM tags t
                JOIN articles_tags at ON t.id = at.tag_id
                WHERE at.article_id = :article_id
            """)
            tags = connection.execute(tags_query, {"article_id": article_id}).fetchall()
            article["tags"] = [tag.name for tag in tags]
            
            return article
            
    except Exception as e:
        logger.error(f"Error retrieving article {article_id}: {e}")
        return None

def search_similar_content(
    query_embedding: List[float], 
    limit: int = 10,
    sdg_filter: Optional[List[int]] = None
) -> List[Dict[str, Any]]:
    """Search for similar content using Weaviate"""
    try:
        client = get_weaviate_client()
        
        query_builder = (
            client.query
            .get("ArticleVector", ["text", "articleId", "sdgGoals", "language", "region"])
            .with_near_vector({
                "vector": query_embedding,
                "certainty": 0.7
            })
            .with_limit(limit)
            .with_additional(["certainty", "distance"])
        )
        
        if sdg_filter:
            where_filter = {
                "operator": "ContainsAny",
                "path": ["sdgGoals"],
                "valueIntArray": sdg_filter
            }
            query_builder = query_builder.with_where(where_filter)
        
        result = query_builder.do()
        
        return result.get("data", {}).get("Get", {}).get("ArticleVector", [])
        
    except Exception as e:
        logger.error(f"Error in similarity search: {e}")
        return []

def _extract_publication_year(metadata: Dict[str, Any]) -> Optional[int]:
    """Extract publication year from various metadata fields"""
    year_fields = ['publication_year', 'year', 'published_year']
    
    for field in year_fields:
        year_value = metadata.get(field)
        if year_value:
            try:
                if isinstance(year_value, list) and len(year_value) > 0:
                    year_value = year_value[0]
                if isinstance(year_value, str):
                    # Extract year from date string like "2023-01-01"
                    year_match = re.match(r'(\d{4})', year_value)
                    if year_match:
                        return int(year_match.group(1))
                elif isinstance(year_value, int):
                    if 1900 <= year_value <= 2030:  # Reasonable year range
                        return year_value
            except (ValueError, TypeError):
                continue
    
    return None

def _parse_publication_date(date_string: str) -> Optional[datetime]:
    """Parse publication date from string"""
    if not date_string:
        return None
        
    try:
        # Try different date formats
        date_formats = [
            '%Y-%m-%d',
            '%Y-%m-%d %H:%M:%S',
            '%Y-%m-%dT%H:%M:%S',
            '%Y-%m-%dT%H:%M:%SZ',
            '%Y/%m/%d',
            '%d/%m/%Y',
            '%Y'
        ]
        
        for fmt in date_formats:
            try:
                return datetime.strptime(str(date_string), fmt)
            except ValueError:
                continue
                
        # If none of the formats work, try to extract year
        year_match = re.search(r'(\d{4})', str(date_string))
        if year_match:
            year = int(year_match.group(1))
            if 1900 <= year <= 2030:
                return datetime(year, 1, 1)
                
    except Exception as e:
        logger.warning(f"Could not parse date '{date_string}': {e}")
    
    return None

def _calculate_content_quality_score(metadata: Dict[str, Any], text_content: str) -> float:

    score = 0.0
    
    # Title quality (0-0.15)
    title = metadata.get('title', '')
    if title and len(title.strip()) > 10:
        score += 0.15
    elif title and len(title.strip()) > 5:
        score += 0.08
    
    # Content length (0-0.25)
    content_length = len(text_content.strip())
    if content_length > 5000:
        score += 0.25
    elif content_length > 2000:
        score += 0.20
    elif content_length > 500:
        score += 0.10
    elif content_length > 100:
        score += 0.05
    
    # Has authors (0-0.10)
    if metadata.get('authors'):
        score += 0.10
    
    # Has publication info (0-0.10)
    if metadata.get('publication_year') or metadata.get('publisher'):
        score += 0.10
    
    # Has DOI or ISBN (0-0.10)
    if metadata.get('doi') or metadata.get('isbn'):
        score += 0.10
    
    # Has abstract (0-0.10)
    if metadata.get('abstract_original') or metadata.get('abstract_english'):
        score += 0.10
    
    # Has source URL (0-0.05)
    if metadata.get('source_url'):
        score += 0.05
    
    # Has keywords or tags (0-0.05)
    if metadata.get('keywords') or metadata.get('tags'):
        score += 0.05
    
    # Has SDG classification (0-0.10)
    if metadata.get('sdg_id') or metadata.get('sdg_goals'):
        score += 0.10
    
    return min(score, 1.0)


def batch_save_to_database(items: List[Dict[str, Any]], batch_size: int = 50):
    """
    Batch save multiple items to database for better performance
    """
    engine = get_database_engine()
    successful_saves = 0
    failed_saves = 0
    
    for i in range(0, len(items), batch_size):
        batch = items[i:i + batch_size]
        
        try:
            with engine.begin() as connection:
                for item in batch:
                    metadata = item.get('metadata', {})
                    text_content = item.get('text_content', '')
                    embeddings = item.get('embeddings', [])
                    chunks_data = item.get('chunks_data')
                    
                    try:
                        article_id = _insert_article(connection, metadata, text_content)
                        
                        if chunks_data:
                            _insert_article_chunks(connection, article_id, chunks_data)
                        
                        _insert_tag_relationships(connection, article_id, metadata)
                        _insert_ai_topic_relationships(connection, article_id, metadata)
                        _insert_sdg_target_relationships(connection, article_id, metadata)
                        
                        # Save to Weaviate in separate transaction
                        _save_to_weaviate(article_id, text_content, embeddings, chunks_data, metadata)
                        
                        successful_saves += 1
                        
                    except Exception as e:
                        logger.error(f"Error saving individual item: {e}")
                        failed_saves += 1
                        continue
                        
        except Exception as e:
            logger.error(f"Error in batch save: {e}")
            failed_saves += len(batch)
    
    logger.info(f"Batch save completed: {successful_saves} successful, {failed_saves} failed")
    return {"successful": successful_saves, "failed": failed_saves}

def cleanup_old_data(retention_days: int = 30):
    """
    Cleanup old data from database based on retention policy
    """
    engine = get_database_engine()
    
    try:
        with engine.begin() as connection:
            cutoff_date = datetime.now() - timedelta(days=retention_days)
            
            # Delete old articles (this will cascade to related tables)
            result = connection.execute(
                text("DELETE FROM articles WHERE created_at < :cutoff_date"),
                {"cutoff_date": cutoff_date}
            )
            
            deleted_count = result.rowcount
            logger.info(f"Cleaned up {deleted_count} old articles older than {retention_days} days")
            
            return deleted_count
            
    except Exception as e:
        logger.error(f"Error during cleanup: {e}")
        raise

def get_database_statistics() -> Dict[str, Any]:
    """Get database statistics"""
    engine = get_database_engine()
    
    try:
        with engine.connect() as connection:
            stats = {}
            
            article_stats = connection.execute(text("""
                SELECT 
                    COUNT(*) as total_articles,
                    COUNT(*) FILTER (WHERE has_embeddings = TRUE) as articles_with_embeddings,
                    AVG(content_quality_score) as avg_quality_score,
                    COUNT(DISTINCT language) as languages,
                    COUNT(DISTINCT region) as regions
                FROM articles
            """)).fetchone()
            
            stats.update(dict(article_stats._mapping))
            
            sdg_stats = connection.execute(text("""
                SELECT 
                    s.name,
                    COUNT(a.id) as article_count
                FROM sdgs s
                LEFT JOIN articles a ON s.id = a.sdg_id
                GROUP BY s.id, s.name
                ORDER BY article_count DESC
            """)).fetchall()
            
            stats["sdg_distribution"] = [dict(row._mapping) for row in sdg_stats]
            
            return stats
            
    except Exception as e:
        logger.error(f"Error getting statistics: {e}")
        return {}

def get_database_health() -> Dict[str, Any]:
    """
    Check database health and return status information
    """
    try:
        engine = get_database_engine()
        
        with engine.connect() as connection:
            # Test basic connectivity
            connection.execute(text("SELECT 1"))
            
            # Get table counts
            tables = ['articles', 'sdgs', 'actors', 'tags', 'ai_topics', 'article_chunks']
            table_counts = {}
            
            for table in tables:
                try:
                    result = connection.execute(text(f"SELECT COUNT(*) FROM {table}"))
                    table_counts[table] = result.scalar()
                except Exception as e:
                    table_counts[table] = f"Error: {e}"
            
            # Check Weaviate health
            try:
                weaviate_client = get_weaviate_client()
                weaviate_ready = weaviate_client.is_ready()
            except Exception as e:
                weaviate_ready = False
                logger.error(f"Weaviate health check failed: {e}")
            
            return {
                "database_status": "healthy",
                "weaviate_status": "healthy" if weaviate_ready else "unhealthy",
                "table_counts": table_counts,
                "timestamp": datetime.now().isoformat()
            }
            
    except Exception as e:
        return {
            "database_status": "unhealthy",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }



-- file_handler.py

import os
import csv
from docx import Document
import PyPDF2
from PIL import Image
import pytesseract
import fitz  # PyMuPDF
import datetime

class FileHandler:
    def __init__(self, images_dir):
        self.images_dir = images_dir
        os.makedirs(self.images_dir, exist_ok=True)

    def _extract_images_from_pdf(self, file_path: str, doc_name: str) -> list:
        """Extrahiert Bilder aus einer PDF-Datei und speichert sie."""
        image_paths = []
        try:
            pdf_document = fitz.open(file_path)
            for page_index in range(len(pdf_document)):
                page = pdf_document[page_index]
                image_list = page.get_images(full=True)
                for image_index, img in enumerate(image_list, start=1):
                    xref = img[0]
                    base_image = pdf_document.extract_image(xref)
                    image_bytes = base_image["image"]
                    file_extension = base_image["ext"]
                    image_path = os.path.join(self.images_dir, f"{doc_name}_page{page_index+1}_img{image_index}.{file_extension}")
                    with open(image_path, "wb") as f:
                        f.write(image_bytes)
                    image_paths.append({
                        "original_path": image_path,
                        "page": page_index + 1,
                        "caption": None,  # Optional bei OCR/Meta extrahieren!
                        "sdg_tags": {},
                        "ai_tags": None,
                        "image_type": file_extension
                    })
        except Exception as e:
            print(f"Fehler beim Extrahieren von Bildern aus PDF: {e}")
        return image_paths

    def _extract_images_from_docx(self, file_path: str, doc_name: str) -> list:
        """Extrahiert Bilder aus einer DOCX-Datei und speichert sie."""
        image_paths = []
        try:
            document = Document(file_path)
            for rel in document.part.rels:
                if "image" in document.part.rels[rel].target_ref:
                    image_part = document.part.rels[rel].target_part
                    image_path = os.path.join(self.images_dir, f"{doc_name}_{os.path.basename(image_part.partname)}")
                    with open(image_path, "wb") as f:
                        f.write(image_part.blob)
                    image_paths.append({
                        "original_path": image_path,
                        "page": None,
                        "caption": None,
                        "sdg_tags": {},
                        "ai_tags": None,
                        "image_type": os.path.splitext(image_path)[-1].replace('.', '')
                    })
        except Exception as e:
            print(f"Fehler beim Extrahieren von Bildern aus DOCX: {e}")
        return image_paths

    def get_text_from_file(self, file_path: str) -> str:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()

    def extract_text_from_pdf(self, file_path: str) -> tuple:
        text = ""
        metadata = {}
        try:
            with open(file_path, 'rb') as f:
                pdf_reader = PyPDF2.PdfReader(f)
                doc_info = pdf_reader.metadata
                if doc_info:
                    metadata['title'] = doc_info.get('/Title', None)
                    metadata['authors'] = doc_info.get('/Author', None)
                    metadata['creation_date'] = doc_info.get('/CreationDate', None)
                for page in pdf_reader.pages:
                    text += page.extract_text()
        except Exception as e:
            print(f"Fehler beim Extrahieren von Text oder Metadaten aus PDF: {e}")
        return text, metadata

    def extract_text_from_docx(self, file_path: str) -> str:
        text = ""
        try:
            doc = Document(file_path)
            for para in doc.paragraphs:
                text += para.text + "\n"
        except Exception as e:
            print(f"Fehler beim Extrahieren von Text aus DOCX: {e}")
        return text

    def extract_text_from_csv(self, file_path: str) -> str:
        text = ""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                csv_reader = csv.reader(f)
                for row in csv_reader:
                    text += " ".join(row) + "\n"
        except Exception as e:
            print(f"Fehler beim Extrahieren von Text aus CSV: {e}")
        return text

    def extract_text(self, file_path: str) -> str:
        if file_path.endswith('.mp3'):
            return ""
        elif file_path.endswith('.pdf'):
            text, _ = self.extract_text_from_pdf(file_path)
            return text
        elif file_path.endswith('.docx'):
            return self.extract_text_from_docx(file_path)
        elif file_path.endswith('.csv'):
            return self.extract_text_from_csv(file_path)
        else:
            return self.get_text_from_file(file_path)

    def get_metadata_from_json(self, file_path: str) -> dict:
        import json
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (FileNotFoundError, json.JSONDecodeError):
            return {}

    def cleanup_processed_data(self, directory: str, retention_days: int):
        """Löscht Dateien in einem Verzeichnis, die älter als retention_days sind."""
        print(f"Starte Bereinigung von {directory}...")
        now = datetime.datetime.now()
        for filename in os.listdir(directory):
            filepath = os.path.join(directory, filename)
            if os.path.isfile(filepath):
                file_creation_time = datetime.datetime.fromtimestamp(os.path.getctime(filepath))
                if (now - file_creation_time).days > retention_days:
                    print(f"Lösche alte Datei: {filepath}")
                    os.remove(filepath)
        print("Bereinigung abgeschlossen.")

        
-- keywords.py

sdg_keywords_dict = {
    "SDG 1 No Poverty": [
        "no poverty", "end poverty", "armut beenden", "keine Armut",
        "aucune pauvreté", "erradicar la pobreza", "消除贫困"
    ],
    "SDG 2 Zero Hunger": [
        "zero hunger", "keinen hunger", "hunger beenden", "faim zéro",
        "hambre cero", "消除饥饿"
    ],
    "SDG 3 Good Health and Well-Being": [
        "good health", "gesundheit und wohlbefinden", "bonne santé", 
        "buena salud", "健康和福祉", "well-being", "bien-être", "bienestar"
    ],
    "SDG 4 Quality Education": [
        "quality education", "bildung von qualität", "éducation de qualité",
        "educación de calidad", "优质教育"
    ],
    "SDG 5 Gender Equality": [
        "gender equality", "gleichstellung der geschlechter", "égalité des sexes",
        "igualdad de género", "性别平等"
    ],
    "SDG 6 Clean Water and Sanitation": [
        "clean water", "sauberes wasser und sanitärversorgung", "eau propre",
        "agua limpia", "清洁饮水和卫生设施", "sanitation", "sanitäre einrichtungen", "assainissement"
    ],
    "SDG 7 Affordable and Clean Energy": [
        "clean energy", "bezahlbare und saubere energie", "énergie propre",
        "energía asequible y no contaminante", "可负担的清洁能源"
    ],
    "SDG 8 Decent Work and Economic Growth": [
        "decent work", "menschenwürdige arbeit", "travail décent",
        "trabajo decente", "体面工作", "economic growth", "wirtschaftswachstum", "croissance économique"
    ],
    "SDG 9 Industry, Innovation and Infrastructure": [
        "industry", "innovation", "industrie", "innovation", "infrastructure",
        "industrie, innovation et infrastructure", "industria, innovación e infraestructura", "产业创新和基础设施"
    ],
    "SDG 10 Reduced Inequalities": [
        "reduced inequalities", "ungleichheiten verringern", "réduction des inégalités",
        "reducción de las desigualdades", "减少不平等"
    ],
    "SDG 11 Sustainable Cities and Communities": [
        "sustainable cities", "nachhaltige städte", "villes durables",
        "ciudades sostenibles", "可持续城市", "communities", "gemeinschaften", "communautés"
    ],
    "SDG 12 Responsible Consumption and Production": [
        "responsible consumption", "verantwortungsvoller konsum", "consommation responsable",
        "consumo responsable", "负责任的消费和生产", "production", "produktion", "production"
    ],
    "SDG 13 Climate Action": [
        "climate action", "klimaschutz", "action climatique",
        "acción por el clima", "气候行动"
    ],
    "SDG 14 Life Below Water": [
        "life below water", "leben unter wasser", "vie aquatique",
        "vida submarina", "水下生物"
    ],
    "SDG 15 Life on Land": [
        "life on land", "leben an land", "vie terrestre",
        "vida de ecosistemas terrestres", "陆地生物"
    ],
    "SDG 16 Peace, Justice and Strong Institutions": [
        "peace", "justice", "friedliche gesellschaften", "paix", "justice", 
        "paz", "justicia", "和平、正义与强大机构", "strong institutions", "starke institutionen", "institutions fortes"
    ],
    "SDG 17 Partnerships for the Goals": [
        "partnerships", "partnerschaften zum erreichen der ziele", "partenariats",
        "alianzas", "可持续发展伙伴关系"
    ]
}


ai_keywords_dict = {
    "AI Governance & Policy": [
        "ai governance", "algorithmic regulation", "policy framework", "ki governance"
    ],
    "Ethics & Trustworthy AI": [
        "ai ethics", "responsible ai", "trustworthy ai", "explainable ai", "fairness", "bias mitigation"
    ],
    "Machine Learning": [
        "machine learning", "deep learning", "supervised learning", "unsupervised learning"
    ],
    "Natural Language Processing": [
        "nlp", "natural language processing", "sprachverarbeitung"
    ],
    "Computer Vision": [
        "computer vision", "object detection", "image recognition"
    ],
    "Robotics & Autonomous Systems": [
        "robotics", "autonomous vehicles", "drones"
    ],
    "Data & Big Data Analytics": [
        "big data", "data analytics", "data governance", "open data"
    ],
    "AI for Social Good": [
        "ai for good", "ai for sdgs", "humanitarian ai"
    ]
}


generic_keywords_dict = {
    "philosophie": ["philosophie", "philosophisch", "denken"],
    "gesellschaft": ["gesellschaft", "sozial", "politik", "soziologie"]
}

-- processing_logic.py

# /sdg_root/src/data_processing/core/processing_logic.py

import re
from typing import Dict, Any, List
from .text_chunker import SDGTextChunker
from .keywords import sdg_keywords_dict, ai_keywords_dict
from deep_translator import GoogleTranslator
import pytesseract
from PIL import Image
import logging

logger = logging.getLogger(__name__)

class ProcessingLogic:
    def __init__(self, whisper_model, sentence_model):
        self.whisper_model = whisper_model
        self.sentence_model = sentence_model
        self.text_chunker = SDGTextChunker(chunk_size=512, overlap=50)
        self.translator = GoogleTranslator()
    
    def process_text_for_ai(self, text_content: str) -> Dict[str, Any]:
        """Process text for AI analysis - single chunk version."""
        embeddings = self.sentence_model.encode(text_content).tolist()
        
        tags = self._extract_tags(text_content)
        
        extracted_info = self.extract_abstract_and_keywords(text_content)
        
        return {
            "embeddings": embeddings,
            "tags": tags,
            "keywords": extracted_info.get('keywords', []),
            "abstract": extracted_info.get('abstract', ''),
            "language": self._detect_language(text_content)
        }
    
    def process_text_for_ai_with_chunking(self, text_content: str) -> Dict[str, Any]:
        """Process text with chunking for large documents."""
        if len(text_content) <= 512:
            return self.process_text_for_ai(text_content)
        
        chunks = self.text_chunker.chunk_by_sdg_sections(text_content)
        chunks = self.text_chunker.generate_embeddings_for_chunks(chunks)
        
        processed_chunks = []
        all_tags = set()
        all_embeddings = []
        
        for chunk in chunks:
            chunk_data = self.process_text_for_ai(chunk["text"])
            chunk.update({
                "sdg_tags": chunk_data["tags"],
                "keywords": chunk_data.get("keywords", []),
                "abstract": chunk_data.get("abstract", "")
            })
            processed_chunks.append(chunk)
            all_tags.update(chunk_data["tags"])
            all_embeddings.extend(chunk_data["embeddings"])
        
        if all_embeddings:
            import numpy as np
            combined_embeddings = np.mean(
                np.array(all_embeddings).reshape(len(processed_chunks), -1), 
                axis=0
            ).tolist()
        else:
            combined_embeddings = []
        
        return {
            "chunks": processed_chunks,
            "combined_tags": list(all_tags),
            "combined_embeddings": combined_embeddings,
            "total_chunks": len(processed_chunks),
            "total_length": len(text_content)
        }
    
    def transcribe_audio(self, audio_path: str) -> str:
        """Transcribe audio file using Whisper."""
        try:
            segments, info = self.whisper_model.transcribe(audio_path)
            transcription = ""
            for segment in segments:
                transcription += segment.text + " "
            return transcription.strip()
        except Exception as e:
            logger.error(f"Error transcribing audio {audio_path}: {e}")
            return ""
    
    def _extract_tags(self, text: str) -> List[str]:
        """Extract SDG and AI tags from text."""
        text_lower = text.lower()
        tags = []
        
        # Extract SDG tags
        for sdg_name, keywords in sdg_keywords_dict.items():
            for keyword in keywords:
                if keyword.lower() in text_lower:
                    tags.append(sdg_name)
                    break
        
        # Extract AI tags
        for ai_name, keywords in ai_keywords_dict.items():
            for keyword in keywords:
                if keyword.lower() in text_lower:
                    tags.append(ai_name)
                    break
        
        return list(set(tags))
    
    def extract_abstract_and_keywords(self, text: str) -> Dict[str, Any]:
        """Extract abstract and keywords from text."""
        paragraphs = text.split('\n\n')
        abstract = paragraphs[0][:300] + "..." if len(paragraphs[0]) > 300 else paragraphs[0]

        words = re.findall(r'\b\w{4,}\b', text.lower())
        word_freq = {}
        for word in words:
            if word not in ['that', 'with', 'have', 'this', 'will', 'from', 'they', 'been', 'their']:
                word_freq[word] = word_freq.get(word, 0) + 1
        
        keywords = [word for word, freq in sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]]
        
        return {
            "abstract": abstract,
            "keywords": keywords
        }
    
    def _detect_language(self, text: str) -> str:
        """Enhanced language detection including Chinese and Hindi."""
        sample = text[:500].lower()  # Increased sample size for better detection
        
        # English indicators
        if any(word in sample for word in ['the', 'and', 'that', 'with', 'have', 'this', 'will', 'from']):
            return 'en'
        
        # German indicators
        elif any(word in sample for word in ['der', 'die', 'das', 'und', 'mit', 'eine', 'einer', 'auch']):
            return 'de'
        
        # French indicators
        elif any(word in sample for word in ['le', 'la', 'et', 'des', 'les', 'une', 'dans', 'pour']):
            return 'fr'
        
        # Spanish indicators
        elif any(word in sample for word in ['el', 'la', 'los', 'las', 'que', 'con', 'una', 'para']):
            return 'es'
        
        # Chinese indicators - check for Chinese characters
        elif any('\u4e00' <= char <= '\u9fff' for char in text[:200]):  # CJK Unified Ideographs
            return 'zh'
        
        # Hindi indicators - check for Devanagari script
        elif any('\u0900' <= char <= '\u097f' for char in text[:200]):  # Devanagari Unicode block
            return 'hi'
        
        # Additional Chinese detection (Traditional Chinese)
        elif any('\u3400' <= char <= '\u4dbf' for char in text[:200]):  # CJK Extension A
            return 'zh'
        
        # Hindi common words in Latin script (transliterated)
        elif any(word in sample for word in ['hai', 'hain', 'mein', 'aur', 'kya', 'koi', 'yeh', 'woh']):
            return 'hi'
        
        # Chinese common words in Pinyin (romanized)
        elif any(word in sample for word in ['shi', 'zai', 'you', 'wei', 'dui', 'gen', 'cong']):
            return 'zh'
        
        # Default to English if no clear detection
        else:
            return 'en'

    def detect_language_advanced(self, text: str) -> Dict[str, Any]:
        """
        Advanced language detection with confidence scores
        """
        sample = text[:1000]  
        
        language_patterns = {
            'en': {
                'common_words': ['the', 'and', 'that', 'with', 'have', 'this', 'will', 'from', 'they', 'been'],
                'weight': 0
            },
            'de': {
                'common_words': ['der', 'die', 'das', 'und', 'mit', 'eine', 'einer', 'auch', 'sich', 'aber'],
                'weight': 0
            },
            'fr': {
                'common_words': ['le', 'la', 'et', 'des', 'les', 'une', 'dans', 'pour', 'qui', 'avec'],
                'weight': 0
            },
            'es': {
                'common_words': ['el', 'la', 'los', 'las', 'que', 'con', 'una', 'para', 'por', 'como'],
                'weight': 0
            },
            'zh': {
                'common_words': [],  # Will use character detection
                'weight': 0
            },
            'hi': {
                'common_words': ['hai', 'hain', 'mein', 'aur', 'kya', 'koi', 'yeh', 'woh', 'iska', 'jab'],
                'weight': 0
            }
        }
        
        sample_lower = sample.lower()
        
        # Count word matches for Latin-script languages
        for lang_code, lang_data in language_patterns.items():
            if lang_code in ['zh', 'hi']:  # Skip for now, handle separately
                continue
            
            word_matches = sum(1 for word in lang_data['common_words'] if word in sample_lower)
            lang_data['weight'] = word_matches
        
        # Chinese character detection
        chinese_chars = sum(1 for char in sample if '\u4e00' <= char <= '\u9fff' or '\u3400' <= char <= '\u4dbf')
        if chinese_chars > 10:  # Threshold for Chinese detection
            language_patterns['zh']['weight'] = chinese_chars * 2  # Give higher weight
        
        # Hindi Devanagari script detection
        hindi_chars = sum(1 for char in sample if '\u0900' <= char <= '\u097f')
        if hindi_chars > 5:  # Threshold for Hindi detection
            language_patterns['hi']['weight'] = hindi_chars * 3  # Give higher weight
        else:
            # Check romanized Hindi words
            hindi_word_matches = sum(1 for word in language_patterns['hi']['common_words'] if word in sample_lower)
            language_patterns['hi']['weight'] = hindi_word_matches
        
        # Find language with highest weight
        detected_lang = max(language_patterns.keys(), key=lambda x: language_patterns[x]['weight'])
        confidence = language_patterns[detected_lang]['weight']
        
        # Normalize confidence score
        max_possible_score = len(sample.split()) * 0.1  # Rough estimate
        normalized_confidence = min(confidence / max(max_possible_score, 1), 1.0)
        
        return {
            'language': detected_lang,
            'confidence': normalized_confidence,
            'scores': {lang: data['weight'] for lang, data in language_patterns.items()},
            'method': 'advanced_pattern_matching'
        }
    
    def extract_abstract_and_keywords(self, text: str) -> Dict[str, Any]:
        """Extract abstract and keywords from text content"""
        
        # Detect language first
        language_info = self.detect_language_advanced(text)
        
        # Extract potential abstract (first few sentences)
        sentences = re.split(r'[.!?]', text)
        abstract_candidates = []
        
        for i, sentence in enumerate(sentences[:5]):  
            sentence = sentence.strip()
            if len(sentence) > 50 and len(sentence) < 500:  
                abstract_candidates.append(sentence)
        
        abstract = '. '.join(abstract_candidates[:3]) if abstract_candidates else text[:300]
        
        words = re.findall(r'\b[a-zA-Z]{4,}\b', text.lower())
        word_freq = {}
        
        stop_words = {
            'en': ['this', 'that', 'with', 'have', 'will', 'from', 'they', 'been', 'each', 'which'],
            'de': ['dass', 'sich', 'aber', 'auch', 'noch', 'nach', 'beim', 'dann', 'kann', 'wird'],
            'fr': ['dans', 'pour', 'avec', 'sont', 'plus', 'tout', 'cette', 'peut', 'comme', 'fait'],
            'es': ['para', 'como', 'este', 'esta', 'pero', 'todo', 'hace', 'muy', 'ahora', 'cada'],
            'zh': [],  # Chinese doesn't use space-separated words in the same way
            'hi': ['hain', 'kiya', 'jata', 'karne', 'hota', 'raha', 'gaya', 'kuch', 'baat', 'saat']
        }
        
        current_stop_words = stop_words.get(language_info['language'], stop_words['en'])
        
        for word in words:
            if word not in current_stop_words and len(word) > 3:
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # Get top keywords
        keywords = [word for word, freq in sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]]
        
        return {
            'abstract': abstract,
            'keywords': keywords,
            'language_info': language_info,
            'word_count': len(text.split()),
            'character_count': len(text)
        }

        
-- processing_worker.py

import os
import time
from .file_handler import FileHandler
from .processing_logic import ProcessingLogic
from .db_utils import save_to_database

class ProcessingWorker:
    def __init__(self, raw_data_dir: str, processed_data_dir: str, database_url: str,
                 whisper_model=None, sentence_model=None):
        self.raw_data_dir = raw_data_dir
        self.processed_data_dir = processed_data_dir
        self.database_url = database_url
        self.file_handler = FileHandler(images_dir=os.path.join(processed_data_dir, "images"))
        self.processing_logic = ProcessingLogic(whisper_model, sentence_model)
        os.makedirs(self.processed_data_dir, exist_ok=True)

    def run_worker(self):
        """Laufender Worker für alle JSON-Metadatendateien in raw_data_dir"""
        print("Starte Data Processing Service...")
        while True:
            json_files = [f for f in os.listdir(self.raw_data_dir) if f.endswith('.json')]
            if not json_files:
                print("Keine neuen Metadaten-Dateien gefunden. Warte...")
                time.sleep(30)
                continue

            for json_file_name in json_files:
                try:
                    metadata_path = os.path.join(self.raw_data_dir, json_file_name)
                    metadata = self.file_handler.get_metadata_from_json(metadata_path)
                    base_name = os.path.splitext(json_file_name)[0]

                    # Erweiterung: Bilder aus Medien extrahieren (optional)
                    # image_data = []
                    # if media_path.endswith('.pdf'):
                    #    image_data = self.file_handler._extract_images_from_pdf(media_path, base_name)
                    # elif media_path.endswith('.docx'):
                    #    image_data = self.file_handler._extract_images_from_docx(media_path, base_name)
                    # ... an API/db_utils übergeben ...

                    # Suche nach zugehöriger Mediendatei
                    media_path = None
                    for ext in ['.mp3', '.txt', '.pdf', '.docx', '.csv']:
                        potential_path = os.path.join(self.raw_data_dir, f"{base_name}{ext}")
                        if os.path.exists(potential_path):
                            media_path = potential_path
                            break

                    if not media_path:
                        print(f"Keine Mediendatei für {json_file_name} gefunden. Überspringe...")
                        os.remove(metadata_path)
                        continue

                    # Inhalt extrahieren
                    if media_path.endswith('.mp3'):
                        text_content = self.processing_logic.transcribe_audio(media_path)
                    else:
                        text_content = self.file_handler.extract_text(media_path)

                    processed_data = self.processing_logic.process_text_for_ai(text_content)
                    # Setze alle neuen Felder auf das Metadatenobjekt
                    for k, v in processed_data.items():
                        metadata[k] = v

                    # Embeddings als Output für Weaviate
                    embeddings = processed_data['embeddings']

                    # Datenbank speichern (DB + Vektor)
                    save_to_database(metadata, text_content, embeddings)

                    # Optional: Backup/Output
                    backup_path = os.path.join(self.processed_data_dir, f"{base_name}_processed.json")
                    with open(backup_path, 'w', encoding='utf-8') as f:
                        import json
                        json.dump({
                            "metadata": metadata,
                            "text": text_content,
                            "embeddings": embeddings
                        }, f, indent=4)
                    os.remove(metadata_path)
                    os.remove(media_path)
                    print(f"Verarbeitung von {json_file_name} erfolgreich. Dateien gelöscht.")

                except Exception as e:
                    print(f"Fehler beim Verarbeiten von {json_file_name}: {e}")
                    time.sleep(5)

if __name__ == "__main__":
    RAW_DATA_DIR = "/app/raw_data"
    PROCESSED_DATA_DIR = "/app/processed_data"
    DATABASE_URL = os.environ.get("DATABASE_URL")
    from faster_whisper import WhisperModel
    from sentence_transformers import SentenceTransformer
    whisper_model = WhisperModel("small", device="cpu")
    sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
    worker = ProcessingWorker(
        raw_data_dir=RAW_DATA_DIR,
        processed_data_dir=PROCESSED_DATA_DIR,
        database_url=DATABASE_URL,
        whisper_model=whisper_model,
        sentence_model=sentence_model
    )
    worker.run_worker()

    
-- sdg_interlinks

SDG_INTERLINKS = {
    "SDG 1 No Poverty": [
        "SDG 2 Zero Hunger",
        "SDG 3 Good Health and Well-being",
        "SDG 4 Quality Education",
        "SDG 5 Gender Equality",
        "SDG 8 Decent Work and Economic Growth",
        "SDG 10 Reduced Inequalities",
        "SDG 13 Climate Action"
    ],
    "SDG 2 Zero Hunger": [
        "SDG 1 No Poverty",
        "SDG 3 Good Health and Well-being",
        "SDG 6 Clean Water and Sanitation",
        "SDG 12 Responsible Consumption and Production",
        "SDG 13 Climate Action",
        "SDG 15 Life on Land"
    ],
    "SDG 3 Good Health and Well-being": [
        "SDG 1 No Poverty",
        "SDG 2 Zero Hunger",
        "SDG 4 Quality Education",
        "SDG 5 Gender Equality",
        "SDG 6 Clean Water and Sanitation",
        "SDG 13 Climate Action",
        "SDG 16 Peace, Justice and Strong Institutions"
    ],
    "SDG 4 Quality Education": [
        "SDG 1 No Poverty",
        "SDG 3 Good Health and Well-being",
        "SDG 5 Gender Equality",
        "SDG 8 Decent Work and Economic Growth",
        "SDG 10 Reduced Inequalities",
        "SDG 13 Climate Action"
    ],
    "SDG 5 Gender Equality": [
        "SDG 1 No Poverty",
        "SDG 3 Good Health and Well-being",
        "SDG 4 Quality Education",
        "SDG 8 Decent Work and Economic Growth",
        "SDG 10 Reduced Inequalities",
        "SDG 16 Peace, Justice and Strong Institutions"
    ],
    "SDG 6 Clean Water and Sanitation": [
        "SDG 2 Zero Hunger",
        "SDG 3 Good Health and Well-being",
        "SDG 7 Affordable and Clean Energy",
        "SDG 11 Sustainable Cities and Communities",
        "SDG 13 Climate Action",
        "SDG 14 Life Below Water",
        "SDG 15 Life on Land"
    ],
    "SDG 7 Affordable and Clean Energy": [
        "SDG 6 Clean Water and Sanitation",
        "SDG 8 Decent Work and Economic Growth",
        "SDG 9 Industry, Innovation and Infrastructure",
        "SDG 11 Sustainable Cities and Communities",
        "SDG 12 Responsible Consumption and Production",
        "SDG 13 Climate Action"
    ],
    "SDG 8 Decent Work and Economic Growth": [
        "SDG 1 No Poverty",
        "SDG 4 Quality Education",
        "SDG 5 Gender Equality",
        "SDG 7 Affordable and Clean Energy",
        "SDG 9 Industry, Innovation and Infrastructure",
        "SDG 10 Reduced Inequalities",
        "SDG 12 Responsible Consumption and Production"
    ],
    "SDG 9 Industry, Innovation and Infrastructure": [
        "SDG 7 Affordable and Clean Energy",
        "SDG 8 Decent Work and Economic Growth",
        "SDG 11 Sustainable Cities and Communities",
        "SDG 12 Responsible Consumption and Production",
        "SDG 13 Climate Action",
        "SDG 17 Partnerships for the Goals"
    ],
    "SDG 10 Reduced Inequalities": [
        "SDG 1 No Poverty",
        "SDG 4 Quality Education",
        "SDG 5 Gender Equality",
        "SDG 8 Decent Work and Economic Growth",
        "SDG 16 Peace, Justice and Strong Institutions",
        "SDG 17 Partnerships for the Goals"
    ],
    "SDG 11 Sustainable Cities and Communities": [
        "SDG 6 Clean Water and Sanitation",
        "SDG 7 Affordable and Clean Energy",
        "SDG 9 Industry, Innovation and Infrastructure",
        "SDG 12 Responsible Consumption and Production",
        "SDG 13 Climate Action",
        "SDG 15 Life on Land"
    ],
    "SDG 12 Responsible Consumption and Production": [
        "SDG 2 Zero Hunger",
        "SDG 7 Affordable and Clean Energy",
        "SDG 8 Decent Work and Economic Growth",
        "SDG 9 Industry, Innovation and Infrastructure",
        "SDG 11 Sustainable Cities and Communities",
        "SDG 13 Climate Action",
        "SDG 14 Life Below Water",
        "SDG 15 Life on Land"
    ],
    "SDG 13 Climate Action": [
        "SDG 1 No Poverty",
        "SDG 2 Zero Hunger",
        "SDG 3 Good Health and Well-being",
        "SDG 4 Quality Education",
        "SDG 6 Clean Water and Sanitation",
        "SDG 7 Affordable and Clean Energy",
        "SDG 9 Industry, Innovation and Infrastructure",
        "SDG 11 Sustainable Cities and Communities",
        "SDG 12 Responsible Consumption and Production",
        "SDG 14 Life Below Water",
        "SDG 15 Life on Land"
    ],
    "SDG 14 Life Below Water": [
        "SDG 6 Clean Water and Sanitation",
        "SDG 12 Responsible Consumption and Production",
        "SDG 13 Climate Action",
        "SDG 15 Life on Land"
    ],
    "SDG 15 Life on Land": [
        "SDG 2 Zero Hunger",
        "SDG 6 Clean Water and Sanitation",
        "SDG 11 Sustainable Cities and Communities",
        "SDG 12 Responsible Consumption and Production",
        "SDG 13 Climate Action",
        "SDG 14 Life Below Water"
    ],
    "SDG 16 Peace, Justice and Strong Institutions": [
        "SDG 3 Good Health and Well-being",
        "SDG 5 Gender Equality",
        "SDG 10 Reduced Inequalities",
        "SDG 17 Partnerships for the Goals"
    ],
    "SDG 17 Partnerships for the Goals": [
        "SDG 9 Industry, Innovation and Infrastructure",
        "SDG 10 Reduced Inequalities",
        "SDG 16 Peace, Justice and Strong Institutions"
    ]
}

-- text_chunker.py

# src/data_processing/core/text_chunker.py
# src/data_processing/core/text_chunker.py
import re
from typing import List, Dict, Any
from sentence_transformers import SentenceTransformer

class SDGTextChunker:
    def __init__(self, chunk_size: int = 512, overlap: int = 50, model_name: str = "all-MiniLM-L6-v2"):
        self.chunk_size = chunk_size
        self.overlap = overlap
        self.model = SentenceTransformer(model_name)
        
    def smart_chunk_by_sentences(self, text: str) -> List[Dict[str, Any]]:
        """
        Intelligently chunks text by sentences while respecting size limits.
        Maintains context and semantic coherence.
        """
        sentences = self._split_into_sentences(text)
        chunks = []
        current_chunk = ""
        current_length = 0
        chunk_id = 0
        
        for sentence in sentences:
            sentence_length = len(sentence)
            
            # If adding this sentence exceeds chunk_size, save current chunk
            if current_length + sentence_length > self.chunk_size and current_chunk:
                chunks.append({
                    "chunk_id": chunk_id,
                    "text": current_chunk.strip(),
                    "length": current_length,
                    "embedding": None  # To be filled later
                })
                
                # Start new chunk with overlap
                overlap_text = self._get_overlap_text(current_chunk)
                current_chunk = overlap_text + " " + sentence
                current_length = len(current_chunk)
                chunk_id += 1
            else:
                current_chunk += " " + sentence
                current_length += sentence_length
        
        # Add final chunk
        if current_chunk.strip():
            chunks.append({
                "chunk_id": chunk_id,
                "text": current_chunk.strip(),
                "length": current_length,
                "embedding": None
            })
            
        return chunks
    
    def chunk_by_sdg_sections(self, text: str) -> List[Dict[str, Any]]:
        """
        Chunks text by SDG-specific sections and topics.
        Uses your existing SDG keywords for intelligent sectioning.
        """
        from .keywords import sdg_keywords_dict
        
        chunks = []
        sections = self._identify_sdg_sections(text, sdg_keywords_dict)
        
        for section_name, section_text in sections.items():
            if len(section_text) > self.chunk_size:
                # Further chunk large sections
                sub_chunks = self.smart_chunk_by_sentences(section_text)
                for i, sub_chunk in enumerate(sub_chunks):
                    sub_chunk.update({
                        "sdg_section": section_name,
                        "sub_section_id": i
                    })
                    chunks.append(sub_chunk)
            else:
                chunks.append({
                    "chunk_id": len(chunks),
                    "text": section_text,
                    "length": len(section_text),
                    "sdg_section": section_name,
                    "embedding": None
                })
        
        return chunks
    
    def generate_embeddings_for_chunks(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Generate embeddings for each chunk using your existing sentence transformer."""
        for chunk in chunks:
            chunk["embedding"] = self.model.encode(chunk["text"]).tolist()
        return chunks
    
    def _split_into_sentences(self, text: str) -> List[str]:
        """Split text into sentences using regex."""
        sentences = re.split(r'(?<=[.!?])\s+', text)
        return [s.strip() for s in sentences if s.strip()]
    
    def _get_overlap_text(self, text: str) -> str:
        """Get last N characters for overlap."""
        return text[-self.overlap:] if len(text) > self.overlap else text
    
    def _identify_sdg_sections(self, text: str, sdg_keywords: Dict) -> Dict[str, str]:
        """Identify sections of text related to specific SDGs."""
        sections = {"general": ""}
        text_lower = text.lower()
        
        for sdg_name, keywords in sdg_keywords.items():
            section_text = ""
            for keyword in keywords:
                sentences = self._split_into_sentences(text)
                for sentence in sentences:
                    if keyword.lower() in sentence.lower():
                        section_text += sentence + " "
            
            if section_text.strip():
                sections[sdg_name] = section_text.strip()
            else:
                sections["general"] += text 
                
        return sections
        
/sdg_root/src/data_processing/core/vektorizer

-- text_vektorizer.py

from sentence_transformers import SentenceTransformer

class TextVectorizer:
    def __init__(self, model_name="all-MiniLM-L6-v2", device="cpu"):
        self.model = SentenceTransformer(model_name, device=device)

    def embed(self, text: str) -> list:
        return self.model.encode(text).tolist()
